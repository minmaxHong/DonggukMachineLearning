{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YuMdw0lKveK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/4class_data.csv\", header = None)\n",
        "\n",
        "train = np.array(df.iloc[[0, 1]])\n",
        "target = np.array(df.iloc[[2]])"
      ],
      "metadata": {
        "id": "awPajEvqMaPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J388yB7VamMM",
        "outputId": "3b2085ec-0290-4a5c-c065-e59a288a9c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 40), (1, 40))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 주어진 데이터\n",
        "train_data = np.array(df.iloc[[0, 1]])\n",
        "target_data = np.array(df.iloc[[2]]).astype(np.uint8)\n",
        "\n",
        "# 네트워크 정의\n",
        "def initialize_parameters(input_dim, output_dim):\n",
        "    np.random.seed(42)\n",
        "    W = np.random.randn(output_dim, input_dim) * 0.01\n",
        "    b = np.zeros((output_dim, 1))\n",
        "    return W, b\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
        "\n",
        "def forward_propagation(X, W, b):\n",
        "    Z = np.dot(W, X) + b\n",
        "    A = softmax(Z)\n",
        "    return A\n",
        "\n",
        "def compute_loss(A, Y):\n",
        "    m = Y.shape[1]\n",
        "    loss = -1/m * np.sum(Y * np.log(A + 1e-8))  # 로그 안에 0이 들어가지 않도록 아주 작은 값 더해줌\n",
        "    return loss\n",
        "\n",
        "def backward_propagation(X, A, Y):\n",
        "    m = X.shape[1]\n",
        "    dZ = A - Y\n",
        "    dW = 1/m * np.dot(dZ, X.T)\n",
        "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    return dW, db\n",
        "\n",
        "def update_parameters(W, b, dW, db, learning_rate):\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n",
        "    return W, b\n",
        "\n",
        "def plot_loss(losses):\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Iterations (per hundreds)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss over Iterations')\n",
        "    plt.show()\n",
        "    plt.savefig('loss.png')\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_labels = labels.shape[1]\n",
        "    one_hot = np.zeros((num_classes, num_labels))\n",
        "    one_hot[labels.flatten(), np.arange(num_labels)] = 1\n",
        "    return one_hot\n",
        "\n",
        "def train(train_data, target_data, num_iterations, learning_rate):\n",
        "    input_dim = train_data.shape[0]\n",
        "    output_dim = np.max(target_data) + 1  # 클래스 개수는 레이블 중 가장 큰 값 + 1\n",
        "    target_one_hot = one_hot_encode(target_data, output_dim)\n",
        "\n",
        "    W, b = initialize_parameters(input_dim, output_dim)\n",
        "    loss_arr = []\n",
        "    for i in range(num_iterations):\n",
        "        # Forward Propagation\n",
        "        A = forward_propagation(train_data, W, b)\n",
        "\n",
        "        # Compute Loss\n",
        "        loss = compute_loss(A, target_one_hot)\n",
        "        loss_arr.append(loss)\n",
        "\n",
        "        # Backward Propagation\n",
        "        dW, db = backward_propagation(train_data, A, target_one_hot)\n",
        "\n",
        "        # Update Parameters\n",
        "        W, b = update_parameters(W, b, dW, db, learning_rate)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Iteration {i}, Loss: {loss}')\n",
        "\n",
        "    plot_loss(loss_arr)\n",
        "    return W, b\n",
        "\n",
        "# 학습\n",
        "num_iterations = 500000\n",
        "learning_rate = 0.01\n",
        "\n",
        "W, b = train(train_data, target_data, num_iterations, learning_rate)\n",
        "\n",
        "# 학습된 파라미터로 예측\n",
        "def predict(X, W, b):\n",
        "    A = forward_propagation(X, W, b)\n",
        "    predictions = np.argmax(A, axis=0)\n",
        "    return predictions\n",
        "\n",
        "# 예측\n",
        "predictions_arr = []\n",
        "predictions = predict(train_data, W, b)\n",
        "predictions_arr.append(predictions)\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual:\", target_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QwiNxFovXWPv",
        "outputId": "c7f3b945-7435-4e58-be3b-b4d34a128098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 1.3852126655522552\n",
            "Iteration 100, Loss: 1.3714732195116073\n",
            "Iteration 200, Loss: 1.3585641109268378\n",
            "Iteration 300, Loss: 1.3461922731416134\n",
            "Iteration 400, Loss: 1.3342139940305282\n",
            "Iteration 500, Loss: 1.322557254135476\n",
            "Iteration 600, Loss: 1.311184316342648\n",
            "Iteration 700, Loss: 1.3000738272205807\n",
            "Iteration 800, Loss: 1.289212289903019\n",
            "Iteration 900, Loss: 1.278590010529098\n",
            "Iteration 1000, Loss: 1.2681991728194213\n",
            "Iteration 1100, Loss: 1.2580329235894436\n",
            "Iteration 1200, Loss: 1.2480849384265273\n",
            "Iteration 1300, Loss: 1.2383492155371159\n",
            "Iteration 1400, Loss: 1.2288199780553952\n",
            "Iteration 1500, Loss: 1.219491627869201\n",
            "Iteration 1600, Loss: 1.2103587238299707\n",
            "Iteration 1700, Loss: 1.2014159713987071\n",
            "Iteration 1800, Loss: 1.1926582175452285\n",
            "Iteration 1900, Loss: 1.1840804479527647\n",
            "Iteration 2000, Loss: 1.1756777851306157\n",
            "Iteration 2100, Loss: 1.1674454867825936\n",
            "Iteration 2200, Loss: 1.1593789441373867\n",
            "Iteration 2300, Loss: 1.1514736801192424\n",
            "Iteration 2400, Loss: 1.143725347319718\n",
            "Iteration 2500, Loss: 1.1361297257699574\n",
            "Iteration 2600, Loss: 1.1286827205304482\n",
            "Iteration 2700, Loss: 1.121380359122348\n",
            "Iteration 2800, Loss: 1.114218788826561\n",
            "Iteration 2900, Loss: 1.1071942738763916\n",
            "Iteration 3000, Loss: 1.100303192568102\n",
            "Iteration 3100, Loss: 1.0935420343117046\n",
            "Iteration 3200, Loss: 1.0869073966421545\n",
            "Iteration 3300, Loss: 1.0803959822089209\n",
            "Iteration 3400, Loss: 1.0740045957598257\n",
            "Iteration 3500, Loss: 1.0677301411330642\n",
            "Iteration 3600, Loss: 1.061569618269492\n",
            "Iteration 3700, Loss: 1.0555201202555733\n",
            "Iteration 3800, Loss: 1.049578830405861\n",
            "Iteration 3900, Loss: 1.0437430193924901\n",
            "Iteration 4000, Loss: 1.0380100424279135\n",
            "Iteration 4100, Loss: 1.0323773365059985\n",
            "Iteration 4200, Loss: 1.0268424177056013\n",
            "Iteration 4300, Loss: 1.021402878559855\n",
            "Iteration 4400, Loss: 1.0160563854936342\n",
            "Iteration 4500, Loss: 1.010800676330963\n",
            "Iteration 4600, Loss: 1.0056335578735476\n",
            "Iteration 4700, Loss: 1.0005529035510896\n",
            "Iteration 4800, Loss: 0.9955566511435848\n",
            "Iteration 4900, Loss: 0.9906428005754301\n",
            "Iteration 5000, Loss: 0.9858094117808325\n",
            "Iteration 5100, Loss: 0.9810546026397319\n",
            "Iteration 5200, Loss: 0.9763765469832175\n",
            "Iteration 5300, Loss: 0.9717734726672203\n",
            "Iteration 5400, Loss: 0.9672436597131147\n",
            "Iteration 5500, Loss: 0.962785438513723\n",
            "Iteration 5600, Loss: 0.958397188103117\n",
            "Iteration 5700, Loss: 0.9540773344885434\n",
            "Iteration 5800, Loss: 0.9498243490427317\n",
            "Iteration 5900, Loss: 0.945636746954807\n",
            "Iteration 6000, Loss: 0.9415130857380133\n",
            "Iteration 6100, Loss: 0.9374519637924316\n",
            "Iteration 6200, Loss: 0.933452019020886\n",
            "Iteration 6300, Loss: 0.9295119274962401\n",
            "Iteration 6400, Loss: 0.9256304021783\n",
            "Iteration 6500, Loss: 0.9218061916785726\n",
            "Iteration 6600, Loss: 0.9180380790711496\n",
            "Iteration 6700, Loss: 0.9143248807480326\n",
            "Iteration 6800, Loss: 0.910665445317245\n",
            "Iteration 6900, Loss: 0.9070586525421241\n",
            "Iteration 7000, Loss: 0.903503412320227\n",
            "Iteration 7100, Loss: 0.8999986637003294\n",
            "Iteration 7200, Loss: 0.896543373936048\n",
            "Iteration 7300, Loss: 0.8931365375746534\n",
            "Iteration 7400, Loss: 0.8897771755797035\n",
            "Iteration 7500, Loss: 0.8864643344861574\n",
            "Iteration 7600, Loss: 0.8831970855866921\n",
            "Iteration 7700, Loss: 0.879974524147979\n",
            "Iteration 7800, Loss: 0.8767957686557329\n",
            "Iteration 7900, Loss: 0.8736599600873828\n",
            "Iteration 8000, Loss: 0.8705662612112635\n",
            "Iteration 8100, Loss: 0.8675138559112686\n",
            "Iteration 8200, Loss: 0.8645019485359442\n",
            "Iteration 8300, Loss: 0.8615297632710508\n",
            "Iteration 8400, Loss: 0.8585965435346488\n",
            "Iteration 8500, Loss: 0.8557015513938139\n",
            "Iteration 8600, Loss: 0.8528440670021151\n",
            "Iteration 8700, Loss: 0.8500233880570319\n",
            "Iteration 8800, Loss: 0.8472388292765125\n",
            "Iteration 8900, Loss: 0.8444897218939188\n",
            "Iteration 9000, Loss: 0.841775413170624\n",
            "Iteration 9100, Loss: 0.8390952659255716\n",
            "Iteration 9200, Loss: 0.8364486580811218\n",
            "Iteration 9300, Loss: 0.8338349822245483\n",
            "Iteration 9400, Loss: 0.8312536451845718\n",
            "Iteration 9500, Loss: 0.828704067622342\n",
            "Iteration 9600, Loss: 0.8261856836363081\n",
            "Iteration 9700, Loss: 0.8236979403804366\n",
            "Iteration 9800, Loss: 0.8212402976952629\n",
            "Iteration 9900, Loss: 0.8188122277512808\n",
            "Iteration 10000, Loss: 0.8164132147041983\n",
            "Iteration 10100, Loss: 0.814042754361606\n",
            "Iteration 10200, Loss: 0.8117003538606243\n",
            "Iteration 10300, Loss: 0.8093855313561127\n",
            "Iteration 10400, Loss: 0.8070978157190455\n",
            "Iteration 10500, Loss: 0.8048367462446671\n",
            "Iteration 10600, Loss: 0.8026018723700684\n",
            "Iteration 10700, Loss: 0.8003927534008274\n",
            "Iteration 10800, Loss: 0.7982089582463812\n",
            "Iteration 10900, Loss: 0.7960500651638069\n",
            "Iteration 11000, Loss: 0.7939156615097043\n",
            "Iteration 11100, Loss: 0.7918053434998806\n",
            "Iteration 11200, Loss: 0.7897187159765564\n",
            "Iteration 11300, Loss: 0.7876553921828221\n",
            "Iteration 11400, Loss: 0.7856149935440778\n",
            "Iteration 11500, Loss: 0.7835971494562134\n",
            "Iteration 11600, Loss: 0.7816014970802836\n",
            "Iteration 11700, Loss: 0.7796276811434487\n",
            "Iteration 11800, Loss: 0.7776753537459596\n",
            "Iteration 11900, Loss: 0.7757441741739758\n",
            "Iteration 12000, Loss: 0.7738338087180092\n",
            "Iteration 12100, Loss: 0.7719439304968018\n",
            "Iteration 12200, Loss: 0.7700742192864454\n",
            "Iteration 12300, Loss: 0.7682243613545635\n",
            "Iteration 12400, Loss: 0.7663940492993844\n",
            "Iteration 12500, Loss: 0.7645829818935334\n",
            "Iteration 12600, Loss: 0.7627908639323878\n",
            "Iteration 12700, Loss: 0.7610174060868393\n",
            "Iteration 12800, Loss: 0.7592623247603137\n",
            "Iteration 12900, Loss: 0.7575253419499075\n",
            "Iteration 13000, Loss: 0.7558061851115034\n",
            "Iteration 13100, Loss: 0.7541045870287313\n",
            "Iteration 13200, Loss: 0.7524202856856507\n",
            "Iteration 13300, Loss: 0.7507530241430276\n",
            "Iteration 13400, Loss: 0.7491025504180953\n",
            "Iteration 13500, Loss: 0.7474686173676767\n",
            "Iteration 13600, Loss: 0.745850982574567\n",
            "Iteration 13700, Loss: 0.744249408237068\n",
            "Iteration 13800, Loss: 0.742663661061572\n",
            "Iteration 13900, Loss: 0.741093512158101\n",
            "Iteration 14000, Loss: 0.7395387369387036\n",
            "Iteration 14100, Loss: 0.737999115018621\n",
            "Iteration 14200, Loss: 0.7364744301201365\n",
            "Iteration 14300, Loss: 0.7349644699790194\n",
            "Iteration 14400, Loss: 0.7334690262534878\n",
            "Iteration 14500, Loss: 0.7319878944356075\n",
            "Iteration 14600, Loss: 0.7305208737650553\n",
            "Iteration 14700, Loss: 0.7290677671451693\n",
            "Iteration 14800, Loss: 0.7276283810612225\n",
            "Iteration 14900, Loss: 0.726202525500844\n",
            "Iteration 15000, Loss: 0.7247900138765291\n",
            "Iteration 15100, Loss: 0.7233906629501712\n",
            "Iteration 15200, Loss: 0.7220042927595568\n",
            "Iteration 15300, Loss: 0.7206307265467622\n",
            "Iteration 15400, Loss: 0.719269790688397\n",
            "Iteration 15500, Loss: 0.7179213146276399\n",
            "Iteration 15600, Loss: 0.7165851308080101\n",
            "Iteration 15700, Loss: 0.7152610746088288\n",
            "Iteration 15800, Loss: 0.7139489842823153\n",
            "Iteration 15900, Loss: 0.712648700892274\n",
            "Iteration 16000, Loss: 0.7113600682543236\n",
            "Iteration 16100, Loss: 0.710082932877625\n",
            "Iteration 16200, Loss: 0.7088171439080638\n",
            "Iteration 16300, Loss: 0.7075625530728463\n",
            "Iteration 16400, Loss: 0.7063190146264673\n",
            "Iteration 16500, Loss: 0.7050863852980109\n",
            "Iteration 16600, Loss: 0.7038645242397501\n",
            "Iteration 16700, Loss: 0.7026532929769989\n",
            "Iteration 16800, Loss: 0.7014525553591935\n",
            "Iteration 16900, Loss: 0.7002621775121587\n",
            "Iteration 17000, Loss: 0.6990820277915311\n",
            "Iteration 17100, Loss: 0.6979119767373057\n",
            "Iteration 17200, Loss: 0.6967518970294743\n",
            "Iteration 17300, Loss: 0.6956016634447253\n",
            "Iteration 17400, Loss: 0.6944611528141769\n",
            "Iteration 17500, Loss: 0.693330243982114\n",
            "Iteration 17600, Loss: 0.6922088177657023\n",
            "Iteration 17700, Loss: 0.6910967569156523\n",
            "Iteration 17800, Loss: 0.6899939460778067\n",
            "Iteration 17900, Loss: 0.6889002717556297\n",
            "Iteration 18000, Loss: 0.687815622273568\n",
            "Iteration 18100, Loss: 0.6867398877412666\n",
            "Iteration 18200, Loss: 0.6856729600186112\n",
            "Iteration 18300, Loss: 0.6846147326815782\n",
            "Iteration 18400, Loss: 0.6835651009888704\n",
            "Iteration 18500, Loss: 0.6825239618493162\n",
            "Iteration 18600, Loss: 0.6814912137900145\n",
            "Iteration 18700, Loss: 0.680466756925203\n",
            "Iteration 18800, Loss: 0.6794504929258336\n",
            "Iteration 18900, Loss: 0.6784423249898346\n",
            "Iteration 19000, Loss: 0.6774421578130428\n",
            "Iteration 19100, Loss: 0.6764498975607872\n",
            "Iteration 19200, Loss: 0.6754654518401093\n",
            "Iteration 19300, Loss: 0.6744887296726012\n",
            "Iteration 19400, Loss: 0.6735196414678477\n",
            "Iteration 19500, Loss: 0.6725580989974559\n",
            "Iteration 19600, Loss: 0.6716040153696583\n",
            "Iteration 19700, Loss: 0.6706573050044727\n",
            "Iteration 19800, Loss: 0.6697178836094086\n",
            "Iteration 19900, Loss: 0.6687856681557014\n",
            "Iteration 20000, Loss: 0.6678605768550663\n",
            "Iteration 20100, Loss: 0.6669425291369543\n",
            "Iteration 20200, Loss: 0.666031445626302\n",
            "Iteration 20300, Loss: 0.6651272481217593\n",
            "Iteration 20400, Loss: 0.6642298595743846\n",
            "Iteration 20500, Loss: 0.6633392040667971\n",
            "Iteration 20600, Loss: 0.662455206792774\n",
            "Iteration 20700, Loss: 0.6615777940372791\n",
            "Iteration 20800, Loss: 0.660706893156919\n",
            "Iteration 20900, Loss: 0.6598424325608081\n",
            "Iteration 21000, Loss: 0.6589843416918386\n",
            "Iteration 21100, Loss: 0.6581325510083444\n",
            "Iteration 21200, Loss: 0.6572869919661468\n",
            "Iteration 21300, Loss: 0.6564475970009754\n",
            "Iteration 21400, Loss: 0.6556142995112563\n",
            "Iteration 21500, Loss: 0.6547870338412534\n",
            "Iteration 21600, Loss: 0.6539657352645615\n",
            "Iteration 21700, Loss: 0.6531503399679375\n",
            "Iteration 21800, Loss: 0.6523407850354644\n",
            "Iteration 21900, Loss: 0.6515370084330387\n",
            "Iteration 22000, Loss: 0.6507389489931743\n",
            "Iteration 22100, Loss: 0.6499465464001171\n",
            "Iteration 22200, Loss: 0.6491597411752585\n",
            "Iteration 22300, Loss: 0.6483784746628466\n",
            "Iteration 22400, Loss: 0.6476026890159834\n",
            "Iteration 22500, Loss: 0.646832327182905\n",
            "Iteration 22600, Loss: 0.6460673328935342\n",
            "Iteration 22700, Loss: 0.6453076506463046\n",
            "Iteration 22800, Loss: 0.644553225695244\n",
            "Iteration 22900, Loss: 0.6438040040373164\n",
            "Iteration 23000, Loss: 0.6430599324000134\n",
            "Iteration 23100, Loss: 0.64232095822919\n",
            "Iteration 23200, Loss: 0.6415870296771411\n",
            "Iteration 23300, Loss: 0.6408580955909102\n",
            "Iteration 23400, Loss: 0.640134105500827\n",
            "Iteration 23500, Loss: 0.6394150096092697\n",
            "Iteration 23600, Loss: 0.6387007587796436\n",
            "Iteration 23700, Loss: 0.6379913045255752\n",
            "Iteration 23800, Loss: 0.6372865990003131\n",
            "Iteration 23900, Loss: 0.6365865949863353\n",
            "Iteration 24000, Loss: 0.6358912458851538\n",
            "Iteration 24100, Loss: 0.6352005057073158\n",
            "Iteration 24200, Loss: 0.634514329062596\n",
            "Iteration 24300, Loss: 0.6338326711503736\n",
            "Iteration 24400, Loss: 0.6331554877501955\n",
            "Iteration 24500, Loss: 0.6324827352125135\n",
            "Iteration 24600, Loss: 0.6318143704496013\n",
            "Iteration 24700, Loss: 0.6311503509266374\n",
            "Iteration 24800, Loss: 0.6304906346529597\n",
            "Iteration 24900, Loss: 0.629835180173481\n",
            "Iteration 25000, Loss: 0.6291839465602646\n",
            "Iteration 25100, Loss: 0.6285368934042588\n",
            "Iteration 25200, Loss: 0.6278939808071824\n",
            "Iteration 25300, Loss: 0.6272551693735617\n",
            "Iteration 25400, Loss: 0.6266204202029132\n",
            "Iteration 25500, Loss: 0.6259896948820708\n",
            "Iteration 25600, Loss: 0.6253629554776546\n",
            "Iteration 25700, Loss: 0.6247401645286745\n",
            "Iteration 25800, Loss: 0.6241212850392728\n",
            "Iteration 25900, Loss: 0.6235062804715945\n",
            "Iteration 26000, Loss: 0.6228951147387903\n",
            "Iteration 26100, Loss: 0.6222877521981447\n",
            "Iteration 26200, Loss: 0.6216841576443275\n",
            "Iteration 26300, Loss: 0.621084296302767\n",
            "Iteration 26400, Loss: 0.6204881338231436\n",
            "Iteration 26500, Loss: 0.6198956362729975\n",
            "Iteration 26600, Loss: 0.619306770131451\n",
            "Iteration 26700, Loss: 0.6187215022830442\n",
            "Iteration 26800, Loss: 0.618139800011677\n",
            "Iteration 26900, Loss: 0.6175616309946612\n",
            "Iteration 27000, Loss: 0.6169869632968755\n",
            "Iteration 27100, Loss: 0.6164157653650246\n",
            "Iteration 27200, Loss: 0.6158480060219976\n",
            "Iteration 27300, Loss: 0.6152836544613272\n",
            "Iteration 27400, Loss: 0.6147226802417435\n",
            "Iteration 27500, Loss: 0.6141650532818235\n",
            "Iteration 27600, Loss: 0.6136107438547341\n",
            "Iteration 27700, Loss: 0.6130597225830648\n",
            "Iteration 27800, Loss: 0.6125119604337503\n",
            "Iteration 27900, Loss: 0.611967428713081\n",
            "Iteration 28000, Loss: 0.6114260990617977\n",
            "Iteration 28100, Loss: 0.6108879434502722\n",
            "Iteration 28200, Loss: 0.610352934173769\n",
            "Iteration 28300, Loss: 0.6098210438477877\n",
            "Iteration 28400, Loss: 0.6092922454034847\n",
            "Iteration 28500, Loss: 0.6087665120831733\n",
            "Iteration 28600, Loss: 0.608243817435899\n",
            "Iteration 28700, Loss: 0.6077241353130889\n",
            "Iteration 28800, Loss: 0.6072074398642758\n",
            "Iteration 28900, Loss: 0.6066937055328927\n",
            "Iteration 29000, Loss: 0.6061829070521378\n",
            "Iteration 29100, Loss: 0.6056750194409095\n",
            "Iteration 29200, Loss: 0.6051700179998076\n",
            "Iteration 29300, Loss: 0.6046678783072023\n",
            "Iteration 29400, Loss: 0.6041685762153662\n",
            "Iteration 29500, Loss: 0.6036720878466735\n",
            "Iteration 29600, Loss: 0.6031783895898575\n",
            "Iteration 29700, Loss: 0.6026874580963343\n",
            "Iteration 29800, Loss: 0.6021992702765824\n",
            "Iteration 29900, Loss: 0.6017138032965843\n",
            "Iteration 30000, Loss: 0.6012310345743249\n",
            "Iteration 30100, Loss: 0.6007509417763477\n",
            "Iteration 30200, Loss: 0.6002735028143654\n",
            "Iteration 30300, Loss: 0.599798695841927\n",
            "Iteration 30400, Loss: 0.5993264992511375\n",
            "Iteration 30500, Loss: 0.5988568916694311\n",
            "Iteration 30600, Loss: 0.5983898519563954\n",
            "Iteration 30700, Loss: 0.5979253592006482\n",
            "Iteration 30800, Loss: 0.5974633927167613\n",
            "Iteration 30900, Loss: 0.597003932042237\n",
            "Iteration 31000, Loss: 0.5965469569345289\n",
            "Iteration 31100, Loss: 0.5960924473681123\n",
            "Iteration 31200, Loss: 0.5956403835316019\n",
            "Iteration 31300, Loss: 0.5951907458249106\n",
            "Iteration 31400, Loss: 0.5947435148564586\n",
            "Iteration 31500, Loss: 0.5942986714404219\n",
            "Iteration 31600, Loss: 0.5938561965940267\n",
            "Iteration 31700, Loss: 0.5934160715348835\n",
            "Iteration 31800, Loss: 0.5929782776783665\n",
            "Iteration 31900, Loss: 0.5925427966350297\n",
            "Iteration 32000, Loss: 0.592109610208065\n",
            "Iteration 32100, Loss: 0.5916787003908003\n",
            "Iteration 32200, Loss: 0.5912500493642349\n",
            "Iteration 32300, Loss: 0.5908236394946144\n",
            "Iteration 32400, Loss: 0.590399453331039\n",
            "Iteration 32500, Loss: 0.589977473603115\n",
            "Iteration 32600, Loss: 0.5895576832186361\n",
            "Iteration 32700, Loss: 0.5891400652613029\n",
            "Iteration 32800, Loss: 0.5887246029884768\n",
            "Iteration 32900, Loss: 0.588311279828968\n",
            "Iteration 33000, Loss: 0.5879000793808568\n",
            "Iteration 33100, Loss: 0.5874909854093472\n",
            "Iteration 33200, Loss: 0.5870839818446548\n",
            "Iteration 33300, Loss: 0.5866790527799235\n",
            "Iteration 33400, Loss: 0.5862761824691766\n",
            "Iteration 33500, Loss: 0.5858753553252957\n",
            "Iteration 33600, Loss: 0.5854765559180323\n",
            "Iteration 33700, Loss: 0.5850797689720476\n",
            "Iteration 33800, Loss: 0.5846849793649807\n",
            "Iteration 33900, Loss: 0.5842921721255476\n",
            "Iteration 34000, Loss: 0.5839013324316665\n",
            "Iteration 34100, Loss: 0.5835124456086117\n",
            "Iteration 34200, Loss: 0.5831254971271935\n",
            "Iteration 34300, Loss: 0.5827404726019673\n",
            "Iteration 34400, Loss: 0.5823573577894651\n",
            "Iteration 34500, Loss: 0.5819761385864576\n",
            "Iteration 34600, Loss: 0.5815968010282372\n",
            "Iteration 34700, Loss: 0.5812193312869284\n",
            "Iteration 34800, Loss: 0.5808437156698226\n",
            "Iteration 34900, Loss: 0.5804699406177362\n",
            "Iteration 35000, Loss: 0.5800979927033936\n",
            "Iteration 35100, Loss: 0.5797278586298319\n",
            "Iteration 35200, Loss: 0.57935952522883\n",
            "Iteration 35300, Loss: 0.5789929794593592\n",
            "Iteration 35400, Loss: 0.578628208406057\n",
            "Iteration 35500, Loss: 0.5782651992777222\n",
            "Iteration 35600, Loss: 0.5779039394058311\n",
            "Iteration 35700, Loss: 0.5775444162430748\n",
            "Iteration 35800, Loss: 0.5771866173619186\n",
            "Iteration 35900, Loss: 0.5768305304531801\n",
            "Iteration 36000, Loss: 0.576476143324628\n",
            "Iteration 36100, Loss: 0.5761234438996009\n",
            "Iteration 36200, Loss: 0.5757724202156459\n",
            "Iteration 36300, Loss: 0.5754230604231748\n",
            "Iteration 36400, Loss: 0.5750753527841409\n",
            "Iteration 36500, Loss: 0.5747292856707326\n",
            "Iteration 36600, Loss: 0.5743848475640867\n",
            "Iteration 36700, Loss: 0.5740420270530192\n",
            "Iteration 36800, Loss: 0.5737008128327731\n",
            "Iteration 36900, Loss: 0.5733611937037836\n",
            "Iteration 37000, Loss: 0.5730231585704609\n",
            "Iteration 37100, Loss: 0.5726866964399904\n",
            "Iteration 37200, Loss: 0.572351796421147\n",
            "Iteration 37300, Loss: 0.5720184477231287\n",
            "Iteration 37400, Loss: 0.571686639654404\n",
            "Iteration 37500, Loss: 0.5713563616215757\n",
            "Iteration 37600, Loss: 0.5710276031282605\n",
            "Iteration 37700, Loss: 0.5707003537739836\n",
            "Iteration 37800, Loss: 0.5703746032530886\n",
            "Iteration 37900, Loss: 0.5700503413536613\n",
            "Iteration 38000, Loss: 0.5697275579564687\n",
            "Iteration 38100, Loss: 0.5694062430339125\n",
            "Iteration 38200, Loss: 0.5690863866489967\n",
            "Iteration 38300, Loss: 0.5687679789543082\n",
            "Iteration 38400, Loss: 0.5684510101910123\n",
            "Iteration 38500, Loss: 0.5681354706878603\n",
            "Iteration 38600, Loss: 0.5678213508602115\n",
            "Iteration 38700, Loss: 0.5675086412090679\n",
            "Iteration 38800, Loss: 0.5671973323201208\n",
            "Iteration 38900, Loss: 0.5668874148628124\n",
            "Iteration 39000, Loss: 0.5665788795894061\n",
            "Iteration 39100, Loss: 0.566271717334073\n",
            "Iteration 39200, Loss: 0.5659659190119884\n",
            "Iteration 39300, Loss: 0.5656614756184395\n",
            "Iteration 39400, Loss: 0.5653583782279468\n",
            "Iteration 39500, Loss: 0.565056617993395\n",
            "Iteration 39600, Loss: 0.5647561861451774\n",
            "Iteration 39700, Loss: 0.5644570739903497\n",
            "Iteration 39800, Loss: 0.5641592729117946\n",
            "Iteration 39900, Loss: 0.5638627743673993\n",
            "Iteration 40000, Loss: 0.5635675698892418\n",
            "Iteration 40100, Loss: 0.5632736510827885\n",
            "Iteration 40200, Loss: 0.5629810096261013\n",
            "Iteration 40300, Loss: 0.5626896372690569\n",
            "Iteration 40400, Loss: 0.5623995258325734\n",
            "Iteration 40500, Loss: 0.5621106672078491\n",
            "Iteration 40600, Loss: 0.56182305335561\n",
            "Iteration 40700, Loss: 0.5615366763053671\n",
            "Iteration 40800, Loss: 0.5612515281546825\n",
            "Iteration 40900, Loss: 0.5609676010684471\n",
            "Iteration 41000, Loss: 0.5606848872781638\n",
            "Iteration 41100, Loss: 0.5604033790812433\n",
            "Iteration 41200, Loss: 0.5601230688403069\n",
            "Iteration 41300, Loss: 0.5598439489824986\n",
            "Iteration 41400, Loss: 0.5595660119988048\n",
            "Iteration 41500, Loss: 0.5592892504433861\n",
            "Iteration 41600, Loss: 0.5590136569329117\n",
            "Iteration 41700, Loss: 0.5587392241459079\n",
            "Iteration 41800, Loss: 0.5584659448221106\n",
            "Iteration 41900, Loss: 0.5581938117618284\n",
            "Iteration 42000, Loss: 0.5579228178253122\n",
            "Iteration 42100, Loss: 0.5576529559321332\n",
            "Iteration 42200, Loss: 0.5573842190605686\n",
            "Iteration 42300, Loss: 0.5571166002469944\n",
            "Iteration 42400, Loss: 0.5568500925852875\n",
            "Iteration 42500, Loss: 0.5565846892262315\n",
            "Iteration 42600, Loss: 0.5563203833769336\n",
            "Iteration 42700, Loss: 0.5560571683002474\n",
            "Iteration 42800, Loss: 0.5557950373142012\n",
            "Iteration 42900, Loss: 0.5555339837914355\n",
            "Iteration 43000, Loss: 0.5552740011586462\n",
            "Iteration 43100, Loss: 0.5550150828960349\n",
            "Iteration 43200, Loss: 0.5547572225367651\n",
            "Iteration 43300, Loss: 0.554500413666427\n",
            "Iteration 43400, Loss: 0.5542446499225063\n",
            "Iteration 43500, Loss: 0.553989924993861\n",
            "Iteration 43600, Loss: 0.5537362326202038\n",
            "Iteration 43700, Loss: 0.5534835665915917\n",
            "Iteration 43800, Loss: 0.5532319207479203\n",
            "Iteration 43900, Loss: 0.552981288978425\n",
            "Iteration 44000, Loss: 0.5527316652211884\n",
            "Iteration 44100, Loss: 0.5524830434626529\n",
            "Iteration 44200, Loss: 0.5522354177371392\n",
            "Iteration 44300, Loss: 0.5519887821263717\n",
            "Iteration 44400, Loss: 0.5517431307590076\n",
            "Iteration 44500, Loss: 0.5514984578101728\n",
            "Iteration 44600, Loss: 0.5512547575010037\n",
            "Iteration 44700, Loss: 0.5510120240981926\n",
            "Iteration 44800, Loss: 0.5507702519135407\n",
            "Iteration 44900, Loss: 0.5505294353035146\n",
            "Iteration 45000, Loss: 0.5502895686688093\n",
            "Iteration 45100, Loss: 0.5500506464539144\n",
            "Iteration 45200, Loss: 0.5498126631466886\n",
            "Iteration 45300, Loss: 0.5495756132779352\n",
            "Iteration 45400, Loss: 0.549339491420986\n",
            "Iteration 45500, Loss: 0.5491042921912881\n",
            "Iteration 45600, Loss: 0.5488700102459955\n",
            "Iteration 45700, Loss: 0.5486366402835667\n",
            "Iteration 45800, Loss: 0.5484041770433662\n",
            "Iteration 45900, Loss: 0.5481726153052694\n",
            "Iteration 46000, Loss: 0.5479419498892748\n",
            "Iteration 46100, Loss: 0.5477121756551178\n",
            "Iteration 46200, Loss: 0.5474832875018905\n",
            "Iteration 46300, Loss: 0.5472552803676661\n",
            "Iteration 46400, Loss: 0.5470281492291268\n",
            "Iteration 46500, Loss: 0.5468018891011952\n",
            "Iteration 46600, Loss: 0.5465764950366726\n",
            "Iteration 46700, Loss: 0.5463519621258781\n",
            "Iteration 46800, Loss: 0.5461282854962947\n",
            "Iteration 46900, Loss: 0.5459054603122165\n",
            "Iteration 47000, Loss: 0.5456834817744033\n",
            "Iteration 47100, Loss: 0.545462345119736\n",
            "Iteration 47200, Loss: 0.5452420456208774\n",
            "Iteration 47300, Loss: 0.5450225785859371\n",
            "Iteration 47400, Loss: 0.5448039393581384\n",
            "Iteration 47500, Loss: 0.5445861233154918\n",
            "Iteration 47600, Loss: 0.5443691258704692\n",
            "Iteration 47700, Loss: 0.5441529424696834\n",
            "Iteration 47800, Loss: 0.5439375685935709\n",
            "Iteration 47900, Loss: 0.5437229997560783\n",
            "Iteration 48000, Loss: 0.5435092315043516\n",
            "Iteration 48100, Loss: 0.5432962594184295\n",
            "Iteration 48200, Loss: 0.5430840791109398\n",
            "Iteration 48300, Loss: 0.5428726862268001\n",
            "Iteration 48400, Loss: 0.5426620764429194\n",
            "Iteration 48500, Loss: 0.5424522454679063\n",
            "Iteration 48600, Loss: 0.5422431890417774\n",
            "Iteration 48700, Loss: 0.5420349029356708\n",
            "Iteration 48800, Loss: 0.5418273829515615\n",
            "Iteration 48900, Loss: 0.5416206249219818\n",
            "Iteration 49000, Loss: 0.5414146247097417\n",
            "Iteration 49100, Loss: 0.5412093782076554\n",
            "Iteration 49200, Loss: 0.5410048813382687\n",
            "Iteration 49300, Loss: 0.5408011300535908\n",
            "Iteration 49400, Loss: 0.5405981203348276\n",
            "Iteration 49500, Loss: 0.5403958481921193\n",
            "Iteration 49600, Loss: 0.5401943096642791\n",
            "Iteration 49700, Loss: 0.5399935008185374\n",
            "Iteration 49800, Loss: 0.5397934177502853\n",
            "Iteration 49900, Loss: 0.5395940565828238\n",
            "Iteration 50000, Loss: 0.5393954134671137\n",
            "Iteration 50100, Loss: 0.5391974845815304\n",
            "Iteration 50200, Loss: 0.5390002661316179\n",
            "Iteration 50300, Loss: 0.5388037543498491\n",
            "Iteration 50400, Loss: 0.5386079454953862\n",
            "Iteration 50500, Loss: 0.5384128358538446\n",
            "Iteration 50600, Loss: 0.5382184217370595\n",
            "Iteration 50700, Loss: 0.5380246994828536\n",
            "Iteration 50800, Loss: 0.5378316654548095\n",
            "Iteration 50900, Loss: 0.537639316042042\n",
            "Iteration 51000, Loss: 0.5374476476589752\n",
            "Iteration 51100, Loss: 0.53725665674512\n",
            "Iteration 51200, Loss: 0.5370663397648551\n",
            "Iteration 51300, Loss: 0.5368766932072095\n",
            "Iteration 51400, Loss: 0.5366877135856484\n",
            "Iteration 51500, Loss: 0.5364993974378599\n",
            "Iteration 51600, Loss: 0.5363117413255449\n",
            "Iteration 51700, Loss: 0.5361247418342091\n",
            "Iteration 51800, Loss: 0.5359383955729564\n",
            "Iteration 51900, Loss: 0.5357526991742861\n",
            "Iteration 52000, Loss: 0.53556764929389\n",
            "Iteration 52100, Loss: 0.5353832426104536\n",
            "Iteration 52200, Loss: 0.5351994758254571\n",
            "Iteration 52300, Loss: 0.5350163456629823\n",
            "Iteration 52400, Loss: 0.5348338488695159\n",
            "Iteration 52500, Loss: 0.5346519822137602\n",
            "Iteration 52600, Loss: 0.5344707424864428\n",
            "Iteration 52700, Loss: 0.5342901265001285\n",
            "Iteration 52800, Loss: 0.5341101310890342\n",
            "Iteration 52900, Loss: 0.5339307531088441\n",
            "Iteration 53000, Loss: 0.5337519894365287\n",
            "Iteration 53100, Loss: 0.5335738369701638\n",
            "Iteration 53200, Loss: 0.5333962926287528\n",
            "Iteration 53300, Loss: 0.5332193533520496\n",
            "Iteration 53400, Loss: 0.533043016100384\n",
            "Iteration 53500, Loss: 0.5328672778544891\n",
            "Iteration 53600, Loss: 0.5326921356153291\n",
            "Iteration 53700, Loss: 0.5325175864039313\n",
            "Iteration 53800, Loss: 0.5323436272612169\n",
            "Iteration 53900, Loss: 0.5321702552478351\n",
            "Iteration 54000, Loss: 0.531997467443999\n",
            "Iteration 54100, Loss: 0.5318252609493227\n",
            "Iteration 54200, Loss: 0.5316536328826598\n",
            "Iteration 54300, Loss: 0.5314825803819437\n",
            "Iteration 54400, Loss: 0.5313121006040303\n",
            "Iteration 54500, Loss: 0.5311421907245409\n",
            "Iteration 54600, Loss: 0.5309728479377072\n",
            "Iteration 54700, Loss: 0.530804069456218\n",
            "Iteration 54800, Loss: 0.5306358525110682\n",
            "Iteration 54900, Loss: 0.5304681943514067\n",
            "Iteration 55000, Loss: 0.5303010922443893\n",
            "Iteration 55100, Loss: 0.5301345434750303\n",
            "Iteration 55200, Loss: 0.5299685453460568\n",
            "Iteration 55300, Loss: 0.5298030951777631\n",
            "Iteration 55400, Loss: 0.5296381903078701\n",
            "Iteration 55500, Loss: 0.5294738280913807\n",
            "Iteration 55600, Loss: 0.5293100059004414\n",
            "Iteration 55700, Loss: 0.5291467211242022\n",
            "Iteration 55800, Loss: 0.5289839711686798\n",
            "Iteration 55900, Loss: 0.5288217534566203\n",
            "Iteration 56000, Loss: 0.5286600654273652\n",
            "Iteration 56100, Loss: 0.5284989045367173\n",
            "Iteration 56200, Loss: 0.528338268256808\n",
            "Iteration 56300, Loss: 0.5281781540759669\n",
            "Iteration 56400, Loss: 0.5280185594985912\n",
            "Iteration 56500, Loss: 0.5278594820450174\n",
            "Iteration 56600, Loss: 0.5277009192513941\n",
            "Iteration 56700, Loss: 0.5275428686695558\n",
            "Iteration 56800, Loss: 0.5273853278668975\n",
            "Iteration 56900, Loss: 0.5272282944262516\n",
            "Iteration 57000, Loss: 0.5270717659457648\n",
            "Iteration 57100, Loss: 0.526915740038777\n",
            "Iteration 57200, Loss: 0.5267602143337009\n",
            "Iteration 57300, Loss: 0.5266051864739031\n",
            "Iteration 57400, Loss: 0.5264506541175857\n",
            "Iteration 57500, Loss: 0.5262966149376698\n",
            "Iteration 57600, Loss: 0.5261430666216796\n",
            "Iteration 57700, Loss: 0.5259900068716277\n",
            "Iteration 57800, Loss: 0.5258374334039015\n",
            "Iteration 57900, Loss: 0.5256853439491509\n",
            "Iteration 58000, Loss: 0.5255337362521765\n",
            "Iteration 58100, Loss: 0.5253826080718191\n",
            "Iteration 58200, Loss: 0.5252319571808508\n",
            "Iteration 58300, Loss: 0.5250817813658663\n",
            "Iteration 58400, Loss: 0.5249320784271755\n",
            "Iteration 58500, Loss: 0.5247828461786972\n",
            "Iteration 58600, Loss: 0.5246340824478536\n",
            "Iteration 58700, Loss: 0.5244857850754664\n",
            "Iteration 58800, Loss: 0.5243379519156527\n",
            "Iteration 58900, Loss: 0.5241905808357228\n",
            "Iteration 59000, Loss: 0.5240436697160791\n",
            "Iteration 59100, Loss: 0.5238972164501146\n",
            "Iteration 59200, Loss: 0.5237512189441142\n",
            "Iteration 59300, Loss: 0.5236056751171553\n",
            "Iteration 59400, Loss: 0.52346058290101\n",
            "Iteration 59500, Loss: 0.523315940240049\n",
            "Iteration 59600, Loss: 0.523171745091144\n",
            "Iteration 59700, Loss: 0.5230279954235739\n",
            "Iteration 59800, Loss: 0.5228846892189302\n",
            "Iteration 59900, Loss: 0.5227418244710228\n",
            "Iteration 60000, Loss: 0.5225993991857883\n",
            "Iteration 60100, Loss: 0.5224574113811983\n",
            "Iteration 60200, Loss: 0.5223158590871669\n",
            "Iteration 60300, Loss: 0.5221747403454632\n",
            "Iteration 60400, Loss: 0.5220340532096196\n",
            "Iteration 60500, Loss: 0.5218937957448445\n",
            "Iteration 60600, Loss: 0.5217539660279348\n",
            "Iteration 60700, Loss: 0.5216145621471885\n",
            "Iteration 60800, Loss: 0.5214755822023184\n",
            "Iteration 60900, Loss: 0.5213370243043676\n",
            "Iteration 61000, Loss: 0.5211988865756242\n",
            "Iteration 61100, Loss: 0.5210611671495377\n",
            "Iteration 61200, Loss: 0.5209238641706362\n",
            "Iteration 61300, Loss: 0.5207869757944434\n",
            "Iteration 61400, Loss: 0.5206505001873979\n",
            "Iteration 61500, Loss: 0.5205144355267715\n",
            "Iteration 61600, Loss: 0.5203787800005897\n",
            "Iteration 61700, Loss: 0.520243531807552\n",
            "Iteration 61800, Loss: 0.5201086891569532\n",
            "Iteration 61900, Loss: 0.5199742502686051\n",
            "Iteration 62000, Loss: 0.5198402133727597\n",
            "Iteration 62100, Loss: 0.5197065767100322\n",
            "Iteration 62200, Loss: 0.5195733385313246\n",
            "Iteration 62300, Loss: 0.5194404970977516\n",
            "Iteration 62400, Loss: 0.5193080506805647\n",
            "Iteration 62500, Loss: 0.5191759975610788\n",
            "Iteration 62600, Loss: 0.5190443360305987\n",
            "Iteration 62700, Loss: 0.5189130643903466\n",
            "Iteration 62800, Loss: 0.5187821809513892\n",
            "Iteration 62900, Loss: 0.5186516840345679\n",
            "Iteration 63000, Loss: 0.5185215719704261\n",
            "Iteration 63100, Loss: 0.5183918430991404\n",
            "Iteration 63200, Loss: 0.51826249577045\n",
            "Iteration 63300, Loss: 0.5181335283435885\n",
            "Iteration 63400, Loss: 0.5180049391872152\n",
            "Iteration 63500, Loss: 0.5178767266793468\n",
            "Iteration 63600, Loss: 0.5177488892072911\n",
            "Iteration 63700, Loss: 0.5176214251675795\n",
            "Iteration 63800, Loss: 0.5174943329659015\n",
            "Iteration 63900, Loss: 0.5173676110170392\n",
            "Iteration 64000, Loss: 0.5172412577448017\n",
            "Iteration 64100, Loss: 0.5171152715819616\n",
            "Iteration 64200, Loss: 0.5169896509701902\n",
            "Iteration 64300, Loss: 0.5168643943599953\n",
            "Iteration 64400, Loss: 0.5167395002106577\n",
            "Iteration 64500, Loss: 0.5166149669901693\n",
            "Iteration 64600, Loss: 0.516490793175171\n",
            "Iteration 64700, Loss: 0.516366977250892\n",
            "Iteration 64800, Loss: 0.516243517711089\n",
            "Iteration 64900, Loss: 0.5161204130579862\n",
            "Iteration 65000, Loss: 0.5159976618022155\n",
            "Iteration 65100, Loss: 0.5158752624627571\n",
            "Iteration 65200, Loss: 0.5157532135668819\n",
            "Iteration 65300, Loss: 0.5156315136500921\n",
            "Iteration 65400, Loss: 0.5155101612560641\n",
            "Iteration 65500, Loss: 0.5153891549365922\n",
            "Iteration 65600, Loss: 0.5152684932515303\n",
            "Iteration 65700, Loss: 0.5151481747687371\n",
            "Iteration 65800, Loss: 0.5150281980640193\n",
            "Iteration 65900, Loss: 0.5149085617210777\n",
            "Iteration 66000, Loss: 0.5147892643314504\n",
            "Iteration 66100, Loss: 0.514670304494461\n",
            "Iteration 66200, Loss: 0.5145516808171622\n",
            "Iteration 66300, Loss: 0.5144333919142838\n",
            "Iteration 66400, Loss: 0.5143154364081799\n",
            "Iteration 66500, Loss: 0.5141978129287752\n",
            "Iteration 66600, Loss: 0.5140805201135138\n",
            "Iteration 66700, Loss: 0.5139635566073073\n",
            "Iteration 66800, Loss: 0.5138469210624834\n",
            "Iteration 66900, Loss: 0.5137306121387349\n",
            "Iteration 67000, Loss: 0.5136146285030697\n",
            "Iteration 67100, Loss: 0.5134989688297605\n",
            "Iteration 67200, Loss: 0.5133836318002954\n",
            "Iteration 67300, Loss: 0.5132686161033284\n",
            "Iteration 67400, Loss: 0.5131539204346308\n",
            "Iteration 67500, Loss: 0.513039543497043\n",
            "Iteration 67600, Loss: 0.5129254840004259\n",
            "Iteration 67700, Loss: 0.512811740661614\n",
            "Iteration 67800, Loss: 0.5126983122043678\n",
            "Iteration 67900, Loss: 0.5125851973593268\n",
            "Iteration 68000, Loss: 0.5124723948639639\n",
            "Iteration 68100, Loss: 0.512359903462538\n",
            "Iteration 68200, Loss: 0.5122477219060493\n",
            "Iteration 68300, Loss: 0.5121358489521936\n",
            "Iteration 68400, Loss: 0.5120242833653176\n",
            "Iteration 68500, Loss: 0.5119130239163734\n",
            "Iteration 68600, Loss: 0.5118020693828756\n",
            "Iteration 68700, Loss: 0.5116914185488567\n",
            "Iteration 68800, Loss: 0.5115810702048231\n",
            "Iteration 68900, Loss: 0.5114710231477131\n",
            "Iteration 69000, Loss: 0.5113612761808526\n",
            "Iteration 69100, Loss: 0.5112518281139135\n",
            "Iteration 69200, Loss: 0.5111426777628719\n",
            "Iteration 69300, Loss: 0.5110338239499654\n",
            "Iteration 69400, Loss: 0.5109252655036521\n",
            "Iteration 69500, Loss: 0.510817001258569\n",
            "Iteration 69600, Loss: 0.5107090300554917\n",
            "Iteration 69700, Loss: 0.5106013507412935\n",
            "Iteration 69800, Loss: 0.510493962168906\n",
            "Iteration 69900, Loss: 0.5103868631972771\n",
            "Iteration 70000, Loss: 0.5102800526913347\n",
            "Iteration 70100, Loss: 0.5101735295219447\n",
            "Iteration 70200, Loss: 0.5100672925658731\n",
            "Iteration 70300, Loss: 0.5099613407057474\n",
            "Iteration 70400, Loss: 0.5098556728300183\n",
            "Iteration 70500, Loss: 0.5097502878329215\n",
            "Iteration 70600, Loss: 0.50964518461444\n",
            "Iteration 70700, Loss: 0.5095403620802664\n",
            "Iteration 70800, Loss: 0.5094358191417667\n",
            "Iteration 70900, Loss: 0.5093315547159426\n",
            "Iteration 71000, Loss: 0.5092275677253947\n",
            "Iteration 71100, Loss: 0.5091238570982872\n",
            "Iteration 71200, Loss: 0.5090204217683111\n",
            "Iteration 71300, Loss: 0.5089172606746489\n",
            "Iteration 71400, Loss: 0.508814372761939\n",
            "Iteration 71500, Loss: 0.5087117569802404\n",
            "Iteration 71600, Loss: 0.5086094122849982\n",
            "Iteration 71700, Loss: 0.5085073376370088\n",
            "Iteration 71800, Loss: 0.5084055320023851\n",
            "Iteration 71900, Loss: 0.5083039943525234\n",
            "Iteration 72000, Loss: 0.5082027236640684\n",
            "Iteration 72100, Loss: 0.5081017189188802\n",
            "Iteration 72200, Loss: 0.5080009791040018\n",
            "Iteration 72300, Loss: 0.5079005032116243\n",
            "Iteration 72400, Loss: 0.5078002902390555\n",
            "Iteration 72500, Loss: 0.507700339188687\n",
            "Iteration 72600, Loss: 0.5076006490679614\n",
            "Iteration 72700, Loss: 0.5075012188893411\n",
            "Iteration 72800, Loss: 0.5074020476702757\n",
            "Iteration 72900, Loss: 0.507303134433171\n",
            "Iteration 73000, Loss: 0.5072044782053572\n",
            "Iteration 73100, Loss: 0.5071060780190584\n",
            "Iteration 73200, Loss: 0.5070079329113608\n",
            "Iteration 73300, Loss: 0.5069100419241833\n",
            "Iteration 73400, Loss: 0.5068124041042461\n",
            "Iteration 73500, Loss: 0.5067150185030409\n",
            "Iteration 73600, Loss: 0.5066178841768016\n",
            "Iteration 73700, Loss: 0.506521000186473\n",
            "Iteration 73800, Loss: 0.506424365597683\n",
            "Iteration 73900, Loss: 0.5063279794807125\n",
            "Iteration 74000, Loss: 0.5062318409104662\n",
            "Iteration 74100, Loss: 0.5061359489664449\n",
            "Iteration 74200, Loss: 0.5060403027327148\n",
            "Iteration 74300, Loss: 0.5059449012978815\n",
            "Iteration 74400, Loss: 0.5058497437550601\n",
            "Iteration 74500, Loss: 0.5057548292018481\n",
            "Iteration 74600, Loss: 0.5056601567402972\n",
            "Iteration 74700, Loss: 0.5055657254768855\n",
            "Iteration 74800, Loss: 0.5054715345224915\n",
            "Iteration 74900, Loss: 0.5053775829923651\n",
            "Iteration 75000, Loss: 0.5052838700061021\n",
            "Iteration 75100, Loss: 0.505190394687617\n",
            "Iteration 75200, Loss: 0.5050971561651151\n",
            "Iteration 75300, Loss: 0.5050041535710692\n",
            "Iteration 75400, Loss: 0.5049113860421903\n",
            "Iteration 75500, Loss: 0.5048188527194036\n",
            "Iteration 75600, Loss: 0.5047265527478222\n",
            "Iteration 75700, Loss: 0.5046344852767204\n",
            "Iteration 75800, Loss: 0.5045426494595106\n",
            "Iteration 75900, Loss: 0.5044510444537161\n",
            "Iteration 76000, Loss: 0.5043596694209468\n",
            "Iteration 76100, Loss: 0.5042685235268742\n",
            "Iteration 76200, Loss: 0.5041776059412072\n",
            "Iteration 76300, Loss: 0.5040869158376673\n",
            "Iteration 76400, Loss: 0.5039964523939638\n",
            "Iteration 76500, Loss: 0.5039062147917702\n",
            "Iteration 76600, Loss: 0.5038162022167008\n",
            "Iteration 76700, Loss: 0.5037264138582856\n",
            "Iteration 76800, Loss: 0.5036368489099475\n",
            "Iteration 76900, Loss: 0.5035475065689792\n",
            "Iteration 77000, Loss: 0.503458386036519\n",
            "Iteration 77100, Loss: 0.5033694865175287\n",
            "Iteration 77200, Loss: 0.5032808072207695\n",
            "Iteration 77300, Loss: 0.5031923473587803\n",
            "Iteration 77400, Loss: 0.5031041061478547\n",
            "Iteration 77500, Loss: 0.5030160828080185\n",
            "Iteration 77600, Loss: 0.5029282765630072\n",
            "Iteration 77700, Loss: 0.5028406866402445\n",
            "Iteration 77800, Loss: 0.50275331227082\n",
            "Iteration 77900, Loss: 0.5026661526894672\n",
            "Iteration 78000, Loss: 0.5025792071345422\n",
            "Iteration 78100, Loss: 0.5024924748480015\n",
            "Iteration 78200, Loss: 0.5024059550753819\n",
            "Iteration 78300, Loss: 0.5023196470657786\n",
            "Iteration 78400, Loss: 0.5022335500718236\n",
            "Iteration 78500, Loss: 0.5021476633496658\n",
            "Iteration 78600, Loss: 0.5020619861589497\n",
            "Iteration 78700, Loss: 0.5019765177627953\n",
            "Iteration 78800, Loss: 0.5018912574277768\n",
            "Iteration 78900, Loss: 0.5018062044239031\n",
            "Iteration 79000, Loss: 0.5017213580245973\n",
            "Iteration 79100, Loss: 0.5016367175066766\n",
            "Iteration 79200, Loss: 0.5015522821503327\n",
            "Iteration 79300, Loss: 0.501468051239112\n",
            "Iteration 79400, Loss: 0.5013840240598956\n",
            "Iteration 79500, Loss: 0.5013001999028807\n",
            "Iteration 79600, Loss: 0.5012165780615601\n",
            "Iteration 79700, Loss: 0.501133157832704\n",
            "Iteration 79800, Loss: 0.5010499385163405\n",
            "Iteration 79900, Loss: 0.5009669194157368\n",
            "Iteration 80000, Loss: 0.5008840998373808\n",
            "Iteration 80100, Loss: 0.5008014790909611\n",
            "Iteration 80200, Loss: 0.5007190564893501\n",
            "Iteration 80300, Loss: 0.5006368313485848\n",
            "Iteration 80400, Loss: 0.5005548029878487\n",
            "Iteration 80500, Loss: 0.500472970729453\n",
            "Iteration 80600, Loss: 0.5003913338988198\n",
            "Iteration 80700, Loss: 0.5003098918244635\n",
            "Iteration 80800, Loss: 0.5002286438379729\n",
            "Iteration 80900, Loss: 0.5001475892739936\n",
            "Iteration 81000, Loss: 0.500066727470211\n",
            "Iteration 81100, Loss: 0.4999860577673326\n",
            "Iteration 81200, Loss: 0.4999055795090701\n",
            "Iteration 81300, Loss: 0.49982529204212317\n",
            "Iteration 81400, Loss: 0.499745194716162\n",
            "Iteration 81500, Loss: 0.4996652868838101\n",
            "Iteration 81600, Loss: 0.4995855679006283\n",
            "Iteration 81700, Loss: 0.4995060371250965\n",
            "Iteration 81800, Loss: 0.499426693918599\n",
            "Iteration 81900, Loss: 0.49934753764540707\n",
            "Iteration 82000, Loss: 0.49926856767266226\n",
            "Iteration 82100, Loss: 0.49918978337036046\n",
            "Iteration 82200, Loss: 0.4991111841113363\n",
            "Iteration 82300, Loss: 0.49903276927124623\n",
            "Iteration 82400, Loss: 0.4989545382285531\n",
            "Iteration 82500, Loss: 0.49887649036451026\n",
            "Iteration 82600, Loss: 0.4987986250631461\n",
            "Iteration 82700, Loss: 0.49872094171124765\n",
            "Iteration 82800, Loss: 0.4986434396983465\n",
            "Iteration 82900, Loss: 0.4985661184167016\n",
            "Iteration 83000, Loss: 0.4984889772612855\n",
            "Iteration 83100, Loss: 0.49841201562976867\n",
            "Iteration 83200, Loss: 0.49833523292250387\n",
            "Iteration 83300, Loss: 0.49825862854251196\n",
            "Iteration 83400, Loss: 0.49818220189546686\n",
            "Iteration 83500, Loss: 0.4981059523896801\n",
            "Iteration 83600, Loss: 0.4980298794360872\n",
            "Iteration 83700, Loss: 0.49795398244823225\n",
            "Iteration 83800, Loss: 0.49787826084225384\n",
            "Iteration 83900, Loss: 0.49780271403687026\n",
            "Iteration 84000, Loss: 0.4977273414533661\n",
            "Iteration 84100, Loss: 0.49765214251557666\n",
            "Iteration 84200, Loss: 0.4975771166498755\n",
            "Iteration 84300, Loss: 0.4975022632851589\n",
            "Iteration 84400, Loss: 0.49742758185283303\n",
            "Iteration 84500, Loss: 0.4973530717867991\n",
            "Iteration 84600, Loss: 0.49727873252344124\n",
            "Iteration 84700, Loss: 0.49720456350161096\n",
            "Iteration 84800, Loss: 0.497130564162615\n",
            "Iteration 84900, Loss: 0.49705673395020117\n",
            "Iteration 85000, Loss: 0.4969830723105451\n",
            "Iteration 85100, Loss: 0.49690957869223723\n",
            "Iteration 85200, Loss: 0.4968362525462694\n",
            "Iteration 85300, Loss: 0.4967630933260216\n",
            "Iteration 85400, Loss: 0.4966901004872492\n",
            "Iteration 85500, Loss: 0.4966172734880699\n",
            "Iteration 85600, Loss: 0.4965446117889508\n",
            "Iteration 85700, Loss: 0.4964721148526957\n",
            "Iteration 85800, Loss: 0.49639978214443287\n",
            "Iteration 85900, Loss: 0.4963276131316013\n",
            "Iteration 86000, Loss: 0.49625560728393947\n",
            "Iteration 86100, Loss: 0.49618376407347164\n",
            "Iteration 86200, Loss: 0.4961120829744971\n",
            "Iteration 86300, Loss: 0.4960405634635763\n",
            "Iteration 86400, Loss: 0.49596920501951924\n",
            "Iteration 86500, Loss: 0.4958980071233738\n",
            "Iteration 86600, Loss: 0.4958269692584132\n",
            "Iteration 86700, Loss: 0.4957560909101242\n",
            "Iteration 86800, Loss: 0.495685371566195\n",
            "Iteration 86900, Loss: 0.49561481071650376\n",
            "Iteration 87000, Loss: 0.49554440785310644\n",
            "Iteration 87100, Loss: 0.4954741624702257\n",
            "Iteration 87200, Loss: 0.49540407406423914\n",
            "Iteration 87300, Loss: 0.49533414213366705\n",
            "Iteration 87400, Loss: 0.4952643661791625\n",
            "Iteration 87500, Loss: 0.49519474570349814\n",
            "Iteration 87600, Loss: 0.49512528021155655\n",
            "Iteration 87700, Loss: 0.495055969210318\n",
            "Iteration 87800, Loss: 0.49498681220884966\n",
            "Iteration 87900, Loss: 0.49491780871829455\n",
            "Iteration 88000, Loss: 0.49484895825186037\n",
            "Iteration 88100, Loss: 0.49478026032480893\n",
            "Iteration 88200, Loss: 0.49471171445444473\n",
            "Iteration 88300, Loss: 0.4946433201601048\n",
            "Iteration 88400, Loss: 0.4945750769631472\n",
            "Iteration 88500, Loss: 0.494506984386941\n",
            "Iteration 88600, Loss: 0.494439041956856\n",
            "Iteration 88700, Loss: 0.494371249200251\n",
            "Iteration 88800, Loss: 0.49430360564646403\n",
            "Iteration 88900, Loss: 0.49423611082680285\n",
            "Iteration 89000, Loss: 0.49416876427453266\n",
            "Iteration 89100, Loss: 0.4941015655248676\n",
            "Iteration 89200, Loss: 0.4940345141149598\n",
            "Iteration 89300, Loss: 0.49396760958388913\n",
            "Iteration 89400, Loss: 0.4939008514726535\n",
            "Iteration 89500, Loss: 0.4938342393241586\n",
            "Iteration 89600, Loss: 0.4937677726832084\n",
            "Iteration 89700, Loss: 0.4937014510964942\n",
            "Iteration 89800, Loss: 0.49363527411258645\n",
            "Iteration 89900, Loss: 0.49356924128192325\n",
            "Iteration 90000, Loss: 0.4935033521568021\n",
            "Iteration 90100, Loss: 0.49343760629136907\n",
            "Iteration 90200, Loss: 0.49337200324161046\n",
            "Iteration 90300, Loss: 0.4933065425653421\n",
            "Iteration 90400, Loss: 0.4932412238222008\n",
            "Iteration 90500, Loss: 0.4931760465736343\n",
            "Iteration 90600, Loss: 0.49311101038289246\n",
            "Iteration 90700, Loss: 0.4930461148150174\n",
            "Iteration 90800, Loss: 0.49298135943683513\n",
            "Iteration 90900, Loss: 0.4929167438169453\n",
            "Iteration 91000, Loss: 0.49285226752571276\n",
            "Iteration 91100, Loss: 0.49278793013525873\n",
            "Iteration 91200, Loss: 0.49272373121945123\n",
            "Iteration 91300, Loss: 0.4926596703538965\n",
            "Iteration 91400, Loss: 0.4925957471159298\n",
            "Iteration 91500, Loss: 0.4925319610846069\n",
            "Iteration 91600, Loss: 0.4924683118406953\n",
            "Iteration 91700, Loss: 0.49240479896666517\n",
            "Iteration 91800, Loss: 0.4923414220466811\n",
            "Iteration 91900, Loss: 0.4922781806665927\n",
            "Iteration 92000, Loss: 0.4922150744139274\n",
            "Iteration 92100, Loss: 0.4921521028778806\n",
            "Iteration 92200, Loss: 0.4920892656493081\n",
            "Iteration 92300, Loss: 0.492026562320717\n",
            "Iteration 92400, Loss: 0.4919639924862579\n",
            "Iteration 92500, Loss: 0.49190155574171635\n",
            "Iteration 92600, Loss: 0.4918392516845045\n",
            "Iteration 92700, Loss: 0.4917770799136532\n",
            "Iteration 92800, Loss: 0.4917150400298034\n",
            "Iteration 92900, Loss: 0.49165313163519875\n",
            "Iteration 93000, Loss: 0.4915913543336765\n",
            "Iteration 93100, Loss: 0.49152970773066074\n",
            "Iteration 93200, Loss: 0.4914681914331531\n",
            "Iteration 93300, Loss: 0.4914068050497262\n",
            "Iteration 93400, Loss: 0.49134554819051446\n",
            "Iteration 93500, Loss: 0.4912844204672072\n",
            "Iteration 93600, Loss: 0.49122342149304077\n",
            "Iteration 93700, Loss: 0.49116255088279054\n",
            "Iteration 93800, Loss: 0.49110180825276306\n",
            "Iteration 93900, Loss: 0.49104119322078904\n",
            "Iteration 94000, Loss: 0.4909807054062152\n",
            "Iteration 94100, Loss: 0.49092034442989735\n",
            "Iteration 94200, Loss: 0.49086010991419204\n",
            "Iteration 94300, Loss: 0.4908000014829497\n",
            "Iteration 94400, Loss: 0.4907400187615072\n",
            "Iteration 94500, Loss: 0.49068016137668025\n",
            "Iteration 94600, Loss: 0.49062042895675645\n",
            "Iteration 94700, Loss: 0.4905608211314874\n",
            "Iteration 94800, Loss: 0.49050133753208236\n",
            "Iteration 94900, Loss: 0.49044197779120025\n",
            "Iteration 95000, Loss: 0.4903827415429426\n",
            "Iteration 95100, Loss: 0.4903236284228471\n",
            "Iteration 95200, Loss: 0.49026463806787995\n",
            "Iteration 95300, Loss: 0.49020577011642885\n",
            "Iteration 95400, Loss: 0.49014702420829614\n",
            "Iteration 95500, Loss: 0.4900883999846922\n",
            "Iteration 95600, Loss: 0.490029897088228\n",
            "Iteration 95700, Loss: 0.4899715151629083\n",
            "Iteration 95800, Loss: 0.48991325385412543\n",
            "Iteration 95900, Loss: 0.4898551128086518\n",
            "Iteration 96000, Loss: 0.4897970916746335\n",
            "Iteration 96100, Loss: 0.48973919010158373\n",
            "Iteration 96200, Loss: 0.48968140774037555\n",
            "Iteration 96300, Loss: 0.4896237442432364\n",
            "Iteration 96400, Loss: 0.48956619926374023\n",
            "Iteration 96500, Loss: 0.48950877245680174\n",
            "Iteration 96600, Loss: 0.4894514634786699\n",
            "Iteration 96700, Loss: 0.4893942719869207\n",
            "Iteration 96800, Loss: 0.4893371976404517\n",
            "Iteration 96900, Loss: 0.48928024009947546\n",
            "Iteration 97000, Loss: 0.4892233990255124\n",
            "Iteration 97100, Loss: 0.4891666740813852\n",
            "Iteration 97200, Loss: 0.4891100649312128\n",
            "Iteration 97300, Loss: 0.48905357124040316\n",
            "Iteration 97400, Loss: 0.4889971926756482\n",
            "Iteration 97500, Loss: 0.48894092890491625\n",
            "Iteration 97600, Loss: 0.4888847795974474\n",
            "Iteration 97700, Loss: 0.48882874442374646\n",
            "Iteration 97800, Loss: 0.48877282305557745\n",
            "Iteration 97900, Loss: 0.4887170151659566\n",
            "Iteration 98000, Loss: 0.4886613204291476\n",
            "Iteration 98100, Loss: 0.4886057385206547\n",
            "Iteration 98200, Loss: 0.4885502691172179\n",
            "Iteration 98300, Loss: 0.48849491189680516\n",
            "Iteration 98400, Loss: 0.48843966653860865\n",
            "Iteration 98500, Loss: 0.48838453272303756\n",
            "Iteration 98600, Loss: 0.4883295101317128\n",
            "Iteration 98700, Loss: 0.48827459844746096\n",
            "Iteration 98800, Loss: 0.48821979735430915\n",
            "Iteration 98900, Loss: 0.4881651065374788\n",
            "Iteration 99000, Loss: 0.48811052568338004\n",
            "Iteration 99100, Loss: 0.48805605447960626\n",
            "Iteration 99200, Loss: 0.4880016926149285\n",
            "Iteration 99300, Loss: 0.48794743977928967\n",
            "Iteration 99400, Loss: 0.48789329566379935\n",
            "Iteration 99500, Loss: 0.48783925996072797\n",
            "Iteration 99600, Loss: 0.48778533236350125\n",
            "Iteration 99700, Loss: 0.487731512566696\n",
            "Iteration 99800, Loss: 0.4876778002660322\n",
            "Iteration 99900, Loss: 0.4876241951583703\n",
            "Iteration 100000, Loss: 0.4875706969417042\n",
            "Iteration 100100, Loss: 0.4875173053151567\n",
            "Iteration 100200, Loss: 0.48746401997897365\n",
            "Iteration 100300, Loss: 0.487410840634519\n",
            "Iteration 100400, Loss: 0.48735776698427014\n",
            "Iteration 100500, Loss: 0.48730479873181126\n",
            "Iteration 100600, Loss: 0.48725193558182955\n",
            "Iteration 100700, Loss: 0.48719917724010964\n",
            "Iteration 100800, Loss: 0.4871465234135284\n",
            "Iteration 100900, Loss: 0.48709397381004926\n",
            "Iteration 101000, Loss: 0.4870415281387188\n",
            "Iteration 101100, Loss: 0.4869891861096598\n",
            "Iteration 101200, Loss: 0.48693694743406735\n",
            "Iteration 101300, Loss: 0.4868848118242042\n",
            "Iteration 101400, Loss: 0.4868327789933945\n",
            "Iteration 101500, Loss: 0.4867808486560202\n",
            "Iteration 101600, Loss: 0.48672902052751543\n",
            "Iteration 101700, Loss: 0.48667729432436196\n",
            "Iteration 101800, Loss: 0.48662566976408383\n",
            "Iteration 101900, Loss: 0.4865741465652438\n",
            "Iteration 102000, Loss: 0.48652272444743705\n",
            "Iteration 102100, Loss: 0.4864714031312871\n",
            "Iteration 102200, Loss: 0.4864201823384415\n",
            "Iteration 102300, Loss: 0.4863690617915662\n",
            "Iteration 102400, Loss: 0.48631804121434236\n",
            "Iteration 102500, Loss: 0.4862671203314597\n",
            "Iteration 102600, Loss: 0.4862162988686134\n",
            "Iteration 102700, Loss: 0.48616557655249865\n",
            "Iteration 102800, Loss: 0.48611495311080744\n",
            "Iteration 102900, Loss: 0.48606442827222174\n",
            "Iteration 103000, Loss: 0.4860140017664111\n",
            "Iteration 103100, Loss: 0.4859636733240271\n",
            "Iteration 103200, Loss: 0.48591344267669917\n",
            "Iteration 103300, Loss: 0.4858633095570296\n",
            "Iteration 103400, Loss: 0.4858132736985905\n",
            "Iteration 103500, Loss: 0.48576333483591744\n",
            "Iteration 103600, Loss: 0.485713492704507\n",
            "Iteration 103700, Loss: 0.4856637470408107\n",
            "Iteration 103800, Loss: 0.48561409758223245\n",
            "Iteration 103900, Loss: 0.4855645440671224\n",
            "Iteration 104000, Loss: 0.4855150862347742\n",
            "Iteration 104100, Loss: 0.48546572382541986\n",
            "Iteration 104200, Loss: 0.48541645658022575\n",
            "Iteration 104300, Loss: 0.4853672842412884\n",
            "Iteration 104400, Loss: 0.4853182065516304\n",
            "Iteration 104500, Loss: 0.48526922325519606\n",
            "Iteration 104600, Loss: 0.4852203340968473\n",
            "Iteration 104700, Loss: 0.4851715388223598\n",
            "Iteration 104800, Loss: 0.4851228371784185\n",
            "Iteration 104900, Loss: 0.4850742289126134\n",
            "Iteration 105000, Loss: 0.48502571377343673\n",
            "Iteration 105100, Loss: 0.484977291510277\n",
            "Iteration 105200, Loss: 0.4849289618734165\n",
            "Iteration 105300, Loss: 0.48488072461402665\n",
            "Iteration 105400, Loss: 0.4848325794841644\n",
            "Iteration 105500, Loss: 0.4847845262367678\n",
            "Iteration 105600, Loss: 0.4847365646256525\n",
            "Iteration 105700, Loss: 0.4846886944055075\n",
            "Iteration 105800, Loss: 0.48464091533189146\n",
            "Iteration 105900, Loss: 0.484593227161229\n",
            "Iteration 106000, Loss: 0.48454562965080666\n",
            "Iteration 106100, Loss: 0.484498122558769\n",
            "Iteration 106200, Loss: 0.4844507056441149\n",
            "Iteration 106300, Loss: 0.4844033786666939\n",
            "Iteration 106400, Loss: 0.48435614138720223\n",
            "Iteration 106500, Loss: 0.48430899356717916\n",
            "Iteration 106600, Loss: 0.48426193496900327\n",
            "Iteration 106700, Loss: 0.48421496535588865\n",
            "Iteration 106800, Loss: 0.48416808449188153\n",
            "Iteration 106900, Loss: 0.48412129214185634\n",
            "Iteration 107000, Loss: 0.484074588071512\n",
            "Iteration 107100, Loss: 0.4840279720473687\n",
            "Iteration 107200, Loss: 0.4839814438367636\n",
            "Iteration 107300, Loss: 0.4839350032078482\n",
            "Iteration 107400, Loss: 0.4838886499295839\n",
            "Iteration 107500, Loss: 0.48384238377173877\n",
            "Iteration 107600, Loss: 0.4837962045048844\n",
            "Iteration 107700, Loss: 0.4837501119003917\n",
            "Iteration 107800, Loss: 0.48370410573042766\n",
            "Iteration 107900, Loss: 0.4836581857679525\n",
            "Iteration 108000, Loss: 0.4836123517867151\n",
            "Iteration 108100, Loss: 0.4835666035612503\n",
            "Iteration 108200, Loss: 0.4835209408668757\n",
            "Iteration 108300, Loss: 0.4834753634796875\n",
            "Iteration 108400, Loss: 0.48342987117655734\n",
            "Iteration 108500, Loss: 0.48338446373512944\n",
            "Iteration 108600, Loss: 0.48333914093381697\n",
            "Iteration 108700, Loss: 0.48329390255179805\n",
            "Iteration 108800, Loss: 0.48324874836901394\n",
            "Iteration 108900, Loss: 0.48320367816616416\n",
            "Iteration 109000, Loss: 0.483158691724704\n",
            "Iteration 109100, Loss: 0.48311378882684136\n",
            "Iteration 109200, Loss: 0.483068969255533\n",
            "Iteration 109300, Loss: 0.4830242327944822\n",
            "Iteration 109400, Loss: 0.4829795792281341\n",
            "Iteration 109500, Loss: 0.482935008341674\n",
            "Iteration 109600, Loss: 0.4828905199210232\n",
            "Iteration 109700, Loss: 0.4828461137528366\n",
            "Iteration 109800, Loss: 0.4828017896244987\n",
            "Iteration 109900, Loss: 0.482757547324121\n",
            "Iteration 110000, Loss: 0.48271338664053887\n",
            "Iteration 110100, Loss: 0.48266930736330843\n",
            "Iteration 110200, Loss: 0.4826253092827035\n",
            "Iteration 110300, Loss: 0.4825813921897119\n",
            "Iteration 110400, Loss: 0.48253755587603353\n",
            "Iteration 110500, Loss: 0.48249380013407683\n",
            "Iteration 110600, Loss: 0.482450124756955\n",
            "Iteration 110700, Loss: 0.4824065295384844\n",
            "Iteration 110800, Loss: 0.4823630142731803\n",
            "Iteration 110900, Loss: 0.4823195787562551\n",
            "Iteration 111000, Loss: 0.4822762227836139\n",
            "Iteration 111100, Loss: 0.482232946151853\n",
            "Iteration 111200, Loss: 0.4821897486582562\n",
            "Iteration 111300, Loss: 0.48214663010079184\n",
            "Iteration 111400, Loss: 0.48210359027811034\n",
            "Iteration 111500, Loss: 0.4820606289895413\n",
            "Iteration 111600, Loss: 0.48201774603508957\n",
            "Iteration 111700, Loss: 0.4819749412154344\n",
            "Iteration 111800, Loss: 0.4819322143319245\n",
            "Iteration 111900, Loss: 0.48188956518657666\n",
            "Iteration 112000, Loss: 0.4818469935820723\n",
            "Iteration 112100, Loss: 0.4818044993217548\n",
            "Iteration 112200, Loss: 0.48176208220962646\n",
            "Iteration 112300, Loss: 0.48171974205034673\n",
            "Iteration 112400, Loss: 0.481677478649228\n",
            "Iteration 112500, Loss: 0.48163529181223413\n",
            "Iteration 112600, Loss: 0.4815931813459766\n",
            "Iteration 112700, Loss: 0.48155114705771296\n",
            "Iteration 112800, Loss: 0.48150918875534315\n",
            "Iteration 112900, Loss: 0.4814673062474077\n",
            "Iteration 113000, Loss: 0.48142549934308376\n",
            "Iteration 113100, Loss: 0.4813837678521841\n",
            "Iteration 113200, Loss: 0.4813421115851533\n",
            "Iteration 113300, Loss: 0.4813005303530652\n",
            "Iteration 113400, Loss: 0.4812590239676208\n",
            "Iteration 113500, Loss: 0.4812175922411456\n",
            "Iteration 113600, Loss: 0.48117623498658624\n",
            "Iteration 113700, Loss: 0.4811349520175089\n",
            "Iteration 113800, Loss: 0.4810937431480959\n",
            "Iteration 113900, Loss: 0.481052608193144\n",
            "Iteration 114000, Loss: 0.4810115469680611\n",
            "Iteration 114100, Loss: 0.48097055928886434\n",
            "Iteration 114200, Loss: 0.48092964497217683\n",
            "Iteration 114300, Loss: 0.4808888038352258\n",
            "Iteration 114400, Loss: 0.48084803569583984\n",
            "Iteration 114500, Loss: 0.48080734037244677\n",
            "Iteration 114600, Loss: 0.48076671768407037\n",
            "Iteration 114700, Loss: 0.4807261674503286\n",
            "Iteration 114800, Loss: 0.48068568949143153\n",
            "Iteration 114900, Loss: 0.4806452836281772\n",
            "Iteration 115000, Loss: 0.4806049496819517\n",
            "Iteration 115100, Loss: 0.4805646874747245\n",
            "Iteration 115200, Loss: 0.4805244968290473\n",
            "Iteration 115300, Loss: 0.48048437756805146\n",
            "Iteration 115400, Loss: 0.4804443295154454\n",
            "Iteration 115500, Loss: 0.48040435249551217\n",
            "Iteration 115600, Loss: 0.48036444633310754\n",
            "Iteration 115700, Loss: 0.4803246108536575\n",
            "Iteration 115800, Loss: 0.4802848458831555\n",
            "Iteration 115900, Loss: 0.4802451512481607\n",
            "Iteration 116000, Loss: 0.4802055267757953\n",
            "Iteration 116100, Loss: 0.48016597229374264\n",
            "Iteration 116200, Loss: 0.48012648763024457\n",
            "Iteration 116300, Loss: 0.4800870726140991\n",
            "Iteration 116400, Loss: 0.4800477270746586\n",
            "Iteration 116500, Loss: 0.48000845084182714\n",
            "Iteration 116600, Loss: 0.47996924374605854\n",
            "Iteration 116700, Loss: 0.4799301056183538\n",
            "Iteration 116800, Loss: 0.4798910362902595\n",
            "Iteration 116900, Loss: 0.4798520355938646\n",
            "Iteration 117000, Loss: 0.4798131033617994\n",
            "Iteration 117100, Loss: 0.4797742394272324\n",
            "Iteration 117200, Loss: 0.47973544362386883\n",
            "Iteration 117300, Loss: 0.4796967157859481\n",
            "Iteration 117400, Loss: 0.47965805574824155\n",
            "Iteration 117500, Loss: 0.479619463346051\n",
            "Iteration 117600, Loss: 0.47958093841520544\n",
            "Iteration 117700, Loss: 0.47954248079206024\n",
            "Iteration 117800, Loss: 0.47950409031349395\n",
            "Iteration 117900, Loss: 0.47946576681690695\n",
            "Iteration 118000, Loss: 0.4794275101402189\n",
            "Iteration 118100, Loss: 0.4793893201218669\n",
            "Iteration 118200, Loss: 0.4793511966008033\n",
            "Iteration 118300, Loss: 0.4793131394164936\n",
            "Iteration 118400, Loss: 0.47927514840891466\n",
            "Iteration 118500, Loss: 0.4792372234185527\n",
            "Iteration 118600, Loss: 0.47919936428640036\n",
            "Iteration 118700, Loss: 0.47916157085395633\n",
            "Iteration 118800, Loss: 0.4791238429632215\n",
            "Iteration 118900, Loss: 0.4790861804566984\n",
            "Iteration 119000, Loss: 0.4790485831773886\n",
            "Iteration 119100, Loss: 0.47901105096879065\n",
            "Iteration 119200, Loss: 0.47897358367489845\n",
            "Iteration 119300, Loss: 0.47893618114019887\n",
            "Iteration 119400, Loss: 0.4788988432096704\n",
            "Iteration 119500, Loss: 0.4788615697287803\n",
            "Iteration 119600, Loss: 0.47882436054348365\n",
            "Iteration 119700, Loss: 0.4787872155002208\n",
            "Iteration 119800, Loss: 0.4787501344459155\n",
            "Iteration 119900, Loss: 0.47871311722797344\n",
            "Iteration 120000, Loss: 0.47867616369427984\n",
            "Iteration 120100, Loss: 0.4786392736931977\n",
            "Iteration 120200, Loss: 0.47860244707356614\n",
            "Iteration 120300, Loss: 0.4785656836846983\n",
            "Iteration 120400, Loss: 0.4785289833763795\n",
            "Iteration 120500, Loss: 0.4784923459988655\n",
            "Iteration 120600, Loss: 0.47845577140288054\n",
            "Iteration 120700, Loss: 0.4784192594396155\n",
            "Iteration 120800, Loss: 0.47838280996072635\n",
            "Iteration 120900, Loss: 0.47834642281833206\n",
            "Iteration 121000, Loss: 0.4783100978650125\n",
            "Iteration 121100, Loss: 0.4782738349538074\n",
            "Iteration 121200, Loss: 0.47823763393821395\n",
            "Iteration 121300, Loss: 0.47820149467218526\n",
            "Iteration 121400, Loss: 0.4781654170101284\n",
            "Iteration 121500, Loss: 0.4781294008069029\n",
            "Iteration 121600, Loss: 0.47809344591781877\n",
            "Iteration 121700, Loss: 0.4780575521986351\n",
            "Iteration 121800, Loss: 0.4780217195055576\n",
            "Iteration 121900, Loss: 0.47798594769523767\n",
            "Iteration 122000, Loss: 0.47795023662477015\n",
            "Iteration 122100, Loss: 0.4779145861516918\n",
            "Iteration 122200, Loss: 0.47787899613397955\n",
            "Iteration 122300, Loss: 0.47784346643004894\n",
            "Iteration 122400, Loss: 0.4778079968987521\n",
            "Iteration 122500, Loss: 0.47777258739937634\n",
            "Iteration 122600, Loss: 0.47773723779164234\n",
            "Iteration 122700, Loss: 0.4777019479357028\n",
            "Iteration 122800, Loss: 0.47766671769213986\n",
            "Iteration 122900, Loss: 0.47763154692196486\n",
            "Iteration 123000, Loss: 0.4775964354866153\n",
            "Iteration 123100, Loss: 0.4775613832479542\n",
            "Iteration 123200, Loss: 0.477526390068268\n",
            "Iteration 123300, Loss: 0.47749145581026514\n",
            "Iteration 123400, Loss: 0.47745658033707383\n",
            "Iteration 123500, Loss: 0.4774217635122415\n",
            "Iteration 123600, Loss: 0.4773870051997324\n",
            "Iteration 123700, Loss: 0.47735230526392614\n",
            "Iteration 123800, Loss: 0.47731766356961636\n",
            "Iteration 123900, Loss: 0.47728307998200914\n",
            "Iteration 124000, Loss: 0.4772485543667207\n",
            "Iteration 124100, Loss: 0.477214086589777\n",
            "Iteration 124200, Loss: 0.47717967651761123\n",
            "Iteration 124300, Loss: 0.47714532401706294\n",
            "Iteration 124400, Loss: 0.47711102895537594\n",
            "Iteration 124500, Loss: 0.47707679120019686\n",
            "Iteration 124600, Loss: 0.477042610619574\n",
            "Iteration 124700, Loss: 0.47700848708195553\n",
            "Iteration 124800, Loss: 0.4769744204561879\n",
            "Iteration 124900, Loss: 0.4769404106115145\n",
            "Iteration 125000, Loss: 0.4769064574175742\n",
            "Iteration 125100, Loss: 0.47687256074439976\n",
            "Iteration 125200, Loss: 0.47683872046241566\n",
            "Iteration 125300, Loss: 0.4768049364424382\n",
            "Iteration 125400, Loss: 0.4767712085556728\n",
            "Iteration 125500, Loss: 0.47673753667371255\n",
            "Iteration 125600, Loss: 0.47670392066853734\n",
            "Iteration 125700, Loss: 0.47667036041251176\n",
            "Iteration 125800, Loss: 0.47663685577838477\n",
            "Iteration 125900, Loss: 0.4766034066392865\n",
            "Iteration 126000, Loss: 0.4765700128687284\n",
            "Iteration 126100, Loss: 0.4765366743406011\n",
            "Iteration 126200, Loss: 0.4765033909291728\n",
            "Iteration 126300, Loss: 0.47647016250908863\n",
            "Iteration 126400, Loss: 0.47643698895536835\n",
            "Iteration 126500, Loss: 0.47640387014340546\n",
            "Iteration 126600, Loss: 0.4763708059489659\n",
            "Iteration 126700, Loss: 0.476337796248186\n",
            "Iteration 126800, Loss: 0.4763048409175721\n",
            "Iteration 126900, Loss: 0.47627193983399807\n",
            "Iteration 127000, Loss: 0.4762390928747049\n",
            "Iteration 127100, Loss: 0.47620629991729874\n",
            "Iteration 127200, Loss: 0.4761735608397496\n",
            "Iteration 127300, Loss: 0.4761408755203904\n",
            "Iteration 127400, Loss: 0.47610824383791517\n",
            "Iteration 127500, Loss: 0.47607566567137777\n",
            "Iteration 127600, Loss: 0.47604314090019084\n",
            "Iteration 127700, Loss: 0.4760106694041242\n",
            "Iteration 127800, Loss: 0.4759782510633036\n",
            "Iteration 127900, Loss: 0.4759458857582094\n",
            "Iteration 128000, Loss: 0.4759135733696754\n",
            "Iteration 128100, Loss: 0.47588131377888704\n",
            "Iteration 128200, Loss: 0.47584910686738113\n",
            "Iteration 128300, Loss: 0.47581695251704326\n",
            "Iteration 128400, Loss: 0.47578485061010717\n",
            "Iteration 128500, Loss: 0.47575280102915407\n",
            "Iteration 128600, Loss: 0.47572080365710967\n",
            "Iteration 128700, Loss: 0.4756888583772451\n",
            "Iteration 128800, Loss: 0.4756569650731736\n",
            "Iteration 128900, Loss: 0.4756251236288509\n",
            "Iteration 129000, Loss: 0.47559333392857256\n",
            "Iteration 129100, Loss: 0.47556159585697383\n",
            "Iteration 129200, Loss: 0.4755299092990278\n",
            "Iteration 129300, Loss: 0.4754982741400444\n",
            "Iteration 129400, Loss: 0.4754666902656691\n",
            "Iteration 129500, Loss: 0.4754351575618815\n",
            "Iteration 129600, Loss: 0.4754036759149945\n",
            "Iteration 129700, Loss: 0.4753722452116527\n",
            "Iteration 129800, Loss: 0.47534086533883163\n",
            "Iteration 129900, Loss: 0.47530953618383587\n",
            "Iteration 130000, Loss: 0.4752782576342985\n",
            "Iteration 130100, Loss: 0.47524702957817966\n",
            "Iteration 130200, Loss: 0.4752158519037652\n",
            "Iteration 130300, Loss: 0.4751847244996656\n",
            "Iteration 130400, Loss: 0.47515364725481535\n",
            "Iteration 130500, Loss: 0.47512262005847056\n",
            "Iteration 130600, Loss: 0.4750916428002088\n",
            "Iteration 130700, Loss: 0.4750607153699279\n",
            "Iteration 130800, Loss: 0.4750298376578439\n",
            "Iteration 130900, Loss: 0.47499900955449115\n",
            "Iteration 131000, Loss: 0.4749682309507199\n",
            "Iteration 131100, Loss: 0.4749375017376963\n",
            "Iteration 131200, Loss: 0.4749068218069005\n",
            "Iteration 131300, Loss: 0.47487619105012585\n",
            "Iteration 131400, Loss: 0.4748456093594774\n",
            "Iteration 131500, Loss: 0.4748150766273713\n",
            "Iteration 131600, Loss: 0.4747845927465335\n",
            "Iteration 131700, Loss: 0.47475415760999823\n",
            "Iteration 131800, Loss: 0.47472377111110753\n",
            "Iteration 131900, Loss: 0.47469343314350976\n",
            "Iteration 132000, Loss: 0.4746631436011586\n",
            "Iteration 132100, Loss: 0.47463290237831157\n",
            "Iteration 132200, Loss: 0.4746027093695297\n",
            "Iteration 132300, Loss: 0.47457256446967605\n",
            "Iteration 132400, Loss: 0.4745424675739144\n",
            "Iteration 132500, Loss: 0.47451241857770843\n",
            "Iteration 132600, Loss: 0.4744824173768207\n",
            "Iteration 132700, Loss: 0.4744524638673114\n",
            "Iteration 132800, Loss: 0.47442255794553745\n",
            "Iteration 132900, Loss: 0.47439269950815144\n",
            "Iteration 133000, Loss: 0.4743628884521003\n",
            "Iteration 133100, Loss: 0.47433312467462446\n",
            "Iteration 133200, Loss: 0.474303408073257\n",
            "Iteration 133300, Loss: 0.4742737385458224\n",
            "Iteration 133400, Loss: 0.4742441159904351\n",
            "Iteration 133500, Loss: 0.47421454030549937\n",
            "Iteration 133600, Loss: 0.47418501138970753\n",
            "Iteration 133700, Loss: 0.47415552914203946\n",
            "Iteration 133800, Loss: 0.4741260934617609\n",
            "Iteration 133900, Loss: 0.4740967042484231\n",
            "Iteration 134000, Loss: 0.4740673614018615\n",
            "Iteration 134100, Loss: 0.47403806482219474\n",
            "Iteration 134200, Loss: 0.47400881440982395\n",
            "Iteration 134300, Loss: 0.4739796100654311\n",
            "Iteration 134400, Loss: 0.47395045168997846\n",
            "Iteration 134500, Loss: 0.4739213391847081\n",
            "Iteration 134600, Loss: 0.47389227245113946\n",
            "Iteration 134700, Loss: 0.47386325139107016\n",
            "Iteration 134800, Loss: 0.4738342759065738\n",
            "Iteration 134900, Loss: 0.4738053458999989\n",
            "Iteration 135000, Loss: 0.47377646127396916\n",
            "Iteration 135100, Loss: 0.4737476219313808\n",
            "Iteration 135200, Loss: 0.4737188277754033\n",
            "Iteration 135300, Loss: 0.4736900787094772\n",
            "Iteration 135400, Loss: 0.4736613746373135\n",
            "Iteration 135500, Loss: 0.47363271546289315\n",
            "Iteration 135600, Loss: 0.4736041010904657\n",
            "Iteration 135700, Loss: 0.473575531424548\n",
            "Iteration 135800, Loss: 0.4735470063699241\n",
            "Iteration 135900, Loss: 0.4735185258316437\n",
            "Iteration 136000, Loss: 0.4734900897150217\n",
            "Iteration 136100, Loss: 0.4734616979256366\n",
            "Iteration 136200, Loss: 0.4734333503693301\n",
            "Iteration 136300, Loss: 0.4734050469522062\n",
            "Iteration 136400, Loss: 0.47337678758063007\n",
            "Iteration 136500, Loss: 0.47334857216122705\n",
            "Iteration 136600, Loss: 0.47332040060088226\n",
            "Iteration 136700, Loss: 0.473292272806739\n",
            "Iteration 136800, Loss: 0.4732641886861984\n",
            "Iteration 136900, Loss: 0.4732361481469182\n",
            "Iteration 137000, Loss: 0.47320815109681225\n",
            "Iteration 137100, Loss: 0.4731801974440491\n",
            "Iteration 137200, Loss: 0.47315228709705137\n",
            "Iteration 137300, Loss: 0.47312441996449506\n",
            "Iteration 137400, Loss: 0.47309659595530856\n",
            "Iteration 137500, Loss: 0.4730688149786712\n",
            "Iteration 137600, Loss: 0.4730410769440136\n",
            "Iteration 137700, Loss: 0.47301338176101554\n",
            "Iteration 137800, Loss: 0.472985729339606\n",
            "Iteration 137900, Loss: 0.472958119589962\n",
            "Iteration 138000, Loss: 0.47293055242250726\n",
            "Iteration 138100, Loss: 0.47290302774791243\n",
            "Iteration 138200, Loss: 0.4728755454770931\n",
            "Iteration 138300, Loss: 0.4728481055212097\n",
            "Iteration 138400, Loss: 0.4728207077916663\n",
            "Iteration 138500, Loss: 0.47279335220011004\n",
            "Iteration 138600, Loss: 0.47276603865843037\n",
            "Iteration 138700, Loss: 0.4727387670787575\n",
            "Iteration 138800, Loss: 0.4727115373734625\n",
            "Iteration 138900, Loss: 0.47268434945515597\n",
            "Iteration 139000, Loss: 0.47265720323668714\n",
            "Iteration 139100, Loss: 0.4726300986311433\n",
            "Iteration 139200, Loss: 0.4726030355518493\n",
            "Iteration 139300, Loss: 0.4725760139123658\n",
            "Iteration 139400, Loss: 0.47254903362648926\n",
            "Iteration 139500, Loss: 0.47252209460825106\n",
            "Iteration 139600, Loss: 0.47249519677191637\n",
            "Iteration 139700, Loss: 0.4724683400319835\n",
            "Iteration 139800, Loss: 0.4724415243031832\n",
            "Iteration 139900, Loss: 0.4724147495004779\n",
            "Iteration 140000, Loss: 0.47238801553906057\n",
            "Iteration 140100, Loss: 0.4723613223343545\n",
            "Iteration 140200, Loss: 0.47233466980201194\n",
            "Iteration 140300, Loss: 0.4723080578579138\n",
            "Iteration 140400, Loss: 0.4722814864181687\n",
            "Iteration 140500, Loss: 0.472254955399112\n",
            "Iteration 140600, Loss: 0.47222846471730534\n",
            "Iteration 140700, Loss: 0.4722020142895355\n",
            "Iteration 140800, Loss: 0.47217560403281456\n",
            "Iteration 140900, Loss: 0.4721492338643775\n",
            "Iteration 141000, Loss: 0.47212290370168325\n",
            "Iteration 141100, Loss: 0.4720966134624126\n",
            "Iteration 141200, Loss: 0.4720703630644682\n",
            "Iteration 141300, Loss: 0.47204415242597353\n",
            "Iteration 141400, Loss: 0.4720179814652721\n",
            "Iteration 141500, Loss: 0.4719918501009268\n",
            "Iteration 141600, Loss: 0.47196575825171927\n",
            "Iteration 141700, Loss: 0.47193970583664924\n",
            "Iteration 141800, Loss: 0.47191369277493306\n",
            "Iteration 141900, Loss: 0.4718877189860042\n",
            "Iteration 142000, Loss: 0.47186178438951104\n",
            "Iteration 142100, Loss: 0.4718358889053182\n",
            "Iteration 142200, Loss: 0.47181003245350334\n",
            "Iteration 142300, Loss: 0.4717842149543585\n",
            "Iteration 142400, Loss: 0.4717584363283882\n",
            "Iteration 142500, Loss: 0.4717326964963093\n",
            "Iteration 142600, Loss: 0.47170699537905\n",
            "Iteration 142700, Loss: 0.4716813328977491\n",
            "Iteration 142800, Loss: 0.4716557089737561\n",
            "Iteration 142900, Loss: 0.4716301235286288\n",
            "Iteration 143000, Loss: 0.47160457648413456\n",
            "Iteration 143100, Loss: 0.4715790677622482\n",
            "Iteration 143200, Loss: 0.47155359728515195\n",
            "Iteration 143300, Loss: 0.47152816497523453\n",
            "Iteration 143400, Loss: 0.4715027707550908\n",
            "Iteration 143500, Loss: 0.4714774145475204\n",
            "Iteration 143600, Loss: 0.47145209627552787\n",
            "Iteration 143700, Loss: 0.4714268158623215\n",
            "Iteration 143800, Loss: 0.47140157323131276\n",
            "Iteration 143900, Loss: 0.4713763683061153\n",
            "Iteration 144000, Loss: 0.47135120101054523\n",
            "Iteration 144100, Loss: 0.47132607126861914\n",
            "Iteration 144200, Loss: 0.4713009790045547\n",
            "Iteration 144300, Loss: 0.47127592414276925\n",
            "Iteration 144400, Loss: 0.47125090660787905\n",
            "Iteration 144500, Loss: 0.4712259263246992\n",
            "Iteration 144600, Loss: 0.47120098321824244\n",
            "Iteration 144700, Loss: 0.47117607721371907\n",
            "Iteration 144800, Loss: 0.47115120823653567\n",
            "Iteration 144900, Loss: 0.47112637621229486\n",
            "Iteration 145000, Loss: 0.4711015810667947\n",
            "Iteration 145100, Loss: 0.47107682272602736\n",
            "Iteration 145200, Loss: 0.4710521011161797\n",
            "Iteration 145300, Loss: 0.47102741616363153\n",
            "Iteration 145400, Loss: 0.47100276779495565\n",
            "Iteration 145500, Loss: 0.4709781559369166\n",
            "Iteration 145600, Loss: 0.4709535805164709\n",
            "Iteration 145700, Loss: 0.47092904146076564\n",
            "Iteration 145800, Loss: 0.470904538697138\n",
            "Iteration 145900, Loss: 0.47088007215311534\n",
            "Iteration 146000, Loss: 0.4708556417564134\n",
            "Iteration 146100, Loss: 0.47083124743493654\n",
            "Iteration 146200, Loss: 0.4708068891167771\n",
            "Iteration 146300, Loss: 0.47078256673021407\n",
            "Iteration 146400, Loss: 0.4707582802037136\n",
            "Iteration 146500, Loss: 0.4707340294659272\n",
            "Iteration 146600, Loss: 0.4707098144456925\n",
            "Iteration 146700, Loss: 0.4706856350720308\n",
            "Iteration 146800, Loss: 0.47066149127414847\n",
            "Iteration 146900, Loss: 0.4706373829814351\n",
            "Iteration 147000, Loss: 0.4706133101234629\n",
            "Iteration 147100, Loss: 0.47058927262998707\n",
            "Iteration 147200, Loss: 0.4705652704309442\n",
            "Iteration 147300, Loss: 0.47054130345645184\n",
            "Iteration 147400, Loss: 0.47051737163680873\n",
            "Iteration 147500, Loss: 0.47049347490249316\n",
            "Iteration 147600, Loss: 0.47046961318416314\n",
            "Iteration 147700, Loss: 0.4704457864126552\n",
            "Iteration 147800, Loss: 0.4704219945189845\n",
            "Iteration 147900, Loss: 0.4703982374343436\n",
            "Iteration 148000, Loss: 0.4703745150901026\n",
            "Iteration 148100, Loss: 0.47035082741780754\n",
            "Iteration 148200, Loss: 0.47032717434918114\n",
            "Iteration 148300, Loss: 0.470303555816121\n",
            "Iteration 148400, Loss: 0.47027997175069997\n",
            "Iteration 148500, Loss: 0.47025642208516505\n",
            "Iteration 148600, Loss: 0.47023290675193685\n",
            "Iteration 148700, Loss: 0.47020942568360957\n",
            "Iteration 148800, Loss: 0.4701859788129496\n",
            "Iteration 148900, Loss: 0.47016256607289564\n",
            "Iteration 149000, Loss: 0.470139187396558\n",
            "Iteration 149100, Loss: 0.47011584271721785\n",
            "Iteration 149200, Loss: 0.47009253196832684\n",
            "Iteration 149300, Loss: 0.4700692550835065\n",
            "Iteration 149400, Loss: 0.4700460119965477\n",
            "Iteration 149500, Loss: 0.4700228026414104\n",
            "Iteration 149600, Loss: 0.4699996269522224\n",
            "Iteration 149700, Loss: 0.46997648486327936\n",
            "Iteration 149800, Loss: 0.4699533763090445\n",
            "Iteration 149900, Loss: 0.46993030122414725\n",
            "Iteration 150000, Loss: 0.4699072595433837\n",
            "Iteration 150100, Loss: 0.4698842512017149\n",
            "Iteration 150200, Loss: 0.4698612761342677\n",
            "Iteration 150300, Loss: 0.46983833427633304\n",
            "Iteration 150400, Loss: 0.4698154255633661\n",
            "Iteration 150500, Loss: 0.46979254993098557\n",
            "Iteration 150600, Loss: 0.46976970731497325\n",
            "Iteration 150700, Loss: 0.4697468976512735\n",
            "Iteration 150800, Loss: 0.46972412087599213\n",
            "Iteration 150900, Loss: 0.4697013769253971\n",
            "Iteration 151000, Loss: 0.4696786657359172\n",
            "Iteration 151100, Loss: 0.46965598724414126\n",
            "Iteration 151200, Loss: 0.46963334138681856\n",
            "Iteration 151300, Loss: 0.46961072810085747\n",
            "Iteration 151400, Loss: 0.4695881473233259\n",
            "Iteration 151500, Loss: 0.4695655989914491\n",
            "Iteration 151600, Loss: 0.46954308304261155\n",
            "Iteration 151700, Loss: 0.4695205994143543\n",
            "Iteration 151800, Loss: 0.4694981480443759\n",
            "Iteration 151900, Loss: 0.46947572887053096\n",
            "Iteration 152000, Loss: 0.4694533418308304\n",
            "Iteration 152100, Loss: 0.4694309868634401\n",
            "Iteration 152200, Loss: 0.4694086639066817\n",
            "Iteration 152300, Loss: 0.46938637289903107\n",
            "Iteration 152400, Loss: 0.46936411377911763\n",
            "Iteration 152500, Loss: 0.46934188648572506\n",
            "Iteration 152600, Loss: 0.4693196909577897\n",
            "Iteration 152700, Loss: 0.46929752713440076\n",
            "Iteration 152800, Loss: 0.4692753949547992\n",
            "Iteration 152900, Loss: 0.46925329435837804\n",
            "Iteration 153000, Loss: 0.46923122528468125\n",
            "Iteration 153100, Loss: 0.46920918767340325\n",
            "Iteration 153200, Loss: 0.4691871814643895\n",
            "Iteration 153300, Loss: 0.46916520659763433\n",
            "Iteration 153400, Loss: 0.4691432630132819\n",
            "Iteration 153500, Loss: 0.4691213506516251\n",
            "Iteration 153600, Loss: 0.46909946945310516\n",
            "Iteration 153700, Loss: 0.4690776193583112\n",
            "Iteration 153800, Loss: 0.4690558003079797\n",
            "Iteration 153900, Loss: 0.46903401224299457\n",
            "Iteration 154000, Loss: 0.4690122551043858\n",
            "Iteration 154100, Loss: 0.4689905288333299\n",
            "Iteration 154200, Loss: 0.4689688333711486\n",
            "Iteration 154300, Loss: 0.46894716865930874\n",
            "Iteration 154400, Loss: 0.46892553463942266\n",
            "Iteration 154500, Loss: 0.46890393125324625\n",
            "Iteration 154600, Loss: 0.4688823584426797\n",
            "Iteration 154700, Loss: 0.46886081614976627\n",
            "Iteration 154800, Loss: 0.4688393043166924\n",
            "Iteration 154900, Loss: 0.46881782288578694\n",
            "Iteration 155000, Loss: 0.468796371799521\n",
            "Iteration 155100, Loss: 0.46877495100050726\n",
            "Iteration 155200, Loss: 0.4687535604314995\n",
            "Iteration 155300, Loss: 0.4687322000353924\n",
            "Iteration 155400, Loss: 0.4687108697552212\n",
            "Iteration 155500, Loss: 0.46868956953416047\n",
            "Iteration 155600, Loss: 0.468668299315525\n",
            "Iteration 155700, Loss: 0.4686470590427682\n",
            "Iteration 155800, Loss: 0.46862584865948204\n",
            "Iteration 155900, Loss: 0.468604668109397\n",
            "Iteration 156000, Loss: 0.4685835173363814\n",
            "Iteration 156100, Loss: 0.4685623962844406\n",
            "Iteration 156200, Loss: 0.4685413048977172\n",
            "Iteration 156300, Loss: 0.46852024312049007\n",
            "Iteration 156400, Loss: 0.4684992108971745\n",
            "Iteration 156500, Loss: 0.46847820817232133\n",
            "Iteration 156600, Loss: 0.46845723489061675\n",
            "Iteration 156700, Loss: 0.46843629099688155\n",
            "Iteration 156800, Loss: 0.4684153764360714\n",
            "Iteration 156900, Loss: 0.4683944911532758\n",
            "Iteration 157000, Loss: 0.4683736350937182\n",
            "Iteration 157100, Loss: 0.46835280820275466\n",
            "Iteration 157200, Loss: 0.46833201042587497\n",
            "Iteration 157300, Loss: 0.4683112417087005\n",
            "Iteration 157400, Loss: 0.46829050199698496\n",
            "Iteration 157500, Loss: 0.4682697912366143\n",
            "Iteration 157600, Loss: 0.46824910937360453\n",
            "Iteration 157700, Loss: 0.46822845635410393\n",
            "Iteration 157800, Loss: 0.46820783212438966\n",
            "Iteration 157900, Loss: 0.4681872366308703\n",
            "Iteration 158000, Loss: 0.46816666982008326\n",
            "Iteration 158100, Loss: 0.4681461316386957\n",
            "Iteration 158200, Loss: 0.46812562203350316\n",
            "Iteration 158300, Loss: 0.46810514095143035\n",
            "Iteration 158400, Loss: 0.4680846883395295\n",
            "Iteration 158500, Loss: 0.4680642641449806\n",
            "Iteration 158600, Loss: 0.4680438683150914\n",
            "Iteration 158700, Loss: 0.4680235007972964\n",
            "Iteration 158800, Loss: 0.4680031615391562\n",
            "Iteration 158900, Loss: 0.46798285048835847\n",
            "Iteration 159000, Loss: 0.46796256759271604\n",
            "Iteration 159100, Loss: 0.46794231280016735\n",
            "Iteration 159200, Loss: 0.46792208605877594\n",
            "Iteration 159300, Loss: 0.46790188731673\n",
            "Iteration 159400, Loss: 0.46788171652234206\n",
            "Iteration 159500, Loss: 0.4678615736240486\n",
            "Iteration 159600, Loss: 0.4678414585704096\n",
            "Iteration 159700, Loss: 0.46782137131010837\n",
            "Iteration 159800, Loss: 0.46780131179195084\n",
            "Iteration 159900, Loss: 0.4677812799648656\n",
            "Iteration 160000, Loss: 0.46776127577790344\n",
            "Iteration 160100, Loss: 0.46774129918023655\n",
            "Iteration 160200, Loss: 0.46772135012115845\n",
            "Iteration 160300, Loss: 0.46770142855008423\n",
            "Iteration 160400, Loss: 0.46768153441654925\n",
            "Iteration 160500, Loss: 0.46766166767020895\n",
            "Iteration 160600, Loss: 0.4676418282608393\n",
            "Iteration 160700, Loss: 0.4676220161383352\n",
            "Iteration 160800, Loss: 0.46760223125271133\n",
            "Iteration 160900, Loss: 0.4675824735541008\n",
            "Iteration 161000, Loss: 0.46756274299275563\n",
            "Iteration 161100, Loss: 0.4675430395190458\n",
            "Iteration 161200, Loss: 0.467523363083459\n",
            "Iteration 161300, Loss: 0.46750371363660026\n",
            "Iteration 161400, Loss: 0.46748409112919254\n",
            "Iteration 161500, Loss: 0.4674644955120746\n",
            "Iteration 161600, Loss: 0.4674449267362024\n",
            "Iteration 161700, Loss: 0.46742538475264733\n",
            "Iteration 161800, Loss: 0.4674058695125969\n",
            "Iteration 161900, Loss: 0.4673863809673542\n",
            "Iteration 162000, Loss: 0.46736691906833666\n",
            "Iteration 162100, Loss: 0.4673474837670771\n",
            "Iteration 162200, Loss: 0.46732807501522267\n",
            "Iteration 162300, Loss: 0.4673086927645345\n",
            "Iteration 162400, Loss: 0.4672893369668872\n",
            "Iteration 162500, Loss: 0.46727000757426884\n",
            "Iteration 162600, Loss: 0.46725070453878087\n",
            "Iteration 162700, Loss: 0.46723142781263705\n",
            "Iteration 162800, Loss: 0.46721217734816367\n",
            "Iteration 162900, Loss: 0.4671929530977991\n",
            "Iteration 163000, Loss: 0.46717375501409353\n",
            "Iteration 163100, Loss: 0.46715458304970847\n",
            "Iteration 163200, Loss: 0.4671354371574163\n",
            "Iteration 163300, Loss: 0.46711631729010056\n",
            "Iteration 163400, Loss: 0.46709722340075466\n",
            "Iteration 163500, Loss: 0.467078155442483\n",
            "Iteration 163600, Loss: 0.4670591133684986\n",
            "Iteration 163700, Loss: 0.46704009713212485\n",
            "Iteration 163800, Loss: 0.467021106686794\n",
            "Iteration 163900, Loss: 0.46700214198604706\n",
            "Iteration 164000, Loss: 0.46698320298353346\n",
            "Iteration 164100, Loss: 0.4669642896330111\n",
            "Iteration 164200, Loss: 0.46694540188834566\n",
            "Iteration 164300, Loss: 0.4669265397035102\n",
            "Iteration 164400, Loss: 0.466907703032585\n",
            "Iteration 164500, Loss: 0.46688889182975796\n",
            "Iteration 164600, Loss: 0.46687010604932255\n",
            "Iteration 164700, Loss: 0.4668513456456794\n",
            "Iteration 164800, Loss: 0.4668326105733347\n",
            "Iteration 164900, Loss: 0.4668139007869007\n",
            "Iteration 165000, Loss: 0.46679521624109466\n",
            "Iteration 165100, Loss: 0.46677655689073916\n",
            "Iteration 165200, Loss: 0.46675792269076155\n",
            "Iteration 165300, Loss: 0.4667393135961936\n",
            "Iteration 165400, Loss: 0.46672072956217137\n",
            "Iteration 165500, Loss: 0.46670217054393465\n",
            "Iteration 165600, Loss: 0.4666836364968271\n",
            "Iteration 165700, Loss: 0.4666651273762952\n",
            "Iteration 165800, Loss: 0.46664664313788884\n",
            "Iteration 165900, Loss: 0.46662818373726056\n",
            "Iteration 166000, Loss: 0.46660974913016506\n",
            "Iteration 166100, Loss: 0.4665913392724592\n",
            "Iteration 166200, Loss: 0.4665729541201019\n",
            "Iteration 166300, Loss: 0.46655459362915325\n",
            "Iteration 166400, Loss: 0.46653625775577484\n",
            "Iteration 166500, Loss: 0.46651794645622896\n",
            "Iteration 166600, Loss: 0.4664996596868788\n",
            "Iteration 166700, Loss: 0.46648139740418765\n",
            "Iteration 166800, Loss: 0.46646315956471907\n",
            "Iteration 166900, Loss: 0.4664449461251364\n",
            "Iteration 167000, Loss: 0.46642675704220216\n",
            "Iteration 167100, Loss: 0.4664085922727784\n",
            "Iteration 167200, Loss: 0.4663904517738262\n",
            "Iteration 167300, Loss: 0.46637233550240487\n",
            "Iteration 167400, Loss: 0.4663542434156723\n",
            "Iteration 167500, Loss: 0.4663361754708847\n",
            "Iteration 167600, Loss: 0.46631813162539565\n",
            "Iteration 167700, Loss: 0.4663001118366566\n",
            "Iteration 167800, Loss: 0.4662821160622159\n",
            "Iteration 167900, Loss: 0.4662641442597193\n",
            "Iteration 168000, Loss: 0.4662461963869089\n",
            "Iteration 168100, Loss: 0.4662282724016234\n",
            "Iteration 168200, Loss: 0.46621037226179757\n",
            "Iteration 168300, Loss: 0.46619249592546197\n",
            "Iteration 168400, Loss: 0.4661746433507432\n",
            "Iteration 168500, Loss: 0.4661568144958624\n",
            "Iteration 168600, Loss: 0.46613900931913643\n",
            "Iteration 168700, Loss: 0.4661212277789766\n",
            "Iteration 168800, Loss: 0.46610346983388906\n",
            "Iteration 168900, Loss: 0.4660857354424739\n",
            "Iteration 169000, Loss: 0.4660680245634252\n",
            "Iteration 169100, Loss: 0.4660503371555311\n",
            "Iteration 169200, Loss: 0.46603267317767294\n",
            "Iteration 169300, Loss: 0.466015032588825\n",
            "Iteration 169400, Loss: 0.46599741534805494\n",
            "Iteration 169500, Loss: 0.465979821414523\n",
            "Iteration 169600, Loss: 0.46596225074748165\n",
            "Iteration 169700, Loss: 0.46594470330627574\n",
            "Iteration 169800, Loss: 0.46592717905034153\n",
            "Iteration 169900, Loss: 0.4659096779392076\n",
            "Iteration 170000, Loss: 0.46589219993249337\n",
            "Iteration 170100, Loss: 0.46587474498990944\n",
            "Iteration 170200, Loss: 0.4658573130712575\n",
            "Iteration 170300, Loss: 0.4658399041364296\n",
            "Iteration 170400, Loss: 0.46582251814540837\n",
            "Iteration 170500, Loss: 0.46580515505826625\n",
            "Iteration 170600, Loss: 0.46578781483516596\n",
            "Iteration 170700, Loss: 0.4657704974363592\n",
            "Iteration 170800, Loss: 0.46575320282218746\n",
            "Iteration 170900, Loss: 0.46573593095308113\n",
            "Iteration 171000, Loss: 0.4657186817895596\n",
            "Iteration 171100, Loss: 0.46570145529223095\n",
            "Iteration 171200, Loss: 0.4656842514217909\n",
            "Iteration 171300, Loss: 0.46566707013902414\n",
            "Iteration 171400, Loss: 0.4656499114048027\n",
            "Iteration 171500, Loss: 0.46563277518008644\n",
            "Iteration 171600, Loss: 0.4656156614259226\n",
            "Iteration 171700, Loss: 0.4655985701034453\n",
            "Iteration 171800, Loss: 0.46558150117387564\n",
            "Iteration 171900, Loss: 0.46556445459852136\n",
            "Iteration 172000, Loss: 0.4655474303387768\n",
            "Iteration 172100, Loss: 0.4655304283561222\n",
            "Iteration 172200, Loss: 0.4655134486121236\n",
            "Iteration 172300, Loss: 0.4654964910684333\n",
            "Iteration 172400, Loss: 0.46547955568678834\n",
            "Iteration 172500, Loss: 0.4654626424290113\n",
            "Iteration 172600, Loss: 0.46544575125700977\n",
            "Iteration 172700, Loss: 0.4654288821327759\n",
            "Iteration 172800, Loss: 0.4654120350183867\n",
            "Iteration 172900, Loss: 0.46539520987600264\n",
            "Iteration 173000, Loss: 0.4653784066678693\n",
            "Iteration 173100, Loss: 0.46536162535631487\n",
            "Iteration 173200, Loss: 0.46534486590375246\n",
            "Iteration 173300, Loss: 0.46532812827267717\n",
            "Iteration 173400, Loss: 0.4653114124256682\n",
            "Iteration 173500, Loss: 0.4652947183253871\n",
            "Iteration 173600, Loss: 0.4652780459345781\n",
            "Iteration 173700, Loss: 0.4652613952160682\n",
            "Iteration 173800, Loss: 0.46524476613276616\n",
            "Iteration 173900, Loss: 0.46522815864766276\n",
            "Iteration 174000, Loss: 0.46521157272383107\n",
            "Iteration 174100, Loss: 0.46519500832442484\n",
            "Iteration 174200, Loss: 0.4651784654126798\n",
            "Iteration 174300, Loss: 0.46516194395191235\n",
            "Iteration 174400, Loss: 0.4651454439055199\n",
            "Iteration 174500, Loss: 0.46512896523698033\n",
            "Iteration 174600, Loss: 0.46511250790985215\n",
            "Iteration 174700, Loss: 0.4650960718877739\n",
            "Iteration 174800, Loss: 0.46507965713446425\n",
            "Iteration 174900, Loss: 0.4650632636137211\n",
            "Iteration 175000, Loss: 0.4650468912894226\n",
            "Iteration 175100, Loss: 0.46503054012552597\n",
            "Iteration 175200, Loss: 0.4650142100860672\n",
            "Iteration 175300, Loss: 0.46499790113516126\n",
            "Iteration 175400, Loss: 0.4649816132370022\n",
            "Iteration 175500, Loss: 0.464965346355862\n",
            "Iteration 175600, Loss: 0.46494910045609106\n",
            "Iteration 175700, Loss: 0.46493287550211804\n",
            "Iteration 175800, Loss: 0.46491667145844906\n",
            "Iteration 175900, Loss: 0.4649004882896679\n",
            "Iteration 176000, Loss: 0.464884325960436\n",
            "Iteration 176100, Loss: 0.4648681844354915\n",
            "Iteration 176200, Loss: 0.4648520636796499\n",
            "Iteration 176300, Loss: 0.46483596365780344\n",
            "Iteration 176400, Loss: 0.46481988433492055\n",
            "Iteration 176500, Loss: 0.46480382567604633\n",
            "Iteration 176600, Loss: 0.4647877876463021\n",
            "Iteration 176700, Loss: 0.4647717702108846\n",
            "Iteration 176800, Loss: 0.464755773335067\n",
            "Iteration 176900, Loss: 0.4647397969841973\n",
            "Iteration 177000, Loss: 0.46472384112369913\n",
            "Iteration 177100, Loss: 0.46470790571907133\n",
            "Iteration 177200, Loss: 0.4646919907358875\n",
            "Iteration 177300, Loss: 0.46467609613979594\n",
            "Iteration 177400, Loss: 0.46466022189651945\n",
            "Iteration 177500, Loss: 0.46464436797185527\n",
            "Iteration 177600, Loss: 0.4646285343316745\n",
            "Iteration 177700, Loss: 0.46461272094192235\n",
            "Iteration 177800, Loss: 0.46459692776861755\n",
            "Iteration 177900, Loss: 0.46458115477785267\n",
            "Iteration 178000, Loss: 0.46456540193579293\n",
            "Iteration 178100, Loss: 0.4645496692086775\n",
            "Iteration 178200, Loss: 0.4645339565628177\n",
            "Iteration 178300, Loss: 0.4645182639645981\n",
            "Iteration 178400, Loss: 0.46450259138047545\n",
            "Iteration 178500, Loss: 0.46448693877697916\n",
            "Iteration 178600, Loss: 0.46447130612071047\n",
            "Iteration 178700, Loss: 0.46445569337834247\n",
            "Iteration 178800, Loss: 0.4644401005166204\n",
            "Iteration 178900, Loss: 0.4644245275023609\n",
            "Iteration 179000, Loss: 0.46440897430245176\n",
            "Iteration 179100, Loss: 0.4643934408838522\n",
            "Iteration 179200, Loss: 0.46437792721359217\n",
            "Iteration 179300, Loss: 0.46436243325877263\n",
            "Iteration 179400, Loss: 0.46434695898656514\n",
            "Iteration 179500, Loss: 0.46433150436421156\n",
            "Iteration 179600, Loss: 0.46431606935902386\n",
            "Iteration 179700, Loss: 0.46430065393838443\n",
            "Iteration 179800, Loss: 0.46428525806974524\n",
            "Iteration 179900, Loss: 0.46426988172062783\n",
            "Iteration 180000, Loss: 0.46425452485862345\n",
            "Iteration 180100, Loss: 0.46423918745139264\n",
            "Iteration 180200, Loss: 0.4642238694666649\n",
            "Iteration 180300, Loss: 0.46420857087223855\n",
            "Iteration 180400, Loss: 0.4641932916359812\n",
            "Iteration 180500, Loss: 0.46417803172582844\n",
            "Iteration 180600, Loss: 0.4641627911097846\n",
            "Iteration 180700, Loss: 0.46414756975592164\n",
            "Iteration 180800, Loss: 0.4641323676323806\n",
            "Iteration 180900, Loss: 0.4641171847073693\n",
            "Iteration 181000, Loss: 0.4641020209491636\n",
            "Iteration 181100, Loss: 0.46408687632610723\n",
            "Iteration 181200, Loss: 0.46407175080661056\n",
            "Iteration 181300, Loss: 0.46405664435915156\n",
            "Iteration 181400, Loss: 0.4640415569522749\n",
            "Iteration 181500, Loss: 0.46402648855459216\n",
            "Iteration 181600, Loss: 0.46401143913478143\n",
            "Iteration 181700, Loss: 0.4639964086615873\n",
            "Iteration 181800, Loss: 0.46398139710382047\n",
            "Iteration 181900, Loss: 0.46396640443035797\n",
            "Iteration 182000, Loss: 0.463951430610142\n",
            "Iteration 182100, Loss: 0.46393647561218165\n",
            "Iteration 182200, Loss: 0.4639215394055507\n",
            "Iteration 182300, Loss: 0.4639066219593884\n",
            "Iteration 182400, Loss: 0.4638917232428996\n",
            "Iteration 182500, Loss: 0.4638768432253537\n",
            "Iteration 182600, Loss: 0.46386198187608524\n",
            "Iteration 182700, Loss: 0.4638471391644934\n",
            "Iteration 182800, Loss: 0.4638323150600419\n",
            "Iteration 182900, Loss: 0.46381750953225875\n",
            "Iteration 183000, Loss: 0.4638027225507362\n",
            "Iteration 183100, Loss: 0.4637879540851306\n",
            "Iteration 183200, Loss: 0.4637732041051618\n",
            "Iteration 183300, Loss: 0.46375847258061376\n",
            "Iteration 183400, Loss: 0.46374375948133384\n",
            "Iteration 183500, Loss: 0.4637290647772326\n",
            "Iteration 183600, Loss: 0.4637143884382837\n",
            "Iteration 183700, Loss: 0.46369973043452434\n",
            "Iteration 183800, Loss: 0.46368509073605396\n",
            "Iteration 183900, Loss: 0.46367046931303507\n",
            "Iteration 184000, Loss: 0.4636558661356926\n",
            "Iteration 184100, Loss: 0.4636412811743136\n",
            "Iteration 184200, Loss: 0.46362671439924785\n",
            "Iteration 184300, Loss: 0.46361216578090697\n",
            "Iteration 184400, Loss: 0.46359763528976394\n",
            "Iteration 184500, Loss: 0.4635831228963544\n",
            "Iteration 184600, Loss: 0.46356862857127457\n",
            "Iteration 184700, Loss: 0.46355415228518293\n",
            "Iteration 184800, Loss: 0.46353969400879846\n",
            "Iteration 184900, Loss: 0.4635252537129018\n",
            "Iteration 185000, Loss: 0.4635108313683343\n",
            "Iteration 185100, Loss: 0.463496426945998\n",
            "Iteration 185200, Loss: 0.4634820404168554\n",
            "Iteration 185300, Loss: 0.46346767175192993\n",
            "Iteration 185400, Loss: 0.4634533209223049\n",
            "Iteration 185500, Loss: 0.46343898789912397\n",
            "Iteration 185600, Loss: 0.46342467265359066\n",
            "Iteration 185700, Loss: 0.46341037515696837\n",
            "Iteration 185800, Loss: 0.4633960953805802\n",
            "Iteration 185900, Loss: 0.4633818332958087\n",
            "Iteration 186000, Loss: 0.4633675888740957\n",
            "Iteration 186100, Loss: 0.46335336208694256\n",
            "Iteration 186200, Loss: 0.4633391529059095\n",
            "Iteration 186300, Loss: 0.4633249613026156\n",
            "Iteration 186400, Loss: 0.46331078724873864\n",
            "Iteration 186500, Loss: 0.4632966307160154\n",
            "Iteration 186600, Loss: 0.4632824916762406\n",
            "Iteration 186700, Loss: 0.4632683701012675\n",
            "Iteration 186800, Loss: 0.4632542659630076\n",
            "Iteration 186900, Loss: 0.4632401792334304\n",
            "Iteration 187000, Loss: 0.46322610988456303\n",
            "Iteration 187100, Loss: 0.4632120578884906\n",
            "Iteration 187200, Loss: 0.46319802321735537\n",
            "Iteration 187300, Loss: 0.46318400584335767\n",
            "Iteration 187400, Loss: 0.4631700057387546\n",
            "Iteration 187500, Loss: 0.46315602287586044\n",
            "Iteration 187600, Loss: 0.4631420572270466\n",
            "Iteration 187700, Loss: 0.4631281087647411\n",
            "Iteration 187800, Loss: 0.4631141774614291\n",
            "Iteration 187900, Loss: 0.4631002632896515\n",
            "Iteration 188000, Loss: 0.46308636622200644\n",
            "Iteration 188100, Loss: 0.4630724862311477\n",
            "Iteration 188200, Loss: 0.4630586232897857\n",
            "Iteration 188300, Loss: 0.46304477737068606\n",
            "Iteration 188400, Loss: 0.4630309484466712\n",
            "Iteration 188500, Loss: 0.46301713649061843\n",
            "Iteration 188600, Loss: 0.46300334147546096\n",
            "Iteration 188700, Loss: 0.4629895633741873\n",
            "Iteration 188800, Loss: 0.4629758021598412\n",
            "Iteration 188900, Loss: 0.46296205780552163\n",
            "Iteration 189000, Loss: 0.4629483302843824\n",
            "Iteration 189100, Loss: 0.46293461956963233\n",
            "Iteration 189200, Loss: 0.46292092563453463\n",
            "Iteration 189300, Loss: 0.4629072484524075\n",
            "Iteration 189400, Loss: 0.4628935879966231\n",
            "Iteration 189500, Loss: 0.46287994424060824\n",
            "Iteration 189600, Loss: 0.46286631715784377\n",
            "Iteration 189700, Loss: 0.46285270672186435\n",
            "Iteration 189800, Loss: 0.4628391129062589\n",
            "Iteration 189900, Loss: 0.46282553568466955\n",
            "Iteration 190000, Loss: 0.46281197503079263\n",
            "Iteration 190100, Loss: 0.46279843091837736\n",
            "Iteration 190200, Loss: 0.4627849033212268\n",
            "Iteration 190300, Loss: 0.4627713922131969\n",
            "Iteration 190400, Loss: 0.46275789756819674\n",
            "Iteration 190500, Loss: 0.4627444193601883\n",
            "Iteration 190600, Loss: 0.46273095756318644\n",
            "Iteration 190700, Loss: 0.46271751215125845\n",
            "Iteration 190800, Loss: 0.46270408309852445\n",
            "Iteration 190900, Loss: 0.4626906703791569\n",
            "Iteration 191000, Loss: 0.46267727396738023\n",
            "Iteration 191100, Loss: 0.4626638938374715\n",
            "Iteration 191200, Loss: 0.4626505299637594\n",
            "Iteration 191300, Loss: 0.46263718232062445\n",
            "Iteration 191400, Loss: 0.4626238508824992\n",
            "Iteration 191500, Loss: 0.4626105356238675\n",
            "Iteration 191600, Loss: 0.4625972365192652\n",
            "Iteration 191700, Loss: 0.46258395354327875\n",
            "Iteration 191800, Loss: 0.4625706866705465\n",
            "Iteration 191900, Loss: 0.46255743587575743\n",
            "Iteration 192000, Loss: 0.4625442011336519\n",
            "Iteration 192100, Loss: 0.46253098241902074\n",
            "Iteration 192200, Loss: 0.4625177797067057\n",
            "Iteration 192300, Loss: 0.4625045929715992\n",
            "Iteration 192400, Loss: 0.4624914221886437\n",
            "Iteration 192500, Loss: 0.4624782673328325\n",
            "Iteration 192600, Loss: 0.4624651283792089\n",
            "Iteration 192700, Loss: 0.4624520053028663\n",
            "Iteration 192800, Loss: 0.4624388980789479\n",
            "Iteration 192900, Loss: 0.4624258066826472\n",
            "Iteration 193000, Loss: 0.4624127310892068\n",
            "Iteration 193100, Loss: 0.46239967127391923\n",
            "Iteration 193200, Loss: 0.4623866272121266\n",
            "Iteration 193300, Loss: 0.4623735988792199\n",
            "Iteration 193400, Loss: 0.46236058625064\n",
            "Iteration 193500, Loss: 0.4623475893018764\n",
            "Iteration 193600, Loss: 0.4623346080084678\n",
            "Iteration 193700, Loss: 0.4623216423460013\n",
            "Iteration 193800, Loss: 0.4623086922901136\n",
            "Iteration 193900, Loss: 0.4622957578164891\n",
            "Iteration 194000, Loss: 0.46228283890086125\n",
            "Iteration 194100, Loss: 0.46226993551901163\n",
            "Iteration 194200, Loss: 0.46225704764677017\n",
            "Iteration 194300, Loss: 0.46224417526001516\n",
            "Iteration 194400, Loss: 0.4622313183346723\n",
            "Iteration 194500, Loss: 0.46221847684671574\n",
            "Iteration 194600, Loss: 0.46220565077216735\n",
            "Iteration 194700, Loss: 0.4621928400870964\n",
            "Iteration 194800, Loss: 0.4621800447676197\n",
            "Iteration 194900, Loss: 0.462167264789902\n",
            "Iteration 195000, Loss: 0.4621545001301547\n",
            "Iteration 195100, Loss: 0.4621417507646367\n",
            "Iteration 195200, Loss: 0.46212901666965395\n",
            "Iteration 195300, Loss: 0.4621162978215596\n",
            "Iteration 195400, Loss: 0.46210359419675306\n",
            "Iteration 195500, Loss: 0.46209090577168105\n",
            "Iteration 195600, Loss: 0.46207823252283675\n",
            "Iteration 195700, Loss: 0.46206557442675955\n",
            "Iteration 195800, Loss: 0.4620529314600356\n",
            "Iteration 195900, Loss: 0.46204030359929693\n",
            "Iteration 196000, Loss: 0.4620276908212224\n",
            "Iteration 196100, Loss: 0.46201509310253625\n",
            "Iteration 196200, Loss: 0.4620025104200087\n",
            "Iteration 196300, Loss: 0.4619899427504563\n",
            "Iteration 196400, Loss: 0.4619773900707409\n",
            "Iteration 196500, Loss: 0.46196485235776996\n",
            "Iteration 196600, Loss: 0.4619523295884966\n",
            "Iteration 196700, Loss: 0.4619398217399191\n",
            "Iteration 196800, Loss: 0.46192732878908116\n",
            "Iteration 196900, Loss: 0.4619148507130717\n",
            "Iteration 197000, Loss: 0.4619023874890246\n",
            "Iteration 197100, Loss: 0.4618899390941185\n",
            "Iteration 197200, Loss: 0.46187750550557727\n",
            "Iteration 197300, Loss: 0.4618650867006691\n",
            "Iteration 197400, Loss: 0.46185268265670687\n",
            "Iteration 197500, Loss: 0.46184029335104815\n",
            "Iteration 197600, Loss: 0.4618279187610947\n",
            "Iteration 197700, Loss: 0.46181555886429265\n",
            "Iteration 197800, Loss: 0.4618032136381322\n",
            "Iteration 197900, Loss: 0.4617908830601479\n",
            "Iteration 198000, Loss: 0.46177856710791815\n",
            "Iteration 198100, Loss: 0.4617662657590648\n",
            "Iteration 198200, Loss: 0.4617539789912539\n",
            "Iteration 198300, Loss: 0.46174170678219506\n",
            "Iteration 198400, Loss: 0.4617294491096415\n",
            "Iteration 198500, Loss: 0.46171720595138943\n",
            "Iteration 198600, Loss: 0.461704977285279\n",
            "Iteration 198700, Loss: 0.46169276308919294\n",
            "Iteration 198800, Loss: 0.4616805633410579\n",
            "Iteration 198900, Loss: 0.46166837801884264\n",
            "Iteration 199000, Loss: 0.46165620710055955\n",
            "Iteration 199100, Loss: 0.4616440505642634\n",
            "Iteration 199200, Loss: 0.46163190838805196\n",
            "Iteration 199300, Loss: 0.46161978055006525\n",
            "Iteration 199400, Loss: 0.46160766702848605\n",
            "Iteration 199500, Loss: 0.4615955678015395\n",
            "Iteration 199600, Loss: 0.46158348284749295\n",
            "Iteration 199700, Loss: 0.461571412144656\n",
            "Iteration 199800, Loss: 0.4615593556713803\n",
            "Iteration 199900, Loss: 0.4615473134060595\n",
            "Iteration 200000, Loss: 0.4615352853271292\n",
            "Iteration 200100, Loss: 0.46152327141306637\n",
            "Iteration 200200, Loss: 0.46151127164239053\n",
            "Iteration 200300, Loss: 0.4614992859936621\n",
            "Iteration 200400, Loss: 0.4614873144454828\n",
            "Iteration 200500, Loss: 0.46147535697649655\n",
            "Iteration 200600, Loss: 0.46146341356538784\n",
            "Iteration 200700, Loss: 0.4614514841908827\n",
            "Iteration 200800, Loss: 0.4614395688317483\n",
            "Iteration 200900, Loss: 0.4614276674667923\n",
            "Iteration 201000, Loss: 0.4614157800748637\n",
            "Iteration 201100, Loss: 0.4614039066348524\n",
            "Iteration 201200, Loss: 0.46139204712568865\n",
            "Iteration 201300, Loss: 0.4613802015263434\n",
            "Iteration 201400, Loss: 0.4613683698158282\n",
            "Iteration 201500, Loss: 0.461356551973195\n",
            "Iteration 201600, Loss: 0.46134474797753583\n",
            "Iteration 201700, Loss: 0.46133295780798317\n",
            "Iteration 201800, Loss: 0.4613211814437097\n",
            "Iteration 201900, Loss: 0.46130941886392796\n",
            "Iteration 202000, Loss: 0.46129767004789024\n",
            "Iteration 202100, Loss: 0.461285934974889\n",
            "Iteration 202200, Loss: 0.4612742136242563\n",
            "Iteration 202300, Loss: 0.46126250597536383\n",
            "Iteration 202400, Loss: 0.46125081200762286\n",
            "Iteration 202500, Loss: 0.4612391317004839\n",
            "Iteration 202600, Loss: 0.46122746503343715\n",
            "Iteration 202700, Loss: 0.4612158119860119\n",
            "Iteration 202800, Loss: 0.4612041725377769\n",
            "Iteration 202900, Loss: 0.46119254666833936\n",
            "Iteration 203000, Loss: 0.4611809343573461\n",
            "Iteration 203100, Loss: 0.4611693355844825\n",
            "Iteration 203200, Loss: 0.46115775032947265\n",
            "Iteration 203300, Loss: 0.4611461785720799\n",
            "Iteration 203400, Loss: 0.4611346202921057\n",
            "Iteration 203500, Loss: 0.46112307546938996\n",
            "Iteration 203600, Loss: 0.46111154408381144\n",
            "Iteration 203700, Loss: 0.46110002611528716\n",
            "Iteration 203800, Loss: 0.4610885215437723\n",
            "Iteration 203900, Loss: 0.4610770303492599\n",
            "Iteration 204000, Loss: 0.46106555251178155\n",
            "Iteration 204100, Loss: 0.46105408801140674\n",
            "Iteration 204200, Loss: 0.46104263682824276\n",
            "Iteration 204300, Loss: 0.46103119894243466\n",
            "Iteration 204400, Loss: 0.46101977433416536\n",
            "Iteration 204500, Loss: 0.4610083629836552\n",
            "Iteration 204600, Loss: 0.46099696487116226\n",
            "Iteration 204700, Loss: 0.4609855799769818\n",
            "Iteration 204800, Loss: 0.46097420828144725\n",
            "Iteration 204900, Loss: 0.4609628497649279\n",
            "Iteration 205000, Loss: 0.46095150440783156\n",
            "Iteration 205100, Loss: 0.4609401721906025\n",
            "Iteration 205200, Loss: 0.4609288530937223\n",
            "Iteration 205300, Loss: 0.46091754709770894\n",
            "Iteration 205400, Loss: 0.4609062541831179\n",
            "Iteration 205500, Loss: 0.46089497433054083\n",
            "Iteration 205600, Loss: 0.46088370752060676\n",
            "Iteration 205700, Loss: 0.46087245373398056\n",
            "Iteration 205800, Loss: 0.4608612129513643\n",
            "Iteration 205900, Loss: 0.4608499851534957\n",
            "Iteration 206000, Loss: 0.4608387703211492\n",
            "Iteration 206100, Loss: 0.4608275684351358\n",
            "Iteration 206200, Loss: 0.4608163794763021\n",
            "Iteration 206300, Loss: 0.46080520342553105\n",
            "Iteration 206400, Loss: 0.4607940402637417\n",
            "Iteration 206500, Loss: 0.46078288997188876\n",
            "Iteration 206600, Loss: 0.460771752530963\n",
            "Iteration 206700, Loss: 0.4607606279219906\n",
            "Iteration 206800, Loss: 0.4607495161260337\n",
            "Iteration 206900, Loss: 0.46073841712419006\n",
            "Iteration 207000, Loss: 0.46072733089759244\n",
            "Iteration 207100, Loss: 0.4607162574274097\n",
            "Iteration 207200, Loss: 0.46070519669484544\n",
            "Iteration 207300, Loss: 0.46069414868113895\n",
            "Iteration 207400, Loss: 0.4606831133675644\n",
            "Iteration 207500, Loss: 0.46067209073543103\n",
            "Iteration 207600, Loss: 0.4606610807660834\n",
            "Iteration 207700, Loss: 0.46065008344090047\n",
            "Iteration 207800, Loss: 0.4606390987412966\n",
            "Iteration 207900, Loss: 0.46062812664872044\n",
            "Iteration 208000, Loss: 0.4606171671446557\n",
            "Iteration 208100, Loss: 0.46060622021062025\n",
            "Iteration 208200, Loss: 0.46059528582816683\n",
            "Iteration 208300, Loss: 0.4605843639788827\n",
            "Iteration 208400, Loss: 0.46057345464438915\n",
            "Iteration 208500, Loss: 0.46056255780634175\n",
            "Iteration 208600, Loss: 0.4605516734464306\n",
            "Iteration 208700, Loss: 0.46054080154637966\n",
            "Iteration 208800, Loss: 0.46052994208794706\n",
            "Iteration 208900, Loss: 0.4605190950529247\n",
            "Iteration 209000, Loss: 0.46050826042313864\n",
            "Iteration 209100, Loss: 0.46049743818044836\n",
            "Iteration 209200, Loss: 0.4604866283067476\n",
            "Iteration 209300, Loss: 0.46047583078396337\n",
            "Iteration 209400, Loss: 0.46046504559405615\n",
            "Iteration 209500, Loss: 0.46045427271902056\n",
            "Iteration 209600, Loss: 0.46044351214088375\n",
            "Iteration 209700, Loss: 0.4604327638417067\n",
            "Iteration 209800, Loss: 0.460422027803584\n",
            "Iteration 209900, Loss: 0.4604113040086425\n",
            "Iteration 210000, Loss: 0.4604005924390432\n",
            "Iteration 210100, Loss: 0.4603898930769793\n",
            "Iteration 210200, Loss: 0.4603792059046774\n",
            "Iteration 210300, Loss: 0.460368530904397\n",
            "Iteration 210400, Loss: 0.46035786805843026\n",
            "Iteration 210500, Loss: 0.4603472173491019\n",
            "Iteration 210600, Loss: 0.46033657875876993\n",
            "Iteration 210700, Loss: 0.4603259522698242\n",
            "Iteration 210800, Loss: 0.4603153378646875\n",
            "Iteration 210900, Loss: 0.460304735525815\n",
            "Iteration 211000, Loss: 0.46029414523569434\n",
            "Iteration 211100, Loss: 0.46028356697684497\n",
            "Iteration 211200, Loss: 0.46027300073181926\n",
            "Iteration 211300, Loss: 0.4602624464832012\n",
            "Iteration 211400, Loss: 0.4602519042136069\n",
            "Iteration 211500, Loss: 0.4602413739056848\n",
            "Iteration 211600, Loss: 0.46023085554211485\n",
            "Iteration 211700, Loss: 0.4602203491056093\n",
            "Iteration 211800, Loss: 0.4602098545789117\n",
            "Iteration 211900, Loss: 0.46019937194479754\n",
            "Iteration 212000, Loss: 0.4601889011860742\n",
            "Iteration 212100, Loss: 0.46017844228557997\n",
            "Iteration 212200, Loss: 0.46016799522618523\n",
            "Iteration 212300, Loss: 0.4601575599907916\n",
            "Iteration 212400, Loss: 0.46014713656233197\n",
            "Iteration 212500, Loss: 0.4601367249237706\n",
            "Iteration 212600, Loss: 0.46012632505810275\n",
            "Iteration 212700, Loss: 0.46011593694835506\n",
            "Iteration 212800, Loss: 0.46010556057758545\n",
            "Iteration 212900, Loss: 0.46009519592888215\n",
            "Iteration 213000, Loss: 0.46008484298536473\n",
            "Iteration 213100, Loss: 0.4600745017301838\n",
            "Iteration 213200, Loss: 0.46006417214652057\n",
            "Iteration 213300, Loss: 0.46005385421758666\n",
            "Iteration 213400, Loss: 0.46004354792662494\n",
            "Iteration 213500, Loss: 0.46003325325690814\n",
            "Iteration 213600, Loss: 0.4600229701917402\n",
            "Iteration 213700, Loss: 0.46001269871445516\n",
            "Iteration 213800, Loss: 0.4600024388084174\n",
            "Iteration 213900, Loss: 0.45999219045702167\n",
            "Iteration 214000, Loss: 0.459981953643693\n",
            "Iteration 214100, Loss: 0.4599717283518864\n",
            "Iteration 214200, Loss: 0.4599615145650872\n",
            "Iteration 214300, Loss: 0.4599513122668107\n",
            "Iteration 214400, Loss: 0.4599411214406022\n",
            "Iteration 214500, Loss: 0.4599309420700367\n",
            "Iteration 214600, Loss: 0.45992077413871923\n",
            "Iteration 214700, Loss: 0.4599106176302845\n",
            "Iteration 214800, Loss: 0.459900472528397\n",
            "Iteration 214900, Loss: 0.45989033881675057\n",
            "Iteration 215000, Loss: 0.45988021647906907\n",
            "Iteration 215100, Loss: 0.4598701054991054\n",
            "Iteration 215200, Loss: 0.45986000586064224\n",
            "Iteration 215300, Loss: 0.4598499175474913\n",
            "Iteration 215400, Loss: 0.459839840543494\n",
            "Iteration 215500, Loss: 0.4598297748325207\n",
            "Iteration 215600, Loss: 0.4598197203984709\n",
            "Iteration 215700, Loss: 0.4598096772252733\n",
            "Iteration 215800, Loss: 0.4597996452968858\n",
            "Iteration 215900, Loss: 0.45978962459729494\n",
            "Iteration 216000, Loss: 0.4597796151105163\n",
            "Iteration 216100, Loss: 0.4597696168205945\n",
            "Iteration 216200, Loss: 0.45975962971160256\n",
            "Iteration 216300, Loss: 0.45974965376764265\n",
            "Iteration 216400, Loss: 0.45973968897284534\n",
            "Iteration 216500, Loss: 0.4597297353113696\n",
            "Iteration 216600, Loss: 0.4597197927674035\n",
            "Iteration 216700, Loss: 0.45970986132516284\n",
            "Iteration 216800, Loss: 0.45969994096889233\n",
            "Iteration 216900, Loss: 0.4596900316828649\n",
            "Iteration 217000, Loss: 0.45968013345138187\n",
            "Iteration 217100, Loss: 0.4596702462587722\n",
            "Iteration 217200, Loss: 0.4596603700893936\n",
            "Iteration 217300, Loss: 0.4596505049276318\n",
            "Iteration 217400, Loss: 0.4596406507579003\n",
            "Iteration 217500, Loss: 0.4596308075646407\n",
            "Iteration 217600, Loss: 0.45962097533232243\n",
            "Iteration 217700, Loss: 0.45961115404544295\n",
            "Iteration 217800, Loss: 0.45960134368852723\n",
            "Iteration 217900, Loss: 0.459591544246128\n",
            "Iteration 218000, Loss: 0.45958175570282606\n",
            "Iteration 218100, Loss: 0.4595719780432287\n",
            "Iteration 218200, Loss: 0.45956221125197216\n",
            "Iteration 218300, Loss: 0.45955245531371924\n",
            "Iteration 218400, Loss: 0.4595427102131604\n",
            "Iteration 218500, Loss: 0.4595329759350133\n",
            "Iteration 218600, Loss: 0.45952325246402315\n",
            "Iteration 218700, Loss: 0.4595135397849621\n",
            "Iteration 218800, Loss: 0.45950383788262983\n",
            "Iteration 218900, Loss: 0.4594941467418525\n",
            "Iteration 219000, Loss: 0.4594844663474842\n",
            "Iteration 219100, Loss: 0.45947479668440505\n",
            "Iteration 219200, Loss: 0.4594651377375228\n",
            "Iteration 219300, Loss: 0.45945548949177195\n",
            "Iteration 219400, Loss: 0.4594458519321135\n",
            "Iteration 219500, Loss: 0.4594362250435355\n",
            "Iteration 219600, Loss: 0.45942660881105235\n",
            "Iteration 219700, Loss: 0.4594170032197056\n",
            "Iteration 219800, Loss: 0.459407408254563\n",
            "Iteration 219900, Loss: 0.4593978239007189\n",
            "Iteration 220000, Loss: 0.45938825014329415\n",
            "Iteration 220100, Loss: 0.4593786869674357\n",
            "Iteration 220200, Loss: 0.45936913435831744\n",
            "Iteration 220300, Loss: 0.4593595923011391\n",
            "Iteration 220400, Loss: 0.4593500607811266\n",
            "Iteration 220500, Loss: 0.4593405397835323\n",
            "Iteration 220600, Loss: 0.45933102929363456\n",
            "Iteration 220700, Loss: 0.4593215292967376\n",
            "Iteration 220800, Loss: 0.45931203977817203\n",
            "Iteration 220900, Loss: 0.45930256072329406\n",
            "Iteration 221000, Loss: 0.45929309211748603\n",
            "Iteration 221100, Loss: 0.4592836339461557\n",
            "Iteration 221200, Loss: 0.45927418619473726\n",
            "Iteration 221300, Loss: 0.45926474884868995\n",
            "Iteration 221400, Loss: 0.459255321893499\n",
            "Iteration 221500, Loss: 0.45924590531467535\n",
            "Iteration 221600, Loss: 0.45923649909775527\n",
            "Iteration 221700, Loss: 0.4592271032283005\n",
            "Iteration 221800, Loss: 0.4592177176918985\n",
            "Iteration 221900, Loss: 0.4592083424741617\n",
            "Iteration 222000, Loss: 0.45919897756072825\n",
            "Iteration 222100, Loss: 0.4591896229372614\n",
            "Iteration 222200, Loss: 0.4591802785894493\n",
            "Iteration 222300, Loss: 0.4591709445030059\n",
            "Iteration 222400, Loss: 0.45916162066366983\n",
            "Iteration 222500, Loss: 0.45915230705720483\n",
            "Iteration 222600, Loss: 0.4591430036693999\n",
            "Iteration 222700, Loss: 0.4591337104860685\n",
            "Iteration 222800, Loss: 0.45912442749304927\n",
            "Iteration 222900, Loss: 0.4591151546762056\n",
            "Iteration 223000, Loss: 0.4591058920214261\n",
            "Iteration 223100, Loss: 0.45909663951462343\n",
            "Iteration 223200, Loss: 0.45908739714173513\n",
            "Iteration 223300, Loss: 0.45907816488872355\n",
            "Iteration 223400, Loss: 0.4590689427415757\n",
            "Iteration 223500, Loss: 0.4590597306863027\n",
            "Iteration 223600, Loss: 0.45905052870894036\n",
            "Iteration 223700, Loss: 0.4590413367955488\n",
            "Iteration 223800, Loss: 0.45903215493221305\n",
            "Iteration 223900, Loss: 0.45902298310504136\n",
            "Iteration 224000, Loss: 0.4590138213001673\n",
            "Iteration 224100, Loss: 0.45900466950374796\n",
            "Iteration 224200, Loss: 0.458995527701965\n",
            "Iteration 224300, Loss: 0.4589863958810238\n",
            "Iteration 224400, Loss: 0.45897727402715405\n",
            "Iteration 224500, Loss: 0.45896816212660935\n",
            "Iteration 224600, Loss: 0.45895906016566707\n",
            "Iteration 224700, Loss: 0.458949968130629\n",
            "Iteration 224800, Loss: 0.45894088600782024\n",
            "Iteration 224900, Loss: 0.4589318137835897\n",
            "Iteration 225000, Loss: 0.45892275144431016\n",
            "Iteration 225100, Loss: 0.45891369897637846\n",
            "Iteration 225200, Loss: 0.45890465636621425\n",
            "Iteration 225300, Loss: 0.45889562360026165\n",
            "Iteration 225400, Loss: 0.4588866006649875\n",
            "Iteration 225500, Loss: 0.45887758754688274\n",
            "Iteration 225600, Loss: 0.4588685842324616\n",
            "Iteration 225700, Loss: 0.45885959070826116\n",
            "Iteration 225800, Loss: 0.4588506069608427\n",
            "Iteration 225900, Loss: 0.45884163297679026\n",
            "Iteration 226000, Loss: 0.4588326687427111\n",
            "Iteration 226100, Loss: 0.4588237142452359\n",
            "Iteration 226200, Loss: 0.45881476947101824\n",
            "Iteration 226300, Loss: 0.45880583440673495\n",
            "Iteration 226400, Loss: 0.4587969090390857\n",
            "Iteration 226500, Loss: 0.45878799335479337\n",
            "Iteration 226600, Loss: 0.45877908734060374\n",
            "Iteration 226700, Loss: 0.45877019098328536\n",
            "Iteration 226800, Loss: 0.45876130426962974\n",
            "Iteration 226900, Loss: 0.45875242718645115\n",
            "Iteration 227000, Loss: 0.4587435597205862\n",
            "Iteration 227100, Loss: 0.45873470185889503\n",
            "Iteration 227200, Loss: 0.45872585358825974\n",
            "Iteration 227300, Loss: 0.45871701489558525\n",
            "Iteration 227400, Loss: 0.45870818576779915\n",
            "Iteration 227500, Loss: 0.4586993661918512\n",
            "Iteration 227600, Loss: 0.45869055615471405\n",
            "Iteration 227700, Loss: 0.45868175564338237\n",
            "Iteration 227800, Loss: 0.4586729646448731\n",
            "Iteration 227900, Loss: 0.45866418314622637\n",
            "Iteration 228000, Loss: 0.4586554111345034\n",
            "Iteration 228100, Loss: 0.45864664859678844\n",
            "Iteration 228200, Loss: 0.4586378955201875\n",
            "Iteration 228300, Loss: 0.45862915189182885\n",
            "Iteration 228400, Loss: 0.45862041769886286\n",
            "Iteration 228500, Loss: 0.4586116929284618\n",
            "Iteration 228600, Loss: 0.45860297756782026\n",
            "Iteration 228700, Loss: 0.4585942716041543\n",
            "Iteration 228800, Loss: 0.45858557502470215\n",
            "Iteration 228900, Loss: 0.45857688781672384\n",
            "Iteration 229000, Loss: 0.45856820996750136\n",
            "Iteration 229100, Loss: 0.4585595414643381\n",
            "Iteration 229200, Loss: 0.4585508822945592\n",
            "Iteration 229300, Loss: 0.45854223244551195\n",
            "Iteration 229400, Loss: 0.45853359190456466\n",
            "Iteration 229500, Loss: 0.45852496065910736\n",
            "Iteration 229600, Loss: 0.4585163386965519\n",
            "Iteration 229700, Loss: 0.4585077260043313\n",
            "Iteration 229800, Loss: 0.45849912256990016\n",
            "Iteration 229900, Loss: 0.4584905283807344\n",
            "Iteration 230000, Loss: 0.4584819434243313\n",
            "Iteration 230100, Loss: 0.45847336768820934\n",
            "Iteration 230200, Loss: 0.4584648011599085\n",
            "Iteration 230300, Loss: 0.4584562438269897\n",
            "Iteration 230400, Loss: 0.4584476956770354\n",
            "Iteration 230500, Loss: 0.45843915669764856\n",
            "Iteration 230600, Loss: 0.4584306268764537\n",
            "Iteration 230700, Loss: 0.45842210620109647\n",
            "Iteration 230800, Loss: 0.4584135946592429\n",
            "Iteration 230900, Loss: 0.4584050922385807\n",
            "Iteration 231000, Loss: 0.45839659892681794\n",
            "Iteration 231100, Loss: 0.4583881147116838\n",
            "Iteration 231200, Loss: 0.4583796395809283\n",
            "Iteration 231300, Loss: 0.45837117352232204\n",
            "Iteration 231400, Loss: 0.4583627165236564\n",
            "Iteration 231500, Loss: 0.4583542685727435\n",
            "Iteration 231600, Loss: 0.4583458296574162\n",
            "Iteration 231700, Loss: 0.4583373997655277\n",
            "Iteration 231800, Loss: 0.45832897888495183\n",
            "Iteration 231900, Loss: 0.45832056700358326\n",
            "Iteration 232000, Loss: 0.45831216410933673\n",
            "Iteration 232100, Loss: 0.4583037701901475\n",
            "Iteration 232200, Loss: 0.45829538523397134\n",
            "Iteration 232300, Loss: 0.45828700922878396\n",
            "Iteration 232400, Loss: 0.45827864216258213\n",
            "Iteration 232500, Loss: 0.45827028402338216\n",
            "Iteration 232600, Loss: 0.4582619347992209\n",
            "Iteration 232700, Loss: 0.4582535944781551\n",
            "Iteration 232800, Loss: 0.4582452630482623\n",
            "Iteration 232900, Loss: 0.4582369404976392\n",
            "Iteration 233000, Loss: 0.45822862681440313\n",
            "Iteration 233100, Loss: 0.4582203219866916\n",
            "Iteration 233200, Loss: 0.45821202600266137\n",
            "Iteration 233300, Loss: 0.4582037388504898\n",
            "Iteration 233400, Loss: 0.45819546051837395\n",
            "Iteration 233500, Loss: 0.4581871909945303\n",
            "Iteration 233600, Loss: 0.45817893026719586\n",
            "Iteration 233700, Loss: 0.45817067832462677\n",
            "Iteration 233800, Loss: 0.4581624351550992\n",
            "Iteration 233900, Loss: 0.4581542007469088\n",
            "Iteration 234000, Loss: 0.45814597508837135\n",
            "Iteration 234100, Loss: 0.4581377581678215\n",
            "Iteration 234200, Loss: 0.45812954997361377\n",
            "Iteration 234300, Loss: 0.45812135049412234\n",
            "Iteration 234400, Loss: 0.45811315971774075\n",
            "Iteration 234500, Loss: 0.45810497763288205\n",
            "Iteration 234600, Loss: 0.4580968042279784\n",
            "Iteration 234700, Loss: 0.45808863949148154\n",
            "Iteration 234800, Loss: 0.4580804834118628\n",
            "Iteration 234900, Loss: 0.458072335977612\n",
            "Iteration 235000, Loss: 0.4580641971772392\n",
            "Iteration 235100, Loss: 0.45805606699927265\n",
            "Iteration 235200, Loss: 0.4580479454322605\n",
            "Iteration 235300, Loss: 0.4580398324647698\n",
            "Iteration 235400, Loss: 0.4580317280853865\n",
            "Iteration 235500, Loss: 0.4580236322827156\n",
            "Iteration 235600, Loss: 0.45801554504538144\n",
            "Iteration 235700, Loss: 0.458007466362027\n",
            "Iteration 235800, Loss: 0.45799939622131414\n",
            "Iteration 235900, Loss: 0.45799133461192376\n",
            "Iteration 236000, Loss: 0.4579832815225558\n",
            "Iteration 236100, Loss: 0.4579752369419283\n",
            "Iteration 236200, Loss: 0.45796720085877907\n",
            "Iteration 236300, Loss: 0.45795917326186386\n",
            "Iteration 236400, Loss: 0.45795115413995724\n",
            "Iteration 236500, Loss: 0.45794314348185294\n",
            "Iteration 236600, Loss: 0.45793514127636253\n",
            "Iteration 236700, Loss: 0.45792714751231656\n",
            "Iteration 236800, Loss: 0.45791916217856427\n",
            "Iteration 236900, Loss: 0.45791118526397306\n",
            "Iteration 237000, Loss: 0.45790321675742907\n",
            "Iteration 237100, Loss: 0.45789525664783676\n",
            "Iteration 237200, Loss: 0.4578873049241189\n",
            "Iteration 237300, Loss: 0.4578793615752165\n",
            "Iteration 237400, Loss: 0.4578714265900894\n",
            "Iteration 237500, Loss: 0.45786349995771514\n",
            "Iteration 237600, Loss: 0.4578555816670895\n",
            "Iteration 237700, Loss: 0.45784767170722707\n",
            "Iteration 237800, Loss: 0.45783977006716\n",
            "Iteration 237900, Loss: 0.45783187673593884\n",
            "Iteration 238000, Loss: 0.457823991702632\n",
            "Iteration 238100, Loss: 0.4578161149563264\n",
            "Iteration 238200, Loss: 0.4578082464861263\n",
            "Iteration 238300, Loss: 0.4578003862811546\n",
            "Iteration 238400, Loss: 0.45779253433055156\n",
            "Iteration 238500, Loss: 0.45778469062347593\n",
            "Iteration 238600, Loss: 0.45777685514910377\n",
            "Iteration 238700, Loss: 0.45776902789662943\n",
            "Iteration 238800, Loss: 0.45776120885526483\n",
            "Iteration 238900, Loss: 0.45775339801423964\n",
            "Iteration 239000, Loss: 0.4577455953628012\n",
            "Iteration 239100, Loss: 0.4577378008902148\n",
            "Iteration 239200, Loss: 0.45773001458576285\n",
            "Iteration 239300, Loss: 0.4577222364387463\n",
            "Iteration 239400, Loss: 0.4577144664384827\n",
            "Iteration 239500, Loss: 0.45770670457430784\n",
            "Iteration 239600, Loss: 0.45769895083557444\n",
            "Iteration 239700, Loss: 0.4576912052116532\n",
            "Iteration 239800, Loss: 0.4576834676919319\n",
            "Iteration 239900, Loss: 0.4576757382658161\n",
            "Iteration 240000, Loss: 0.45766801692272835\n",
            "Iteration 240100, Loss: 0.4576603036521088\n",
            "Iteration 240200, Loss: 0.4576525984434147\n",
            "Iteration 240300, Loss: 0.4576449012861208\n",
            "Iteration 240400, Loss: 0.4576372121697187\n",
            "Iteration 240500, Loss: 0.4576295310837177\n",
            "Iteration 240600, Loss: 0.45762185801764366\n",
            "Iteration 240700, Loss: 0.4576141929610403\n",
            "Iteration 240800, Loss: 0.4576065359034675\n",
            "Iteration 240900, Loss: 0.45759888683450334\n",
            "Iteration 241000, Loss: 0.45759124574374205\n",
            "Iteration 241100, Loss: 0.45758361262079483\n",
            "Iteration 241200, Loss: 0.4575759874552904\n",
            "Iteration 241300, Loss: 0.4575683702368742\n",
            "Iteration 241400, Loss: 0.45756076095520815\n",
            "Iteration 241500, Loss: 0.4575531595999717\n",
            "Iteration 241600, Loss: 0.45754556616086045\n",
            "Iteration 241700, Loss: 0.4575379806275872\n",
            "Iteration 241800, Loss: 0.45753040298988135\n",
            "Iteration 241900, Loss: 0.4575228332374892\n",
            "Iteration 242000, Loss: 0.45751527136017356\n",
            "Iteration 242100, Loss: 0.4575077173477137\n",
            "Iteration 242200, Loss: 0.4575001711899061\n",
            "Iteration 242300, Loss: 0.45749263287656317\n",
            "Iteration 242400, Loss: 0.4574851023975141\n",
            "Iteration 242500, Loss: 0.45747757974260517\n",
            "Iteration 242600, Loss: 0.457470064901698\n",
            "Iteration 242700, Loss: 0.4574625578646716\n",
            "Iteration 242800, Loss: 0.4574550586214211\n",
            "Iteration 242900, Loss: 0.4574475671618579\n",
            "Iteration 243000, Loss: 0.45744008347591003\n",
            "Iteration 243100, Loss: 0.45743260755352155\n",
            "Iteration 243200, Loss: 0.457425139384653\n",
            "Iteration 243300, Loss: 0.45741767895928115\n",
            "Iteration 243400, Loss: 0.45741022626739875\n",
            "Iteration 243500, Loss: 0.4574027812990152\n",
            "Iteration 243600, Loss: 0.4573953440441556\n",
            "Iteration 243700, Loss: 0.4573879144928615\n",
            "Iteration 243800, Loss: 0.4573804926351905\n",
            "Iteration 243900, Loss: 0.457373078461216\n",
            "Iteration 244000, Loss: 0.4573656719610278\n",
            "Iteration 244100, Loss: 0.45735827312473143\n",
            "Iteration 244200, Loss: 0.45735088194244844\n",
            "Iteration 244300, Loss: 0.4573434984043166\n",
            "Iteration 244400, Loss: 0.45733612250048905\n",
            "Iteration 244500, Loss: 0.4573287542211352\n",
            "Iteration 244600, Loss: 0.4573213935564402\n",
            "Iteration 244700, Loss: 0.45731404049660523\n",
            "Iteration 244800, Loss: 0.4573066950318467\n",
            "Iteration 244900, Loss: 0.4572993571523973\n",
            "Iteration 245000, Loss: 0.457292026848505\n",
            "Iteration 245100, Loss: 0.4572847041104341\n",
            "Iteration 245200, Loss: 0.4572773889284637\n",
            "Iteration 245300, Loss: 0.4572700812928894\n",
            "Iteration 245400, Loss: 0.45726278119402153\n",
            "Iteration 245500, Loss: 0.4572554886221866\n",
            "Iteration 245600, Loss: 0.45724820356772666\n",
            "Iteration 245700, Loss: 0.4572409260209989\n",
            "Iteration 245800, Loss: 0.4572336559723761\n",
            "Iteration 245900, Loss: 0.4572263934122465\n",
            "Iteration 246000, Loss: 0.45721913833101396\n",
            "Iteration 246100, Loss: 0.4572118907190974\n",
            "Iteration 246200, Loss: 0.45720465056693116\n",
            "Iteration 246300, Loss: 0.45719741786496515\n",
            "Iteration 246400, Loss: 0.45719019260366417\n",
            "Iteration 246500, Loss: 0.4571829747735088\n",
            "Iteration 246600, Loss: 0.45717576436499424\n",
            "Iteration 246700, Loss: 0.4571685613686312\n",
            "Iteration 246800, Loss: 0.4571613657749456\n",
            "Iteration 246900, Loss: 0.4571541775744787\n",
            "Iteration 247000, Loss: 0.4571469967577863\n",
            "Iteration 247100, Loss: 0.45713982331543956\n",
            "Iteration 247200, Loss: 0.45713265723802493\n",
            "Iteration 247300, Loss: 0.4571254985161436\n",
            "Iteration 247400, Loss: 0.4571183471404117\n",
            "Iteration 247500, Loss: 0.45711120310146075\n",
            "Iteration 247600, Loss: 0.4571040663899366\n",
            "Iteration 247700, Loss: 0.45709693699650045\n",
            "Iteration 247800, Loss: 0.45708981491182804\n",
            "Iteration 247900, Loss: 0.4570827001266105\n",
            "Iteration 248000, Loss: 0.4570755926315531\n",
            "Iteration 248100, Loss: 0.4570684924173762\n",
            "Iteration 248200, Loss: 0.4570613994748152\n",
            "Iteration 248300, Loss: 0.45705431379461975\n",
            "Iteration 248400, Loss: 0.45704723536755437\n",
            "Iteration 248500, Loss: 0.45704016418439847\n",
            "Iteration 248600, Loss: 0.457033100235946\n",
            "Iteration 248700, Loss: 0.4570260435130051\n",
            "Iteration 248800, Loss: 0.45701899400639895\n",
            "Iteration 248900, Loss: 0.4570119517069653\n",
            "Iteration 249000, Loss: 0.4570049166055563\n",
            "Iteration 249100, Loss: 0.4569978886930386\n",
            "Iteration 249200, Loss: 0.4569908679602934\n",
            "Iteration 249300, Loss: 0.45698385439821626\n",
            "Iteration 249400, Loss: 0.456976847997717\n",
            "Iteration 249500, Loss: 0.4569698487497201\n",
            "Iteration 249600, Loss: 0.45696285664516445\n",
            "Iteration 249700, Loss: 0.4569558716750029\n",
            "Iteration 249800, Loss: 0.456948893830203\n",
            "Iteration 249900, Loss: 0.4569419231017463\n",
            "Iteration 250000, Loss: 0.4569349594806289\n",
            "Iteration 250100, Loss: 0.45692800295786073\n",
            "Iteration 250200, Loss: 0.4569210535244661\n",
            "Iteration 250300, Loss: 0.4569141111714837\n",
            "Iteration 250400, Loss: 0.4569071758899661\n",
            "Iteration 250500, Loss: 0.4569002476709798\n",
            "Iteration 250600, Loss: 0.45689332650560605\n",
            "Iteration 250700, Loss: 0.45688641238493943\n",
            "Iteration 250800, Loss: 0.4568795053000889\n",
            "Iteration 250900, Loss: 0.45687260524217743\n",
            "Iteration 251000, Loss: 0.456865712202342\n",
            "Iteration 251100, Loss: 0.45685882617173323\n",
            "Iteration 251200, Loss: 0.45685194714151617\n",
            "Iteration 251300, Loss: 0.45684507510286904\n",
            "Iteration 251400, Loss: 0.45683821004698494\n",
            "Iteration 251500, Loss: 0.45683135196506974\n",
            "Iteration 251600, Loss: 0.4568245008483441\n",
            "Iteration 251700, Loss: 0.4568176566880416\n",
            "Iteration 251800, Loss: 0.45681081947541013\n",
            "Iteration 251900, Loss: 0.45680398920171106\n",
            "Iteration 252000, Loss: 0.4567971658582197\n",
            "Iteration 252100, Loss: 0.45679034943622504\n",
            "Iteration 252200, Loss: 0.45678353992702914\n",
            "Iteration 252300, Loss: 0.45677673732194857\n",
            "Iteration 252400, Loss: 0.45676994161231294\n",
            "Iteration 252500, Loss: 0.45676315278946567\n",
            "Iteration 252600, Loss: 0.4567563708447635\n",
            "Iteration 252700, Loss: 0.45674959576957674\n",
            "Iteration 252800, Loss: 0.4567428275552896\n",
            "Iteration 252900, Loss: 0.4567360661932993\n",
            "Iteration 253000, Loss: 0.45672931167501646\n",
            "Iteration 253100, Loss: 0.45672256399186584\n",
            "Iteration 253200, Loss: 0.4567158231352846\n",
            "Iteration 253300, Loss: 0.4567090890967238\n",
            "Iteration 253400, Loss: 0.45670236186764807\n",
            "Iteration 253500, Loss: 0.45669564143953506\n",
            "Iteration 253600, Loss: 0.4566889278038755\n",
            "Iteration 253700, Loss: 0.4566822209521738\n",
            "Iteration 253800, Loss: 0.45667552087594726\n",
            "Iteration 253900, Loss: 0.4566688275667269\n",
            "Iteration 254000, Loss: 0.4566621410160563\n",
            "Iteration 254100, Loss: 0.4566554612154928\n",
            "Iteration 254200, Loss: 0.45664878815660637\n",
            "Iteration 254300, Loss: 0.45664212183098035\n",
            "Iteration 254400, Loss: 0.4566354622302113\n",
            "Iteration 254500, Loss: 0.45662880934590855\n",
            "Iteration 254600, Loss: 0.4566221631696946\n",
            "Iteration 254700, Loss: 0.45661552369320524\n",
            "Iteration 254800, Loss: 0.45660889090808876\n",
            "Iteration 254900, Loss: 0.45660226480600663\n",
            "Iteration 255000, Loss: 0.45659564537863356\n",
            "Iteration 255100, Loss: 0.45658903261765665\n",
            "Iteration 255200, Loss: 0.4565824265147762\n",
            "Iteration 255300, Loss: 0.4565758270617053\n",
            "Iteration 255400, Loss: 0.4565692342501702\n",
            "Iteration 255500, Loss: 0.4565626480719093\n",
            "Iteration 255600, Loss: 0.4565560685186745\n",
            "Iteration 255700, Loss: 0.4565494955822298\n",
            "Iteration 255800, Loss: 0.45654292925435275\n",
            "Iteration 255900, Loss: 0.45653636952683263\n",
            "Iteration 256000, Loss: 0.4565298163914724\n",
            "Iteration 256100, Loss: 0.4565232698400872\n",
            "Iteration 256200, Loss: 0.45651672986450464\n",
            "Iteration 256300, Loss: 0.45651019645656554\n",
            "Iteration 256400, Loss: 0.45650366960812266\n",
            "Iteration 256500, Loss: 0.45649714931104185\n",
            "Iteration 256600, Loss: 0.4564906355572014\n",
            "Iteration 256700, Loss: 0.4564841283384917\n",
            "Iteration 256800, Loss: 0.45647762764681626\n",
            "Iteration 256900, Loss: 0.4564711334740909\n",
            "Iteration 257000, Loss: 0.4564646458122436\n",
            "Iteration 257100, Loss: 0.4564581646532152\n",
            "Iteration 257200, Loss: 0.45645168998895874\n",
            "Iteration 257300, Loss: 0.45644522181143937\n",
            "Iteration 257400, Loss: 0.45643876011263523\n",
            "Iteration 257500, Loss: 0.45643230488453634\n",
            "Iteration 257600, Loss: 0.4564258561191452\n",
            "Iteration 257700, Loss: 0.4564194138084765\n",
            "Iteration 257800, Loss: 0.4564129779445574\n",
            "Iteration 257900, Loss: 0.456406548519427\n",
            "Iteration 258000, Loss: 0.4564001255251373\n",
            "Iteration 258100, Loss: 0.45639370895375136\n",
            "Iteration 258200, Loss: 0.45638729879734563\n",
            "Iteration 258300, Loss: 0.4563808950480076\n",
            "Iteration 258400, Loss: 0.4563744976978383\n",
            "Iteration 258500, Loss: 0.4563681067389494\n",
            "Iteration 258600, Loss: 0.45636172216346527\n",
            "Iteration 258700, Loss: 0.45635534396352284\n",
            "Iteration 258800, Loss: 0.45634897213127035\n",
            "Iteration 258900, Loss: 0.4563426066588685\n",
            "Iteration 259000, Loss: 0.45633624753848956\n",
            "Iteration 259100, Loss: 0.45632989476231833\n",
            "Iteration 259200, Loss: 0.456323548322551\n",
            "Iteration 259300, Loss: 0.4563172082113964\n",
            "Iteration 259400, Loss: 0.4563108744210744\n",
            "Iteration 259500, Loss: 0.4563045469438173\n",
            "Iteration 259600, Loss: 0.45629822577186946\n",
            "Iteration 259700, Loss: 0.45629191089748655\n",
            "Iteration 259800, Loss: 0.45628560231293636\n",
            "Iteration 259900, Loss: 0.4562793000104984\n",
            "Iteration 260000, Loss: 0.456273003982464\n",
            "Iteration 260100, Loss: 0.45626671422113624\n",
            "Iteration 260200, Loss: 0.45626043071883005\n",
            "Iteration 260300, Loss: 0.4562541534678715\n",
            "Iteration 260400, Loss: 0.45624788246059933\n",
            "Iteration 260500, Loss: 0.4562416176893632\n",
            "Iteration 260600, Loss: 0.45623535914652447\n",
            "Iteration 260700, Loss: 0.4562291068244566\n",
            "Iteration 260800, Loss: 0.4562228607155439\n",
            "Iteration 260900, Loss: 0.45621662081218334\n",
            "Iteration 261000, Loss: 0.45621038710678197\n",
            "Iteration 261100, Loss: 0.45620415959175986\n",
            "Iteration 261200, Loss: 0.4561979382595478\n",
            "Iteration 261300, Loss: 0.45619172310258804\n",
            "Iteration 261400, Loss: 0.45618551411333463\n",
            "Iteration 261500, Loss: 0.4561793112842528\n",
            "Iteration 261600, Loss: 0.4561731146078197\n",
            "Iteration 261700, Loss: 0.45616692407652304\n",
            "Iteration 261800, Loss: 0.45616073968286275\n",
            "Iteration 261900, Loss: 0.4561545614193496\n",
            "Iteration 262000, Loss: 0.4561483892785061\n",
            "Iteration 262100, Loss: 0.45614222325286563\n",
            "Iteration 262200, Loss: 0.45613606333497325\n",
            "Iteration 262300, Loss: 0.45612990951738513\n",
            "Iteration 262400, Loss: 0.4561237617926688\n",
            "Iteration 262500, Loss: 0.45611762015340285\n",
            "Iteration 262600, Loss: 0.4561114845921774\n",
            "Iteration 262700, Loss: 0.45610535510159367\n",
            "Iteration 262800, Loss: 0.4560992316742636\n",
            "Iteration 262900, Loss: 0.45609311430281085\n",
            "Iteration 263000, Loss: 0.4560870029798702\n",
            "Iteration 263100, Loss: 0.4560808976980873\n",
            "Iteration 263200, Loss: 0.4560747984501188\n",
            "Iteration 263300, Loss: 0.4560687052286328\n",
            "Iteration 263400, Loss: 0.4560626180263082\n",
            "Iteration 263500, Loss: 0.4560565368358349\n",
            "Iteration 263600, Loss: 0.4560504616499141\n",
            "Iteration 263700, Loss: 0.4560443924612577\n",
            "Iteration 263800, Loss: 0.45603832926258864\n",
            "Iteration 263900, Loss: 0.456032272046641\n",
            "Iteration 264000, Loss: 0.45602622080615945\n",
            "Iteration 264100, Loss: 0.45602017553389995\n",
            "Iteration 264200, Loss: 0.45601413622262915\n",
            "Iteration 264300, Loss: 0.4560081028651246\n",
            "Iteration 264400, Loss: 0.45600207545417465\n",
            "Iteration 264500, Loss: 0.4559960539825785\n",
            "Iteration 264600, Loss: 0.4559900384431463\n",
            "Iteration 264700, Loss: 0.4559840288286989\n",
            "Iteration 264800, Loss: 0.45597802513206764\n",
            "Iteration 264900, Loss: 0.4559720273460955\n",
            "Iteration 265000, Loss: 0.455966035463635\n",
            "Iteration 265100, Loss: 0.4559600494775502\n",
            "Iteration 265200, Loss: 0.45595406938071587\n",
            "Iteration 265300, Loss: 0.45594809516601686\n",
            "Iteration 265400, Loss: 0.45594212682634905\n",
            "Iteration 265500, Loss: 0.455936164354619\n",
            "Iteration 265600, Loss: 0.4559302077437439\n",
            "Iteration 265700, Loss: 0.45592425698665145\n",
            "Iteration 265800, Loss: 0.45591831207627975\n",
            "Iteration 265900, Loss: 0.4559123730055779\n",
            "Iteration 266000, Loss: 0.455906439767505\n",
            "Iteration 266100, Loss: 0.45590051235503115\n",
            "Iteration 266200, Loss: 0.4558945907611368\n",
            "Iteration 266300, Loss: 0.45588867497881264\n",
            "Iteration 266400, Loss: 0.4558827650010603\n",
            "Iteration 266500, Loss: 0.45587686082089124\n",
            "Iteration 266600, Loss: 0.4558709624313279\n",
            "Iteration 266700, Loss: 0.4558650698254029\n",
            "Iteration 266800, Loss: 0.45585918299615913\n",
            "Iteration 266900, Loss: 0.4558533019366502\n",
            "Iteration 267000, Loss: 0.45584742663993955\n",
            "Iteration 267100, Loss: 0.45584155709910146\n",
            "Iteration 267200, Loss: 0.4558356933072203\n",
            "Iteration 267300, Loss: 0.4558298352573905\n",
            "Iteration 267400, Loss: 0.4558239829427172\n",
            "Iteration 267500, Loss: 0.4558181363563154\n",
            "Iteration 267600, Loss: 0.45581229549131097\n",
            "Iteration 267700, Loss: 0.455806460340839\n",
            "Iteration 267800, Loss: 0.4558006308980458\n",
            "Iteration 267900, Loss: 0.4557948071560871\n",
            "Iteration 268000, Loss: 0.4557889891081292\n",
            "Iteration 268100, Loss: 0.45578317674734836\n",
            "Iteration 268200, Loss: 0.4557773700669312\n",
            "Iteration 268300, Loss: 0.45577156906007404\n",
            "Iteration 268400, Loss: 0.45576577371998384\n",
            "Iteration 268500, Loss: 0.455759984039877\n",
            "Iteration 268600, Loss: 0.45575420001298056\n",
            "Iteration 268700, Loss: 0.4557484216325312\n",
            "Iteration 268800, Loss: 0.45574264889177585\n",
            "Iteration 268900, Loss: 0.4557368817839711\n",
            "Iteration 269000, Loss: 0.4557311203023841\n",
            "Iteration 269100, Loss: 0.45572536444029127\n",
            "Iteration 269200, Loss: 0.4557196141909796\n",
            "Iteration 269300, Loss: 0.45571386954774556\n",
            "Iteration 269400, Loss: 0.4557081305038958\n",
            "Iteration 269500, Loss: 0.4557023970527467\n",
            "Iteration 269600, Loss: 0.45569666918762447\n",
            "Iteration 269700, Loss: 0.45569094690186546\n",
            "Iteration 269800, Loss: 0.4556852301888153\n",
            "Iteration 269900, Loss: 0.45567951904183035\n",
            "Iteration 270000, Loss: 0.4556738134542759\n",
            "Iteration 270100, Loss: 0.4556681134195274\n",
            "Iteration 270200, Loss: 0.45566241893097\n",
            "Iteration 270300, Loss: 0.45565672998199847\n",
            "Iteration 270400, Loss: 0.4556510465660177\n",
            "Iteration 270500, Loss: 0.45564536867644184\n",
            "Iteration 270600, Loss: 0.4556396963066952\n",
            "Iteration 270700, Loss: 0.45563402945021125\n",
            "Iteration 270800, Loss: 0.45562836810043345\n",
            "Iteration 270900, Loss: 0.45562271225081474\n",
            "Iteration 271000, Loss: 0.45561706189481815\n",
            "Iteration 271100, Loss: 0.45561141702591557\n",
            "Iteration 271200, Loss: 0.455605777637589\n",
            "Iteration 271300, Loss: 0.4556001437233297\n",
            "Iteration 271400, Loss: 0.45559451527663897\n",
            "Iteration 271500, Loss: 0.45558889229102706\n",
            "Iteration 271600, Loss: 0.45558327476001426\n",
            "Iteration 271700, Loss: 0.45557766267712996\n",
            "Iteration 271800, Loss: 0.45557205603591316\n",
            "Iteration 271900, Loss: 0.45556645482991254\n",
            "Iteration 272000, Loss: 0.4555608590526859\n",
            "Iteration 272100, Loss: 0.45555526869780094\n",
            "Iteration 272200, Loss: 0.4555496837588341\n",
            "Iteration 272300, Loss: 0.4555441042293719\n",
            "Iteration 272400, Loss: 0.4555385301030098\n",
            "Iteration 272500, Loss: 0.45553296137335286\n",
            "Iteration 272600, Loss: 0.4555273980340154\n",
            "Iteration 272700, Loss: 0.4555218400786212\n",
            "Iteration 272800, Loss: 0.45551628750080325\n",
            "Iteration 272900, Loss: 0.4555107402942037\n",
            "Iteration 273000, Loss: 0.45550519845247434\n",
            "Iteration 273100, Loss: 0.4554996619692757\n",
            "Iteration 273200, Loss: 0.4554941308382783\n",
            "Iteration 273300, Loss: 0.45548860505316124\n",
            "Iteration 273400, Loss: 0.4554830846076131\n",
            "Iteration 273500, Loss: 0.4554775694953319\n",
            "Iteration 273600, Loss: 0.45547205971002436\n",
            "Iteration 273700, Loss: 0.45546655524540675\n",
            "Iteration 273800, Loss: 0.45546105609520426\n",
            "Iteration 273900, Loss: 0.45545556225315154\n",
            "Iteration 274000, Loss: 0.45545007371299173\n",
            "Iteration 274100, Loss: 0.45544459046847785\n",
            "Iteration 274200, Loss: 0.4554391125133716\n",
            "Iteration 274300, Loss: 0.4554336398414438\n",
            "Iteration 274400, Loss: 0.45542817244647416\n",
            "Iteration 274500, Loss: 0.45542271032225196\n",
            "Iteration 274600, Loss: 0.4554172534625748\n",
            "Iteration 274700, Loss: 0.45541180186124974\n",
            "Iteration 274800, Loss: 0.455406355512093\n",
            "Iteration 274900, Loss: 0.45540091440892916\n",
            "Iteration 275000, Loss: 0.45539547854559237\n",
            "Iteration 275100, Loss: 0.45539004791592524\n",
            "Iteration 275200, Loss: 0.45538462251377987\n",
            "Iteration 275300, Loss: 0.4553792023330166\n",
            "Iteration 275400, Loss: 0.45537378736750544\n",
            "Iteration 275500, Loss: 0.4553683776111244\n",
            "Iteration 275600, Loss: 0.455362973057761\n",
            "Iteration 275700, Loss: 0.4553575737013116\n",
            "Iteration 275800, Loss: 0.4553521795356812\n",
            "Iteration 275900, Loss: 0.4553467905547833\n",
            "Iteration 276000, Loss: 0.455341406752541\n",
            "Iteration 276100, Loss: 0.45533602812288554\n",
            "Iteration 276200, Loss: 0.45533065465975703\n",
            "Iteration 276300, Loss: 0.4553252863571047\n",
            "Iteration 276400, Loss: 0.45531992320888603\n",
            "Iteration 276500, Loss: 0.45531456520906777\n",
            "Iteration 276600, Loss: 0.45530921235162475\n",
            "Iteration 276700, Loss: 0.455303864630541\n",
            "Iteration 276800, Loss: 0.4552985220398089\n",
            "Iteration 276900, Loss: 0.45529318457343\n",
            "Iteration 277000, Loss: 0.45528785222541385\n",
            "Iteration 277100, Loss: 0.4552825249897791\n",
            "Iteration 277200, Loss: 0.4552772028605526\n",
            "Iteration 277300, Loss: 0.45527188583177036\n",
            "Iteration 277400, Loss: 0.4552665738974767\n",
            "Iteration 277500, Loss: 0.45526126705172437\n",
            "Iteration 277600, Loss: 0.4552559652885748\n",
            "Iteration 277700, Loss: 0.45525066860209806\n",
            "Iteration 277800, Loss: 0.45524537698637246\n",
            "Iteration 277900, Loss: 0.45524009043548547\n",
            "Iteration 278000, Loss: 0.4552348089435323\n",
            "Iteration 278100, Loss: 0.4552295325046169\n",
            "Iteration 278200, Loss: 0.455224261112852\n",
            "Iteration 278300, Loss: 0.45521899476235855\n",
            "Iteration 278400, Loss: 0.4552137334472656\n",
            "Iteration 278500, Loss: 0.45520847716171126\n",
            "Iteration 278600, Loss: 0.4552032258998418\n",
            "Iteration 278700, Loss: 0.45519797965581166\n",
            "Iteration 278800, Loss: 0.45519273842378394\n",
            "Iteration 278900, Loss: 0.4551875021979299\n",
            "Iteration 279000, Loss: 0.45518227097242936\n",
            "Iteration 279100, Loss: 0.4551770447414704\n",
            "Iteration 279200, Loss: 0.45517182349924923\n",
            "Iteration 279300, Loss: 0.4551666072399709\n",
            "Iteration 279400, Loss: 0.45516139595784805\n",
            "Iteration 279500, Loss: 0.45515618964710214\n",
            "Iteration 279600, Loss: 0.45515098830196266\n",
            "Iteration 279700, Loss: 0.4551457919166674\n",
            "Iteration 279800, Loss: 0.45514060048546234\n",
            "Iteration 279900, Loss: 0.4551354140026016\n",
            "Iteration 280000, Loss: 0.4551302324623479\n",
            "Iteration 280100, Loss: 0.45512505585897184\n",
            "Iteration 280200, Loss: 0.4551198841867521\n",
            "Iteration 280300, Loss: 0.4551147174399757\n",
            "Iteration 280400, Loss: 0.4551095556129379\n",
            "Iteration 280500, Loss: 0.45510439869994174\n",
            "Iteration 280600, Loss: 0.455099246695299\n",
            "Iteration 280700, Loss: 0.4550940995933288\n",
            "Iteration 280800, Loss: 0.455088957388359\n",
            "Iteration 280900, Loss: 0.4550838200747251\n",
            "Iteration 281000, Loss: 0.4550786876467709\n",
            "Iteration 281100, Loss: 0.45507356009884836\n",
            "Iteration 281200, Loss: 0.4550684374253171\n",
            "Iteration 281300, Loss: 0.45506331962054525\n",
            "Iteration 281400, Loss: 0.45505820667890834\n",
            "Iteration 281500, Loss: 0.4550530985947905\n",
            "Iteration 281600, Loss: 0.45504799536258356\n",
            "Iteration 281700, Loss: 0.4550428969766873\n",
            "Iteration 281800, Loss: 0.4550378034315095\n",
            "Iteration 281900, Loss: 0.45503271472146595\n",
            "Iteration 282000, Loss: 0.45502763084098014\n",
            "Iteration 282100, Loss: 0.4550225517844837\n",
            "Iteration 282200, Loss: 0.4550174775464161\n",
            "Iteration 282300, Loss: 0.4550124081212248\n",
            "Iteration 282400, Loss: 0.45500734350336497\n",
            "Iteration 282500, Loss: 0.45500228368729956\n",
            "Iteration 282600, Loss: 0.45499722866749953\n",
            "Iteration 282700, Loss: 0.4549921784384438\n",
            "Iteration 282800, Loss: 0.45498713299461874\n",
            "Iteration 282900, Loss: 0.4549820923305191\n",
            "Iteration 283000, Loss: 0.45497705644064657\n",
            "Iteration 283100, Loss: 0.45497202531951136\n",
            "Iteration 283200, Loss: 0.45496699896163123\n",
            "Iteration 283300, Loss: 0.4549619773615316\n",
            "Iteration 283400, Loss: 0.45495696051374557\n",
            "Iteration 283500, Loss: 0.4549519484128144\n",
            "Iteration 283600, Loss: 0.45494694105328615\n",
            "Iteration 283700, Loss: 0.45494193842971775\n",
            "Iteration 283800, Loss: 0.45493694053667294\n",
            "Iteration 283900, Loss: 0.45493194736872344\n",
            "Iteration 284000, Loss: 0.4549269589204487\n",
            "Iteration 284100, Loss: 0.4549219751864357\n",
            "Iteration 284200, Loss: 0.45491699616127906\n",
            "Iteration 284300, Loss: 0.454912021839581\n",
            "Iteration 284400, Loss: 0.4549070522159513\n",
            "Iteration 284500, Loss: 0.45490208728500775\n",
            "Iteration 284600, Loss: 0.4548971270413749\n",
            "Iteration 284700, Loss: 0.45489217147968586\n",
            "Iteration 284800, Loss: 0.4548872205945804\n",
            "Iteration 284900, Loss: 0.4548822743807064\n",
            "Iteration 285000, Loss: 0.45487733283271903\n",
            "Iteration 285100, Loss: 0.45487239594528095\n",
            "Iteration 285200, Loss: 0.45486746371306275\n",
            "Iteration 285300, Loss: 0.4548625361307419\n",
            "Iteration 285400, Loss: 0.4548576131930034\n",
            "Iteration 285500, Loss: 0.45485269489454033\n",
            "Iteration 285600, Loss: 0.4548477812300526\n",
            "Iteration 285700, Loss: 0.4548428721942479\n",
            "Iteration 285800, Loss: 0.45483796778184105\n",
            "Iteration 285900, Loss: 0.45483306798755446\n",
            "Iteration 286000, Loss: 0.4548281728061181\n",
            "Iteration 286100, Loss: 0.45482328223226914\n",
            "Iteration 286200, Loss: 0.454818396260752\n",
            "Iteration 286300, Loss: 0.4548135148863187\n",
            "Iteration 286400, Loss: 0.4548086381037286\n",
            "Iteration 286500, Loss: 0.45480376590774835\n",
            "Iteration 286600, Loss: 0.4547988982931516\n",
            "Iteration 286700, Loss: 0.45479403525471973\n",
            "Iteration 286800, Loss: 0.45478917678724146\n",
            "Iteration 286900, Loss: 0.45478432288551257\n",
            "Iteration 287000, Loss: 0.45477947354433623\n",
            "Iteration 287100, Loss: 0.45477462875852254\n",
            "Iteration 287200, Loss: 0.4547697885228894\n",
            "Iteration 287300, Loss: 0.4547649528322617\n",
            "Iteration 287400, Loss: 0.4547601216814714\n",
            "Iteration 287500, Loss: 0.4547552950653577\n",
            "Iteration 287600, Loss: 0.4547504729787674\n",
            "Iteration 287700, Loss: 0.454745655416554\n",
            "Iteration 287800, Loss: 0.45474084237357854\n",
            "Iteration 287900, Loss: 0.45473603384470884\n",
            "Iteration 288000, Loss: 0.4547312298248205\n",
            "Iteration 288100, Loss: 0.45472643030879534\n",
            "Iteration 288200, Loss: 0.45472163529152304\n",
            "Iteration 288300, Loss: 0.45471684476790025\n",
            "Iteration 288400, Loss: 0.45471205873283066\n",
            "Iteration 288500, Loss: 0.4547072771812249\n",
            "Iteration 288600, Loss: 0.45470250010800106\n",
            "Iteration 288700, Loss: 0.45469772750808385\n",
            "Iteration 288800, Loss: 0.45469295937640547\n",
            "Iteration 288900, Loss: 0.45468819570790464\n",
            "Iteration 289000, Loss: 0.45468343649752757\n",
            "Iteration 289100, Loss: 0.4546786817402275\n",
            "Iteration 289200, Loss: 0.4546739314309644\n",
            "Iteration 289300, Loss: 0.45466918556470515\n",
            "Iteration 289400, Loss: 0.45466444413642404\n",
            "Iteration 289500, Loss: 0.45465970714110215\n",
            "Iteration 289600, Loss: 0.4546549745737274\n",
            "Iteration 289700, Loss: 0.45465024642929486\n",
            "Iteration 289800, Loss: 0.4546455227028062\n",
            "Iteration 289900, Loss: 0.4546408033892704\n",
            "Iteration 290000, Loss: 0.45463608848370307\n",
            "Iteration 290100, Loss: 0.45463137798112696\n",
            "Iteration 290200, Loss: 0.4546266718765717\n",
            "Iteration 290300, Loss: 0.45462197016507333\n",
            "Iteration 290400, Loss: 0.4546172728416755\n",
            "Iteration 290500, Loss: 0.45461257990142806\n",
            "Iteration 290600, Loss: 0.4546078913393884\n",
            "Iteration 290700, Loss: 0.45460320715061986\n",
            "Iteration 290800, Loss: 0.45459852733019324\n",
            "Iteration 290900, Loss: 0.4545938518731862\n",
            "Iteration 291000, Loss: 0.45458918077468263\n",
            "Iteration 291100, Loss: 0.45458451402977373\n",
            "Iteration 291200, Loss: 0.4545798516335575\n",
            "Iteration 291300, Loss: 0.4545751935811382\n",
            "Iteration 291400, Loss: 0.45457053986762724\n",
            "Iteration 291500, Loss: 0.45456589048814267\n",
            "Iteration 291600, Loss: 0.45456124543780957\n",
            "Iteration 291700, Loss: 0.4545566047117589\n",
            "Iteration 291800, Loss: 0.45455196830512945\n",
            "Iteration 291900, Loss: 0.4545473362130659\n",
            "Iteration 292000, Loss: 0.4545427084307198\n",
            "Iteration 292100, Loss: 0.45453808495324943\n",
            "Iteration 292200, Loss: 0.45453346577582004\n",
            "Iteration 292300, Loss: 0.4545288508936029\n",
            "Iteration 292400, Loss: 0.4545242403017762\n",
            "Iteration 292500, Loss: 0.4545196339955251\n",
            "Iteration 292600, Loss: 0.454515031970041\n",
            "Iteration 292700, Loss: 0.45451043422052184\n",
            "Iteration 292800, Loss: 0.45450584074217243\n",
            "Iteration 292900, Loss: 0.4545012515302042\n",
            "Iteration 293000, Loss: 0.4544966665798348\n",
            "Iteration 293100, Loss: 0.45449208588628864\n",
            "Iteration 293200, Loss: 0.4544875094447969\n",
            "Iteration 293300, Loss: 0.45448293725059696\n",
            "Iteration 293400, Loss: 0.4544783692989327\n",
            "Iteration 293500, Loss: 0.454473805585055\n",
            "Iteration 293600, Loss: 0.4544692461042208\n",
            "Iteration 293700, Loss: 0.45446469085169366\n",
            "Iteration 293800, Loss: 0.4544601398227437\n",
            "Iteration 293900, Loss: 0.4544555930126474\n",
            "Iteration 294000, Loss: 0.4544510504166878\n",
            "Iteration 294100, Loss: 0.4544465120301543\n",
            "Iteration 294200, Loss: 0.45444197784834295\n",
            "Iteration 294300, Loss: 0.45443744786655604\n",
            "Iteration 294400, Loss: 0.45443292208010233\n",
            "Iteration 294500, Loss: 0.45442840048429683\n",
            "Iteration 294600, Loss: 0.45442388307446147\n",
            "Iteration 294700, Loss: 0.4544193698459238\n",
            "Iteration 294800, Loss: 0.45441486079401844\n",
            "Iteration 294900, Loss: 0.45441035591408624\n",
            "Iteration 295000, Loss: 0.454405855201474\n",
            "Iteration 295100, Loss: 0.4544013586515352\n",
            "Iteration 295200, Loss: 0.4543968662596298\n",
            "Iteration 295300, Loss: 0.45439237802112364\n",
            "Iteration 295400, Loss: 0.45438789393138923\n",
            "Iteration 295500, Loss: 0.4543834139858056\n",
            "Iteration 295600, Loss: 0.45437893817975744\n",
            "Iteration 295700, Loss: 0.454374466508636\n",
            "Iteration 295800, Loss: 0.4543699989678389\n",
            "Iteration 295900, Loss: 0.4543655355527702\n",
            "Iteration 296000, Loss: 0.4543610762588399\n",
            "Iteration 296100, Loss: 0.45435662108146424\n",
            "Iteration 296200, Loss: 0.45435217001606587\n",
            "Iteration 296300, Loss: 0.4543477230580737\n",
            "Iteration 296400, Loss: 0.45434328020292264\n",
            "Iteration 296500, Loss: 0.4543388414460539\n",
            "Iteration 296600, Loss: 0.454334406782915\n",
            "Iteration 296700, Loss: 0.45432997620895954\n",
            "Iteration 296800, Loss: 0.4543255497196471\n",
            "Iteration 296900, Loss: 0.45432112731044383\n",
            "Iteration 297000, Loss: 0.45431670897682164\n",
            "Iteration 297100, Loss: 0.45431229471425905\n",
            "Iteration 297200, Loss: 0.45430788451824\n",
            "Iteration 297300, Loss: 0.45430347838425544\n",
            "Iteration 297400, Loss: 0.4542990763078016\n",
            "Iteration 297500, Loss: 0.45429467828438136\n",
            "Iteration 297600, Loss: 0.45429028430950336\n",
            "Iteration 297700, Loss: 0.4542858943786827\n",
            "Iteration 297800, Loss: 0.4542815084874402\n",
            "Iteration 297900, Loss: 0.45427712663130304\n",
            "Iteration 298000, Loss: 0.45427274880580404\n",
            "Iteration 298100, Loss: 0.4542683750064825\n",
            "Iteration 298200, Loss: 0.45426400522888316\n",
            "Iteration 298300, Loss: 0.45425963946855774\n",
            "Iteration 298400, Loss: 0.4542552777210631\n",
            "Iteration 298500, Loss: 0.4542509199819623\n",
            "Iteration 298600, Loss: 0.4542465662468249\n",
            "Iteration 298700, Loss: 0.4542422165112255\n",
            "Iteration 298800, Loss: 0.45423787077074584\n",
            "Iteration 298900, Loss: 0.4542335290209727\n",
            "Iteration 299000, Loss: 0.45422919125749917\n",
            "Iteration 299100, Loss: 0.454224857475924\n",
            "Iteration 299200, Loss: 0.4542205276718525\n",
            "Iteration 299300, Loss: 0.4542162018408951\n",
            "Iteration 299400, Loss: 0.4542118799786689\n",
            "Iteration 299500, Loss: 0.4542075620807964\n",
            "Iteration 299600, Loss: 0.45420324814290614\n",
            "Iteration 299700, Loss: 0.4541989381606326\n",
            "Iteration 299800, Loss: 0.45419463212961625\n",
            "Iteration 299900, Loss: 0.45419033004550313\n",
            "Iteration 300000, Loss: 0.45418603190394524\n",
            "Iteration 300100, Loss: 0.45418173770060055\n",
            "Iteration 300200, Loss: 0.45417744743113286\n",
            "Iteration 300300, Loss: 0.45417316109121153\n",
            "Iteration 300400, Loss: 0.4541688786765121\n",
            "Iteration 300500, Loss: 0.45416460018271565\n",
            "Iteration 300600, Loss: 0.45416032560550923\n",
            "Iteration 300700, Loss: 0.4541560549405856\n",
            "Iteration 300800, Loss: 0.4541517881836432\n",
            "Iteration 300900, Loss: 0.4541475253303867\n",
            "Iteration 301000, Loss: 0.4541432663765259\n",
            "Iteration 301100, Loss: 0.45413901131777656\n",
            "Iteration 301200, Loss: 0.45413476014986076\n",
            "Iteration 301300, Loss: 0.45413051286850525\n",
            "Iteration 301400, Loss: 0.45412626946944334\n",
            "Iteration 301500, Loss: 0.45412202994841383\n",
            "Iteration 301600, Loss: 0.45411779430116095\n",
            "Iteration 301700, Loss: 0.4541135625234352\n",
            "Iteration 301800, Loss: 0.45410933461099234\n",
            "Iteration 301900, Loss: 0.4541051105595937\n",
            "Iteration 302000, Loss: 0.45410089036500656\n",
            "Iteration 302100, Loss: 0.45409667402300385\n",
            "Iteration 302200, Loss: 0.45409246152936417\n",
            "Iteration 302300, Loss: 0.45408825287987153\n",
            "Iteration 302400, Loss: 0.4540840480703159\n",
            "Iteration 302500, Loss: 0.4540798470964923\n",
            "Iteration 302600, Loss: 0.45407564995420213\n",
            "Iteration 302700, Loss: 0.45407145663925186\n",
            "Iteration 302800, Loss: 0.4540672671474538\n",
            "Iteration 302900, Loss: 0.4540630814746256\n",
            "Iteration 303000, Loss: 0.45405889961659085\n",
            "Iteration 303100, Loss: 0.4540547215691785\n",
            "Iteration 303200, Loss: 0.4540505473282228\n",
            "Iteration 303300, Loss: 0.4540463768895642\n",
            "Iteration 303400, Loss: 0.454042210249048\n",
            "Iteration 303500, Loss: 0.45403804740252546\n",
            "Iteration 303600, Loss: 0.45403388834585323\n",
            "Iteration 303700, Loss: 0.4540297330748937\n",
            "Iteration 303800, Loss: 0.4540255815855142\n",
            "Iteration 303900, Loss: 0.4540214338735883\n",
            "Iteration 304000, Loss: 0.4540172899349944\n",
            "Iteration 304100, Loss: 0.4540131497656167\n",
            "Iteration 304200, Loss: 0.45400901336134486\n",
            "Iteration 304300, Loss: 0.45400488071807416\n",
            "Iteration 304400, Loss: 0.4540007518317049\n",
            "Iteration 304500, Loss: 0.45399662669814317\n",
            "Iteration 304600, Loss: 0.45399250531330027\n",
            "Iteration 304700, Loss: 0.4539883876730933\n",
            "Iteration 304800, Loss: 0.4539842737734441\n",
            "Iteration 304900, Loss: 0.4539801636102809\n",
            "Iteration 305000, Loss: 0.45397605717953643\n",
            "Iteration 305100, Loss: 0.4539719544771494\n",
            "Iteration 305200, Loss: 0.45396785549906327\n",
            "Iteration 305300, Loss: 0.45396376024122764\n",
            "Iteration 305400, Loss: 0.4539596686995971\n",
            "Iteration 305500, Loss: 0.4539555808701315\n",
            "Iteration 305600, Loss: 0.4539514967487961\n",
            "Iteration 305700, Loss: 0.4539474163315615\n",
            "Iteration 305800, Loss: 0.45394333961440375\n",
            "Iteration 305900, Loss: 0.45393926659330425\n",
            "Iteration 306000, Loss: 0.45393519726424947\n",
            "Iteration 306100, Loss: 0.4539311316232315\n",
            "Iteration 306200, Loss: 0.45392706966624746\n",
            "Iteration 306300, Loss: 0.4539230113892998\n",
            "Iteration 306400, Loss: 0.45391895678839644\n",
            "Iteration 306500, Loss: 0.4539149058595502\n",
            "Iteration 306600, Loss: 0.4539108585987798\n",
            "Iteration 306700, Loss: 0.4539068150021085\n",
            "Iteration 306800, Loss: 0.45390277506556526\n",
            "Iteration 306900, Loss: 0.4538987387851842\n",
            "Iteration 307000, Loss: 0.45389470615700445\n",
            "Iteration 307100, Loss: 0.4538906771770707\n",
            "Iteration 307200, Loss: 0.45388665184143256\n",
            "Iteration 307300, Loss: 0.4538826301461451\n",
            "Iteration 307400, Loss: 0.45387861208726826\n",
            "Iteration 307500, Loss: 0.4538745976608676\n",
            "Iteration 307600, Loss: 0.4538705868630135\n",
            "Iteration 307700, Loss: 0.45386657968978156\n",
            "Iteration 307800, Loss: 0.45386257613725284\n",
            "Iteration 307900, Loss: 0.45385857620151315\n",
            "Iteration 308000, Loss: 0.45385457987865374\n",
            "Iteration 308100, Loss: 0.45385058716477095\n",
            "Iteration 308200, Loss: 0.45384659805596606\n",
            "Iteration 308300, Loss: 0.4538426125483457\n",
            "Iteration 308400, Loss: 0.4538386306380213\n",
            "Iteration 308500, Loss: 0.45383465232111003\n",
            "Iteration 308600, Loss: 0.45383067759373347\n",
            "Iteration 308700, Loss: 0.4538267064520186\n",
            "Iteration 308800, Loss: 0.4538227388920977\n",
            "Iteration 308900, Loss: 0.45381877491010747\n",
            "Iteration 309000, Loss: 0.4538148145021902\n",
            "Iteration 309100, Loss: 0.4538108576644934\n",
            "Iteration 309200, Loss: 0.4538069043931692\n",
            "Iteration 309300, Loss: 0.4538029546843747\n",
            "Iteration 309400, Loss: 0.4537990085342726\n",
            "Iteration 309500, Loss: 0.45379506593903013\n",
            "Iteration 309600, Loss: 0.4537911268948196\n",
            "Iteration 309700, Loss: 0.4537871913978185\n",
            "Iteration 309800, Loss: 0.4537832594442094\n",
            "Iteration 309900, Loss: 0.4537793310301795\n",
            "Iteration 310000, Loss: 0.45377540615192147\n",
            "Iteration 310100, Loss: 0.45377148480563234\n",
            "Iteration 310200, Loss: 0.4537675669875147\n",
            "Iteration 310300, Loss: 0.4537636526937755\n",
            "Iteration 310400, Loss: 0.4537597419206276\n",
            "Iteration 310500, Loss: 0.45375583466428787\n",
            "Iteration 310600, Loss: 0.4537519309209783\n",
            "Iteration 310700, Loss: 0.4537480306869263\n",
            "Iteration 310800, Loss: 0.4537441339583637\n",
            "Iteration 310900, Loss: 0.4537402407315275\n",
            "Iteration 311000, Loss: 0.45373635100265947\n",
            "Iteration 311100, Loss: 0.4537324647680063\n",
            "Iteration 311200, Loss: 0.4537285820238198\n",
            "Iteration 311300, Loss: 0.4537247027663564\n",
            "Iteration 311400, Loss: 0.4537208269918773\n",
            "Iteration 311500, Loss: 0.45371695469664897\n",
            "Iteration 311600, Loss: 0.4537130858769427\n",
            "Iteration 311700, Loss: 0.4537092205290339\n",
            "Iteration 311800, Loss: 0.4537053586492039\n",
            "Iteration 311900, Loss: 0.4537015002337381\n",
            "Iteration 312000, Loss: 0.45369764527892714\n",
            "Iteration 312100, Loss: 0.45369379378106633\n",
            "Iteration 312200, Loss: 0.45368994573645555\n",
            "Iteration 312300, Loss: 0.45368610114140007\n",
            "Iteration 312400, Loss: 0.45368225999220946\n",
            "Iteration 312500, Loss: 0.45367842228519834\n",
            "Iteration 312600, Loss: 0.45367458801668586\n",
            "Iteration 312700, Loss: 0.4536707571829963\n",
            "Iteration 312800, Loss: 0.4536669297804585\n",
            "Iteration 312900, Loss: 0.45366310580540603\n",
            "Iteration 313000, Loss: 0.4536592852541773\n",
            "Iteration 313100, Loss: 0.4536554681231154\n",
            "Iteration 313200, Loss: 0.4536516544085684\n",
            "Iteration 313300, Loss: 0.45364784410688874\n",
            "Iteration 313400, Loss: 0.45364403721443375\n",
            "Iteration 313500, Loss: 0.45364023372756573\n",
            "Iteration 313600, Loss: 0.4536364336426509\n",
            "Iteration 313700, Loss: 0.4536326369560613\n",
            "Iteration 313800, Loss: 0.453628843664173\n",
            "Iteration 313900, Loss: 0.45362505376336665\n",
            "Iteration 314000, Loss: 0.4536212672500279\n",
            "Iteration 314100, Loss: 0.4536174841205471\n",
            "Iteration 314200, Loss: 0.4536137043713188\n",
            "Iteration 314300, Loss: 0.45360992799874267\n",
            "Iteration 314400, Loss: 0.4536061549992232\n",
            "Iteration 314500, Loss: 0.45360238536916886\n",
            "Iteration 314600, Loss: 0.4535986191049934\n",
            "Iteration 314700, Loss: 0.45359485620311485\n",
            "Iteration 314800, Loss: 0.4535910966599559\n",
            "Iteration 314900, Loss: 0.4535873404719437\n",
            "Iteration 315000, Loss: 0.45358358763551065\n",
            "Iteration 315100, Loss: 0.453579838147093\n",
            "Iteration 315200, Loss: 0.45357609200313204\n",
            "Iteration 315300, Loss: 0.45357234920007355\n",
            "Iteration 315400, Loss: 0.45356860973436763\n",
            "Iteration 315500, Loss: 0.4535648736024695\n",
            "Iteration 315600, Loss: 0.4535611408008384\n",
            "Iteration 315700, Loss: 0.4535574113259384\n",
            "Iteration 315800, Loss: 0.4535536851742381\n",
            "Iteration 315900, Loss: 0.4535499623422107\n",
            "Iteration 316000, Loss: 0.4535462428263339\n",
            "Iteration 316100, Loss: 0.4535425266230895\n",
            "Iteration 316200, Loss: 0.45353881372896476\n",
            "Iteration 316300, Loss: 0.4535351041404505\n",
            "Iteration 316400, Loss: 0.4535313978540426\n",
            "Iteration 316500, Loss: 0.4535276948662415\n",
            "Iteration 316600, Loss: 0.4535239951735518\n",
            "Iteration 316700, Loss: 0.4535202987724827\n",
            "Iteration 316800, Loss: 0.4535166056595477\n",
            "Iteration 316900, Loss: 0.4535129158312654\n",
            "Iteration 317000, Loss: 0.45350922928415827\n",
            "Iteration 317100, Loss: 0.45350554601475346\n",
            "Iteration 317200, Loss: 0.4535018660195824\n",
            "Iteration 317300, Loss: 0.45349818929518143\n",
            "Iteration 317400, Loss: 0.45349451583809075\n",
            "Iteration 317500, Loss: 0.45349084564485537\n",
            "Iteration 317600, Loss: 0.4534871787120244\n",
            "Iteration 317700, Loss: 0.45348351503615203\n",
            "Iteration 317800, Loss: 0.45347985461379575\n",
            "Iteration 317900, Loss: 0.45347619744151885\n",
            "Iteration 318000, Loss: 0.4534725435158878\n",
            "Iteration 318100, Loss: 0.453468892833474\n",
            "Iteration 318200, Loss: 0.4534652453908534\n",
            "Iteration 318300, Loss: 0.4534616011846061\n",
            "Iteration 318400, Loss: 0.4534579602113164\n",
            "Iteration 318500, Loss: 0.4534543224675733\n",
            "Iteration 318600, Loss: 0.4534506879499697\n",
            "Iteration 318700, Loss: 0.4534470566551038\n",
            "Iteration 318800, Loss: 0.45344342857957703\n",
            "Iteration 318900, Loss: 0.4534398037199958\n",
            "Iteration 319000, Loss: 0.45343618207297065\n",
            "Iteration 319100, Loss: 0.45343256363511647\n",
            "Iteration 319200, Loss: 0.45342894840305253\n",
            "Iteration 319300, Loss: 0.45342533637340243\n",
            "Iteration 319400, Loss: 0.45342172754279364\n",
            "Iteration 319500, Loss: 0.45341812190785896\n",
            "Iteration 319600, Loss: 0.4534145194652342\n",
            "Iteration 319700, Loss: 0.45341092021156054\n",
            "Iteration 319800, Loss: 0.4534073241434827\n",
            "Iteration 319900, Loss: 0.45340373125765016\n",
            "Iteration 320000, Loss: 0.4534001415507161\n",
            "Iteration 320100, Loss: 0.45339655501933873\n",
            "Iteration 320200, Loss: 0.45339297166017983\n",
            "Iteration 320300, Loss: 0.4533893914699061\n",
            "Iteration 320400, Loss: 0.45338581444518733\n",
            "Iteration 320500, Loss: 0.4533822405826989\n",
            "Iteration 320600, Loss: 0.4533786698791197\n",
            "Iteration 320700, Loss: 0.4533751023311327\n",
            "Iteration 320800, Loss: 0.45337153793542556\n",
            "Iteration 320900, Loss: 0.45336797668869\n",
            "Iteration 321000, Loss: 0.45336441858762166\n",
            "Iteration 321100, Loss: 0.4533608636289206\n",
            "Iteration 321200, Loss: 0.45335731180929106\n",
            "Iteration 321300, Loss: 0.4533537631254414\n",
            "Iteration 321400, Loss: 0.45335021757408445\n",
            "Iteration 321500, Loss: 0.4533466751519365\n",
            "Iteration 321600, Loss: 0.4533431358557188\n",
            "Iteration 321700, Loss: 0.4533395996821563\n",
            "Iteration 321800, Loss: 0.45333606662797826\n",
            "Iteration 321900, Loss: 0.4533325366899179\n",
            "Iteration 322000, Loss: 0.4533290098647129\n",
            "Iteration 322100, Loss: 0.45332548614910473\n",
            "Iteration 322200, Loss: 0.4533219655398393\n",
            "Iteration 322300, Loss: 0.45331844803366617\n",
            "Iteration 322400, Loss: 0.45331493362733977\n",
            "Iteration 322500, Loss: 0.45331142231761795\n",
            "Iteration 322600, Loss: 0.45330791410126264\n",
            "Iteration 322700, Loss: 0.4533044089750403\n",
            "Iteration 322800, Loss: 0.4533009069357215\n",
            "Iteration 322900, Loss: 0.45329740798008056\n",
            "Iteration 323000, Loss: 0.45329391210489584\n",
            "Iteration 323100, Loss: 0.45329041930695\n",
            "Iteration 323200, Loss: 0.4532869295830298\n",
            "Iteration 323300, Loss: 0.4532834429299259\n",
            "Iteration 323400, Loss: 0.4532799593444331\n",
            "Iteration 323500, Loss: 0.45327647882334987\n",
            "Iteration 323600, Loss: 0.45327300136347937\n",
            "Iteration 323700, Loss: 0.4532695269616282\n",
            "Iteration 323800, Loss: 0.4532660556146076\n",
            "Iteration 323900, Loss: 0.4532625873192322\n",
            "Iteration 324000, Loss: 0.45325912207232105\n",
            "Iteration 324100, Loss: 0.4532556598706968\n",
            "Iteration 324200, Loss: 0.45325220071118677\n",
            "Iteration 324300, Loss: 0.45324874459062175\n",
            "Iteration 324400, Loss: 0.4532452915058365\n",
            "Iteration 324500, Loss: 0.4532418414536699\n",
            "Iteration 324600, Loss: 0.45323839443096503\n",
            "Iteration 324700, Loss: 0.4532349504345685\n",
            "Iteration 324800, Loss: 0.45323150946133134\n",
            "Iteration 324900, Loss: 0.4532280715081079\n",
            "Iteration 325000, Loss: 0.4532246365717576\n",
            "Iteration 325100, Loss: 0.4532212046491424\n",
            "Iteration 325200, Loss: 0.4532177757371291\n",
            "Iteration 325300, Loss: 0.45321434983258835\n",
            "Iteration 325400, Loss: 0.4532109269323947\n",
            "Iteration 325500, Loss: 0.4532075070334262\n",
            "Iteration 325600, Loss: 0.45320409013256535\n",
            "Iteration 325700, Loss: 0.4532006762266983\n",
            "Iteration 325800, Loss: 0.4531972653127152\n",
            "Iteration 325900, Loss: 0.45319385738751\n",
            "Iteration 326000, Loss: 0.4531904524479806\n",
            "Iteration 326100, Loss: 0.4531870504910289\n",
            "Iteration 326200, Loss: 0.45318365151356055\n",
            "Iteration 326300, Loss: 0.45318025551248503\n",
            "Iteration 326400, Loss: 0.45317686248471567\n",
            "Iteration 326500, Loss: 0.4531734724271699\n",
            "Iteration 326600, Loss: 0.4531700853367688\n",
            "Iteration 326700, Loss: 0.45316670121043723\n",
            "Iteration 326800, Loss: 0.4531633200451044\n",
            "Iteration 326900, Loss: 0.4531599418377025\n",
            "Iteration 327000, Loss: 0.4531565665851683\n",
            "Iteration 327100, Loss: 0.45315319428444223\n",
            "Iteration 327200, Loss: 0.4531498249324683\n",
            "Iteration 327300, Loss: 0.4531464585261943\n",
            "Iteration 327400, Loss: 0.4531430950625723\n",
            "Iteration 327500, Loss: 0.45313973453855794\n",
            "Iteration 327600, Loss: 0.4531363769511103\n",
            "Iteration 327700, Loss: 0.453133022297193\n",
            "Iteration 327800, Loss: 0.45312967057377257\n",
            "Iteration 327900, Loss: 0.45312632177781986\n",
            "Iteration 328000, Loss: 0.4531229759063097\n",
            "Iteration 328100, Loss: 0.45311963295622015\n",
            "Iteration 328200, Loss: 0.45311629292453315\n",
            "Iteration 328300, Loss: 0.453112955808235\n",
            "Iteration 328400, Loss: 0.4531096216043148\n",
            "Iteration 328500, Loss: 0.4531062903097661\n",
            "Iteration 328600, Loss: 0.45310296192158606\n",
            "Iteration 328700, Loss: 0.4530996364367752\n",
            "Iteration 328800, Loss: 0.4530963138523385\n",
            "Iteration 328900, Loss: 0.4530929941652839\n",
            "Iteration 329000, Loss: 0.4530896773726234\n",
            "Iteration 329100, Loss: 0.4530863634713729\n",
            "Iteration 329200, Loss: 0.4530830524585516\n",
            "Iteration 329300, Loss: 0.4530797443311827\n",
            "Iteration 329400, Loss: 0.4530764390862933\n",
            "Iteration 329500, Loss: 0.45307313672091354\n",
            "Iteration 329600, Loss: 0.4530698372320777\n",
            "Iteration 329700, Loss: 0.4530665406168238\n",
            "Iteration 329800, Loss: 0.45306324687219335\n",
            "Iteration 329900, Loss: 0.45305995599523147\n",
            "Iteration 330000, Loss: 0.4530566679829871\n",
            "Iteration 330100, Loss: 0.4530533828325129\n",
            "Iteration 330200, Loss: 0.4530501005408649\n",
            "Iteration 330300, Loss: 0.4530468211051031\n",
            "Iteration 330400, Loss: 0.4530435445222911\n",
            "Iteration 330500, Loss: 0.45304027078949577\n",
            "Iteration 330600, Loss: 0.45303699990378804\n",
            "Iteration 330700, Loss: 0.4530337318622424\n",
            "Iteration 330800, Loss: 0.4530304666619367\n",
            "Iteration 330900, Loss: 0.4530272042999528\n",
            "Iteration 331000, Loss: 0.4530239447733757\n",
            "Iteration 331100, Loss: 0.45302068807929424\n",
            "Iteration 331200, Loss: 0.4530174342148011\n",
            "Iteration 331300, Loss: 0.45301418317699227\n",
            "Iteration 331400, Loss: 0.4530109349629674\n",
            "Iteration 331500, Loss: 0.45300768956982956\n",
            "Iteration 331600, Loss: 0.4530044469946858\n",
            "Iteration 331700, Loss: 0.45300120723464626\n",
            "Iteration 331800, Loss: 0.452997970286825\n",
            "Iteration 331900, Loss: 0.4529947361483396\n",
            "Iteration 332000, Loss: 0.45299150481631084\n",
            "Iteration 332100, Loss: 0.45298827628786387\n",
            "Iteration 332200, Loss: 0.4529850505601264\n",
            "Iteration 332300, Loss: 0.4529818276302301\n",
            "Iteration 332400, Loss: 0.4529786074953106\n",
            "Iteration 332500, Loss: 0.4529753901525063\n",
            "Iteration 332600, Loss: 0.45297217559895986\n",
            "Iteration 332700, Loss: 0.45296896383181695\n",
            "Iteration 332800, Loss: 0.45296575484822665\n",
            "Iteration 332900, Loss: 0.4529625486453423\n",
            "Iteration 333000, Loss: 0.4529593452203198\n",
            "Iteration 333100, Loss: 0.4529561445703193\n",
            "Iteration 333200, Loss: 0.45295294669250413\n",
            "Iteration 333300, Loss: 0.4529497515840409\n",
            "Iteration 333400, Loss: 0.45294655924210014\n",
            "Iteration 333500, Loss: 0.4529433696638556\n",
            "Iteration 333600, Loss: 0.45294018284648463\n",
            "Iteration 333700, Loss: 0.452936998787168\n",
            "Iteration 333800, Loss: 0.45293381748308975\n",
            "Iteration 333900, Loss: 0.4529306389314375\n",
            "Iteration 334000, Loss: 0.4529274631294027\n",
            "Iteration 334100, Loss: 0.4529242900741796\n",
            "Iteration 334200, Loss: 0.45292111976296656\n",
            "Iteration 334300, Loss: 0.45291795219296466\n",
            "Iteration 334400, Loss: 0.45291478736137897\n",
            "Iteration 334500, Loss: 0.4529116252654179\n",
            "Iteration 334600, Loss: 0.45290846590229306\n",
            "Iteration 334700, Loss: 0.4529053092692197\n",
            "Iteration 334800, Loss: 0.4529021553634161\n",
            "Iteration 334900, Loss: 0.45289900418210466\n",
            "Iteration 335000, Loss: 0.4528958557225107\n",
            "Iteration 335100, Loss: 0.4528927099818627\n",
            "Iteration 335200, Loss: 0.452889566957393\n",
            "Iteration 335300, Loss: 0.4528864266463372\n",
            "Iteration 335400, Loss: 0.45288328904593417\n",
            "Iteration 335500, Loss: 0.4528801541534264\n",
            "Iteration 335600, Loss: 0.45287702196605956\n",
            "Iteration 335700, Loss: 0.45287389248108245\n",
            "Iteration 335800, Loss: 0.4528707656957476\n",
            "Iteration 335900, Loss: 0.45286764160731113\n",
            "Iteration 336000, Loss: 0.4528645202130319\n",
            "Iteration 336100, Loss: 0.4528614015101724\n",
            "Iteration 336200, Loss: 0.4528582854959984\n",
            "Iteration 336300, Loss: 0.45285517216777926\n",
            "Iteration 336400, Loss: 0.45285206152278734\n",
            "Iteration 336500, Loss: 0.45284895355829846\n",
            "Iteration 336600, Loss: 0.45284584827159197\n",
            "Iteration 336700, Loss: 0.4528427456599502\n",
            "Iteration 336800, Loss: 0.452839645720659\n",
            "Iteration 336900, Loss: 0.4528365484510074\n",
            "Iteration 337000, Loss: 0.4528334538482878\n",
            "Iteration 337100, Loss: 0.452830361909796\n",
            "Iteration 337200, Loss: 0.45282727263283074\n",
            "Iteration 337300, Loss: 0.4528241860146947\n",
            "Iteration 337400, Loss: 0.4528211020526932\n",
            "Iteration 337500, Loss: 0.4528180207441353\n",
            "Iteration 337600, Loss: 0.45281494208633294\n",
            "Iteration 337700, Loss: 0.45281186607660157\n",
            "Iteration 337800, Loss: 0.4528087927122599\n",
            "Iteration 337900, Loss: 0.45280572199062963\n",
            "Iteration 338000, Loss: 0.4528026539090364\n",
            "Iteration 338100, Loss: 0.45279958846480817\n",
            "Iteration 338200, Loss: 0.45279652565527706\n",
            "Iteration 338300, Loss: 0.45279346547777755\n",
            "Iteration 338400, Loss: 0.45279040792964803\n",
            "Iteration 338500, Loss: 0.45278735300823\n",
            "Iteration 338600, Loss: 0.45278430071086806\n",
            "Iteration 338700, Loss: 0.4527812510349099\n",
            "Iteration 338800, Loss: 0.4527782039777066\n",
            "Iteration 338900, Loss: 0.4527751595366127\n",
            "Iteration 339000, Loss: 0.4527721177089853\n",
            "Iteration 339100, Loss: 0.4527690784921854\n",
            "Iteration 339200, Loss: 0.4527660418835767\n",
            "Iteration 339300, Loss: 0.4527630078805264\n",
            "Iteration 339400, Loss: 0.45275997648040484\n",
            "Iteration 339500, Loss: 0.45275694768058533\n",
            "Iteration 339600, Loss: 0.45275392147844473\n",
            "Iteration 339700, Loss: 0.4527508978713624\n",
            "Iteration 339800, Loss: 0.4527478768567219\n",
            "Iteration 339900, Loss: 0.4527448584319091\n",
            "Iteration 340000, Loss: 0.4527418425943133\n",
            "Iteration 340100, Loss: 0.45273882934132725\n",
            "Iteration 340200, Loss: 0.4527358186703463\n",
            "Iteration 340300, Loss: 0.45273281057876935\n",
            "Iteration 340400, Loss: 0.4527298050639981\n",
            "Iteration 340500, Loss: 0.4527268021234383\n",
            "Iteration 340600, Loss: 0.4527238017544972\n",
            "Iteration 340700, Loss: 0.45272080395458697\n",
            "Iteration 340800, Loss: 0.45271780872112166\n",
            "Iteration 340900, Loss: 0.45271481605151886\n",
            "Iteration 341000, Loss: 0.4527118259431994\n",
            "Iteration 341100, Loss: 0.4527088383935869\n",
            "Iteration 341200, Loss: 0.4527058534001087\n",
            "Iteration 341300, Loss: 0.45270287096019435\n",
            "Iteration 341400, Loss: 0.4526998910712774\n",
            "Iteration 341500, Loss: 0.4526969137307936\n",
            "Iteration 341600, Loss: 0.4526939389361827\n",
            "Iteration 341700, Loss: 0.4526909666848867\n",
            "Iteration 341800, Loss: 0.4526879969743515\n",
            "Iteration 341900, Loss: 0.4526850298020252\n",
            "Iteration 342000, Loss: 0.45268206516535997\n",
            "Iteration 342100, Loss: 0.45267910306181014\n",
            "Iteration 342200, Loss: 0.4526761434888334\n",
            "Iteration 342300, Loss: 0.4526731864438908\n",
            "Iteration 342400, Loss: 0.45267023192444616\n",
            "Iteration 342500, Loss: 0.4526672799279663\n",
            "Iteration 342600, Loss: 0.4526643304519211\n",
            "Iteration 342700, Loss: 0.4526613834937838\n",
            "Iteration 342800, Loss: 0.4526584390510305\n",
            "Iteration 342900, Loss: 0.45265549712114006\n",
            "Iteration 343000, Loss: 0.45265255770159474\n",
            "Iteration 343100, Loss: 0.4526496207898795\n",
            "Iteration 343200, Loss: 0.4526466863834827\n",
            "Iteration 343300, Loss: 0.4526437544798954\n",
            "Iteration 343400, Loss: 0.45264082507661174\n",
            "Iteration 343500, Loss: 0.452637898171129\n",
            "Iteration 343600, Loss: 0.4526349737609474\n",
            "Iteration 343700, Loss: 0.4526320518435699\n",
            "Iteration 343800, Loss: 0.4526291324165028\n",
            "Iteration 343900, Loss: 0.45262621547725534\n",
            "Iteration 344000, Loss: 0.4526233010233397\n",
            "Iteration 344100, Loss: 0.4526203890522708\n",
            "Iteration 344200, Loss: 0.4526174795615669\n",
            "Iteration 344300, Loss: 0.4526145725487492\n",
            "Iteration 344400, Loss: 0.45261166801134145\n",
            "Iteration 344500, Loss: 0.4526087659468709\n",
            "Iteration 344600, Loss: 0.4526058663528675\n",
            "Iteration 344700, Loss: 0.45260296922686405\n",
            "Iteration 344800, Loss: 0.4526000745663965\n",
            "Iteration 344900, Loss: 0.4525971823690036\n",
            "Iteration 345000, Loss: 0.4525942926322275\n",
            "Iteration 345100, Loss: 0.45259140535361236\n",
            "Iteration 345200, Loss: 0.45258852053070603\n",
            "Iteration 345300, Loss: 0.45258563816105923\n",
            "Iteration 345400, Loss: 0.45258275824222527\n",
            "Iteration 345500, Loss: 0.4525798807717607\n",
            "Iteration 345600, Loss: 0.4525770057472247\n",
            "Iteration 345700, Loss: 0.4525741331661795\n",
            "Iteration 345800, Loss: 0.45257126302619055\n",
            "Iteration 345900, Loss: 0.45256839532482557\n",
            "Iteration 346000, Loss: 0.45256553005965583\n",
            "Iteration 346100, Loss: 0.4525626672282549\n",
            "Iteration 346200, Loss: 0.4525598068281996\n",
            "Iteration 346300, Loss: 0.45255694885706976\n",
            "Iteration 346400, Loss: 0.45255409331244767\n",
            "Iteration 346500, Loss: 0.4525512401919187\n",
            "Iteration 346600, Loss: 0.4525483894930714\n",
            "Iteration 346700, Loss: 0.4525455412134968\n",
            "Iteration 346800, Loss: 0.4525426953507887\n",
            "Iteration 346900, Loss: 0.45253985190254425\n",
            "Iteration 347000, Loss: 0.45253701086636317\n",
            "Iteration 347100, Loss: 0.4525341722398478\n",
            "Iteration 347200, Loss: 0.4525313360206038\n",
            "Iteration 347300, Loss: 0.4525285022062395\n",
            "Iteration 347400, Loss: 0.4525256707943659\n",
            "Iteration 347500, Loss: 0.45252284178259705\n",
            "Iteration 347600, Loss: 0.4525200151685496\n",
            "Iteration 347700, Loss: 0.45251719094984355\n",
            "Iteration 347800, Loss: 0.45251436912410087\n",
            "Iteration 347900, Loss: 0.45251154968894736\n",
            "Iteration 348000, Loss: 0.45250873264201064\n",
            "Iteration 348100, Loss: 0.4525059179809221\n",
            "Iteration 348200, Loss: 0.4525031057033152\n",
            "Iteration 348300, Loss: 0.4525002958068264\n",
            "Iteration 348400, Loss: 0.4524974882890953\n",
            "Iteration 348500, Loss: 0.45249468314776387\n",
            "Iteration 348600, Loss: 0.4524918803804771\n",
            "Iteration 348700, Loss: 0.45248907998488264\n",
            "Iteration 348800, Loss: 0.4524862819586313\n",
            "Iteration 348900, Loss: 0.45248348629937585\n",
            "Iteration 349000, Loss: 0.45248069300477267\n",
            "Iteration 349100, Loss: 0.4524779020724807\n",
            "Iteration 349200, Loss: 0.45247511350016156\n",
            "Iteration 349300, Loss: 0.4524723272854795\n",
            "Iteration 349400, Loss: 0.4524695434261015\n",
            "Iteration 349500, Loss: 0.4524667619196981\n",
            "Iteration 349600, Loss: 0.4524639827639412\n",
            "Iteration 349700, Loss: 0.45246120595650674\n",
            "Iteration 349800, Loss: 0.4524584314950727\n",
            "Iteration 349900, Loss: 0.45245565937732\n",
            "Iteration 350000, Loss: 0.4524528896009324\n",
            "Iteration 350100, Loss: 0.45245012216359615\n",
            "Iteration 350200, Loss: 0.45244735706300027\n",
            "Iteration 350300, Loss: 0.45244459429683687\n",
            "Iteration 350400, Loss: 0.4524418338628003\n",
            "Iteration 350500, Loss: 0.4524390757585881\n",
            "Iteration 350600, Loss: 0.4524363199819\n",
            "Iteration 350700, Loss: 0.45243356653043904\n",
            "Iteration 350800, Loss: 0.4524308154019103\n",
            "Iteration 350900, Loss: 0.4524280665940221\n",
            "Iteration 351000, Loss: 0.4524253201044852\n",
            "Iteration 351100, Loss: 0.45242257593101337\n",
            "Iteration 351200, Loss: 0.4524198340713225\n",
            "Iteration 351300, Loss: 0.4524170945231317\n",
            "Iteration 351400, Loss: 0.4524143572841626\n",
            "Iteration 351500, Loss: 0.4524116223521395\n",
            "Iteration 351600, Loss: 0.4524088897247891\n",
            "Iteration 351700, Loss: 0.4524061593998413\n",
            "Iteration 351800, Loss: 0.4524034313750283\n",
            "Iteration 351900, Loss: 0.4524007056480852\n",
            "Iteration 352000, Loss: 0.4523979822167496\n",
            "Iteration 352100, Loss: 0.45239526107876166\n",
            "Iteration 352200, Loss: 0.45239254223186454\n",
            "Iteration 352300, Loss: 0.4523898256738037\n",
            "Iteration 352400, Loss: 0.4523871114023276\n",
            "Iteration 352500, Loss: 0.4523843994151868\n",
            "Iteration 352600, Loss: 0.45238168971013515\n",
            "Iteration 352700, Loss: 0.4523789822849287\n",
            "Iteration 352800, Loss: 0.4523762771373263\n",
            "Iteration 352900, Loss: 0.4523735742650894\n",
            "Iteration 353000, Loss: 0.45237087366598205\n",
            "Iteration 353100, Loss: 0.45236817533777085\n",
            "Iteration 353200, Loss: 0.4523654792782254\n",
            "Iteration 353300, Loss: 0.45236278548511744\n",
            "Iteration 353400, Loss: 0.45236009395622134\n",
            "Iteration 353500, Loss: 0.45235740468931446\n",
            "Iteration 353600, Loss: 0.4523547176821766\n",
            "Iteration 353700, Loss: 0.4523520329325901\n",
            "Iteration 353800, Loss: 0.4523493504383397\n",
            "Iteration 353900, Loss: 0.45234667019721314\n",
            "Iteration 354000, Loss: 0.45234399220700044\n",
            "Iteration 354100, Loss: 0.4523413164654946\n",
            "Iteration 354200, Loss: 0.4523386429704904\n",
            "Iteration 354300, Loss: 0.45233597171978634\n",
            "Iteration 354400, Loss: 0.45233330271118233\n",
            "Iteration 354500, Loss: 0.4523306359424817\n",
            "Iteration 354600, Loss: 0.45232797141148995\n",
            "Iteration 354700, Loss: 0.4523253091160152\n",
            "Iteration 354800, Loss: 0.4523226490538683\n",
            "Iteration 354900, Loss: 0.45231999122286254\n",
            "Iteration 355000, Loss: 0.4523173356208135\n",
            "Iteration 355100, Loss: 0.4523146822455399\n",
            "Iteration 355200, Loss: 0.4523120310948623\n",
            "Iteration 355300, Loss: 0.4523093821666045\n",
            "Iteration 355400, Loss: 0.45230673545859235\n",
            "Iteration 355500, Loss: 0.4523040909686545\n",
            "Iteration 355600, Loss: 0.45230144869462185\n",
            "Iteration 355700, Loss: 0.45229880863432825\n",
            "Iteration 355800, Loss: 0.45229617078560963\n",
            "Iteration 355900, Loss: 0.45229353514630477\n",
            "Iteration 356000, Loss: 0.45229090171425473\n",
            "Iteration 356100, Loss: 0.45228827048730325\n",
            "Iteration 356200, Loss: 0.4522856414632965\n",
            "Iteration 356300, Loss: 0.4522830146400832\n",
            "Iteration 356400, Loss: 0.4522803900155147\n",
            "Iteration 356500, Loss: 0.45227776758744453\n",
            "Iteration 356600, Loss: 0.4522751473537289\n",
            "Iteration 356700, Loss: 0.4522725293122267\n",
            "Iteration 356800, Loss: 0.4522699134607988\n",
            "Iteration 356900, Loss: 0.4522672997973093\n",
            "Iteration 357000, Loss: 0.4522646883196242\n",
            "Iteration 357100, Loss: 0.45226207902561183\n",
            "Iteration 357200, Loss: 0.4522594719131438\n",
            "Iteration 357300, Loss: 0.45225686698009326\n",
            "Iteration 357400, Loss: 0.45225426422433657\n",
            "Iteration 357500, Loss: 0.4522516636437522\n",
            "Iteration 357600, Loss: 0.4522490652362212\n",
            "Iteration 357700, Loss: 0.45224646899962667\n",
            "Iteration 357800, Loss: 0.4522438749318551\n",
            "Iteration 357900, Loss: 0.45224128303079425\n",
            "Iteration 358000, Loss: 0.4522386932943354\n",
            "Iteration 358100, Loss: 0.4522361057203715\n",
            "Iteration 358200, Loss: 0.4522335203067985\n",
            "Iteration 358300, Loss: 0.45223093705151424\n",
            "Iteration 358400, Loss: 0.4522283559524194\n",
            "Iteration 358500, Loss: 0.45222577700741723\n",
            "Iteration 358600, Loss: 0.4522232002144129\n",
            "Iteration 358700, Loss: 0.45222062557131437\n",
            "Iteration 358800, Loss: 0.4522180530760318\n",
            "Iteration 358900, Loss: 0.452215482726478\n",
            "Iteration 359000, Loss: 0.4522129145205681\n",
            "Iteration 359100, Loss: 0.45221034845621966\n",
            "Iteration 359200, Loss: 0.45220778453135235\n",
            "Iteration 359300, Loss: 0.4522052227438888\n",
            "Iteration 359400, Loss: 0.45220266309175366\n",
            "Iteration 359500, Loss: 0.45220010557287404\n",
            "Iteration 359600, Loss: 0.4521975501851794\n",
            "Iteration 359700, Loss: 0.452194996926602\n",
            "Iteration 359800, Loss: 0.45219244579507567\n",
            "Iteration 359900, Loss: 0.4521898967885375\n",
            "Iteration 360000, Loss: 0.45218734990492654\n",
            "Iteration 360100, Loss: 0.45218480514218395\n",
            "Iteration 360200, Loss: 0.4521822624982539\n",
            "Iteration 360300, Loss: 0.4521797219710825\n",
            "Iteration 360400, Loss: 0.4521771835586183\n",
            "Iteration 360500, Loss: 0.4521746472588123\n",
            "Iteration 360600, Loss: 0.4521721130696178\n",
            "Iteration 360700, Loss: 0.4521695809889904\n",
            "Iteration 360800, Loss: 0.4521670510148881\n",
            "Iteration 360900, Loss: 0.45216452314527145\n",
            "Iteration 361000, Loss: 0.45216199737810303\n",
            "Iteration 361100, Loss: 0.4521594737113481\n",
            "Iteration 361200, Loss: 0.4521569521429738\n",
            "Iteration 361300, Loss: 0.4521544326709501\n",
            "Iteration 361400, Loss: 0.45215191529324894\n",
            "Iteration 361500, Loss: 0.452149400007845\n",
            "Iteration 361600, Loss: 0.4521468868127147\n",
            "Iteration 361700, Loss: 0.4521443757058373\n",
            "Iteration 361800, Loss: 0.45214186668519435\n",
            "Iteration 361900, Loss: 0.4521393597487694\n",
            "Iteration 362000, Loss: 0.45213685489454836\n",
            "Iteration 362100, Loss: 0.4521343521205199\n",
            "Iteration 362200, Loss: 0.45213185142467466\n",
            "Iteration 362300, Loss: 0.4521293528050054\n",
            "Iteration 362400, Loss: 0.45212685625950755\n",
            "Iteration 362500, Loss: 0.4521243617861789\n",
            "Iteration 362600, Loss: 0.45212186938301885\n",
            "Iteration 362700, Loss: 0.45211937904802985\n",
            "Iteration 362800, Loss: 0.45211689077921646\n",
            "Iteration 362900, Loss: 0.4521144045745853\n",
            "Iteration 363000, Loss: 0.45211192043214565\n",
            "Iteration 363100, Loss: 0.4521094383499087\n",
            "Iteration 363200, Loss: 0.45210695832588765\n",
            "Iteration 363300, Loss: 0.452104480358099\n",
            "Iteration 363400, Loss: 0.4521020044445607\n",
            "Iteration 363500, Loss: 0.4520995305832933\n",
            "Iteration 363600, Loss: 0.45209705877231904\n",
            "Iteration 363700, Loss: 0.4520945890096633\n",
            "Iteration 363800, Loss: 0.45209212129335347\n",
            "Iteration 363900, Loss: 0.45208965562141845\n",
            "Iteration 364000, Loss: 0.4520871919918905\n",
            "Iteration 364100, Loss: 0.4520847304028034\n",
            "Iteration 364200, Loss: 0.4520822708521932\n",
            "Iteration 364300, Loss: 0.45207981333809877\n",
            "Iteration 364400, Loss: 0.4520773578585606\n",
            "Iteration 364500, Loss: 0.4520749044116217\n",
            "Iteration 364600, Loss: 0.4520724529953274\n",
            "Iteration 364700, Loss: 0.45207000360772476\n",
            "Iteration 364800, Loss: 0.4520675562468639\n",
            "Iteration 364900, Loss: 0.4520651109107965\n",
            "Iteration 365000, Loss: 0.45206266759757663\n",
            "Iteration 365100, Loss: 0.45206022630526066\n",
            "Iteration 365200, Loss: 0.45205778703190713\n",
            "Iteration 365300, Loss: 0.4520553497755769\n",
            "Iteration 365400, Loss: 0.45205291453433305\n",
            "Iteration 365500, Loss: 0.4520504813062405\n",
            "Iteration 365600, Loss: 0.4520480500893667\n",
            "Iteration 365700, Loss: 0.4520456208817816\n",
            "Iteration 365800, Loss: 0.4520431936815566\n",
            "Iteration 365900, Loss: 0.45204076848676583\n",
            "Iteration 366000, Loss: 0.4520383452954856\n",
            "Iteration 366100, Loss: 0.452035924105794\n",
            "Iteration 366200, Loss: 0.452033504915772\n",
            "Iteration 366300, Loss: 0.4520310877235019\n",
            "Iteration 366400, Loss: 0.45202867252706924\n",
            "Iteration 366500, Loss: 0.45202625932456053\n",
            "Iteration 366600, Loss: 0.4520238481140654\n",
            "Iteration 366700, Loss: 0.45202143889367535\n",
            "Iteration 366800, Loss: 0.4520190316614839\n",
            "Iteration 366900, Loss: 0.4520166264155869\n",
            "Iteration 367000, Loss: 0.45201422315408224\n",
            "Iteration 367100, Loss: 0.45201182187507016\n",
            "Iteration 367200, Loss: 0.4520094225766528\n",
            "Iteration 367300, Loss: 0.4520070252569349\n",
            "Iteration 367400, Loss: 0.4520046299140228\n",
            "Iteration 367500, Loss: 0.45200223654602545\n",
            "Iteration 367600, Loss: 0.4519998451510535\n",
            "Iteration 367700, Loss: 0.45199745572722033\n",
            "Iteration 367800, Loss: 0.45199506827264085\n",
            "Iteration 367900, Loss: 0.45199268278543236\n",
            "Iteration 368000, Loss: 0.45199029926371453\n",
            "Iteration 368100, Loss: 0.45198791770560887\n",
            "Iteration 368200, Loss: 0.45198553810923897\n",
            "Iteration 368300, Loss: 0.451983160472731\n",
            "Iteration 368400, Loss: 0.4519807847942125\n",
            "Iteration 368500, Loss: 0.4519784110718139\n",
            "Iteration 368600, Loss: 0.45197603930366725\n",
            "Iteration 368700, Loss: 0.451973669487907\n",
            "Iteration 368800, Loss: 0.4519713016226694\n",
            "Iteration 368900, Loss: 0.4519689357060932\n",
            "Iteration 369000, Loss: 0.45196657173631893\n",
            "Iteration 369100, Loss: 0.4519642097114893\n",
            "Iteration 369200, Loss: 0.45196184962974945\n",
            "Iteration 369300, Loss: 0.451959491489246\n",
            "Iteration 369400, Loss: 0.4519571352881282\n",
            "Iteration 369500, Loss: 0.4519547810245471\n",
            "Iteration 369600, Loss: 0.451952428696656\n",
            "Iteration 369700, Loss: 0.45195007830261036\n",
            "Iteration 369800, Loss: 0.4519477298405672\n",
            "Iteration 369900, Loss: 0.4519453833086862\n",
            "Iteration 370000, Loss: 0.4519430387051291\n",
            "Iteration 370100, Loss: 0.4519406960280595\n",
            "Iteration 370200, Loss: 0.45193835527564286\n",
            "Iteration 370300, Loss: 0.4519360164460471\n",
            "Iteration 370400, Loss: 0.45193367953744235\n",
            "Iteration 370500, Loss: 0.4519313445480001\n",
            "Iteration 370600, Loss: 0.45192901147589454\n",
            "Iteration 370700, Loss: 0.45192668031930183\n",
            "Iteration 370800, Loss: 0.4519243510763997\n",
            "Iteration 370900, Loss: 0.4519220237453687\n",
            "Iteration 371000, Loss: 0.4519196983243908\n",
            "Iteration 371100, Loss: 0.4519173748116503\n",
            "Iteration 371200, Loss: 0.4519150532053335\n",
            "Iteration 371300, Loss: 0.4519127335036286\n",
            "Iteration 371400, Loss: 0.45191041570472623\n",
            "Iteration 371500, Loss: 0.4519080998068186\n",
            "Iteration 371600, Loss: 0.45190578580810037\n",
            "Iteration 371700, Loss: 0.45190347370676787\n",
            "Iteration 371800, Loss: 0.4519011635010195\n",
            "Iteration 371900, Loss: 0.45189885518905587\n",
            "Iteration 372000, Loss: 0.4518965487690798\n",
            "Iteration 372100, Loss: 0.4518942442392955\n",
            "Iteration 372200, Loss: 0.4518919415979097\n",
            "Iteration 372300, Loss: 0.4518896408431312\n",
            "Iteration 372400, Loss: 0.4518873419731703\n",
            "Iteration 372500, Loss: 0.45188504498623977\n",
            "Iteration 372600, Loss: 0.45188274988055427\n",
            "Iteration 372700, Loss: 0.45188045665433063\n",
            "Iteration 372800, Loss: 0.45187816530578717\n",
            "Iteration 372900, Loss: 0.4518758758331446\n",
            "Iteration 373000, Loss: 0.45187358823462564\n",
            "Iteration 373100, Loss: 0.45187130250845514\n",
            "Iteration 373200, Loss: 0.45186901865285933\n",
            "Iteration 373300, Loss: 0.4518667366660671\n",
            "Iteration 373400, Loss: 0.4518644565463088\n",
            "Iteration 373500, Loss: 0.4518621782918173\n",
            "Iteration 373600, Loss: 0.4518599019008271\n",
            "Iteration 373700, Loss: 0.45185762737157464\n",
            "Iteration 373800, Loss: 0.45185535470229843\n",
            "Iteration 373900, Loss: 0.4518530838912391\n",
            "Iteration 374000, Loss: 0.45185081493663903\n",
            "Iteration 374100, Loss: 0.4518485478367426\n",
            "Iteration 374200, Loss: 0.4518462825897963\n",
            "Iteration 374300, Loss: 0.4518440191940485\n",
            "Iteration 374400, Loss: 0.4518417576477493\n",
            "Iteration 374500, Loss: 0.4518394979491513\n",
            "Iteration 374600, Loss: 0.45183724009650855\n",
            "Iteration 374700, Loss: 0.4518349840880772\n",
            "Iteration 374800, Loss: 0.45183272992211543\n",
            "Iteration 374900, Loss: 0.4518304775968832\n",
            "Iteration 375000, Loss: 0.45182822711064274\n",
            "Iteration 375100, Loss: 0.45182597846165806\n",
            "Iteration 375200, Loss: 0.45182373164819456\n",
            "Iteration 375300, Loss: 0.4518214866685208\n",
            "Iteration 375400, Loss: 0.4518192435209059\n",
            "Iteration 375500, Loss: 0.45181700220362186\n",
            "Iteration 375600, Loss: 0.4518147627149423\n",
            "Iteration 375700, Loss: 0.45181252505314284\n",
            "Iteration 375800, Loss: 0.4518102892165008\n",
            "Iteration 375900, Loss: 0.4518080552032957\n",
            "Iteration 376000, Loss: 0.4518058230118087\n",
            "Iteration 376100, Loss: 0.45180359264032327\n",
            "Iteration 376200, Loss: 0.4518013640871244\n",
            "Iteration 376300, Loss: 0.4517991373504991\n",
            "Iteration 376400, Loss: 0.45179691242873643\n",
            "Iteration 376500, Loss: 0.45179468932012734\n",
            "Iteration 376600, Loss: 0.4517924680229644\n",
            "Iteration 376700, Loss: 0.4517902485355423\n",
            "Iteration 376800, Loss: 0.4517880308561578\n",
            "Iteration 376900, Loss: 0.4517858149831094\n",
            "Iteration 377000, Loss: 0.4517836009146972\n",
            "Iteration 377100, Loss: 0.45178138864922357\n",
            "Iteration 377200, Loss: 0.45177917818499264\n",
            "Iteration 377300, Loss: 0.45177696952031055\n",
            "Iteration 377400, Loss: 0.4517747626534851\n",
            "Iteration 377500, Loss: 0.45177255758282603\n",
            "Iteration 377600, Loss: 0.4517703543066451\n",
            "Iteration 377700, Loss: 0.4517681528232558\n",
            "Iteration 377800, Loss: 0.45176595313097345\n",
            "Iteration 377900, Loss: 0.4517637552281157\n",
            "Iteration 378000, Loss: 0.4517615591130012\n",
            "Iteration 378100, Loss: 0.45175936478395134\n",
            "Iteration 378200, Loss: 0.4517571722392888\n",
            "Iteration 378300, Loss: 0.45175498147733856\n",
            "Iteration 378400, Loss: 0.451752792496427\n",
            "Iteration 378500, Loss: 0.4517506052948827\n",
            "Iteration 378600, Loss: 0.4517484198710358\n",
            "Iteration 378700, Loss: 0.4517462362232187\n",
            "Iteration 378800, Loss: 0.4517440543497653\n",
            "Iteration 378900, Loss: 0.45174187424901124\n",
            "Iteration 379000, Loss: 0.45173969591929475\n",
            "Iteration 379100, Loss: 0.4517375193589548\n",
            "Iteration 379200, Loss: 0.451735344566333\n",
            "Iteration 379300, Loss: 0.4517331715397728\n",
            "Iteration 379400, Loss: 0.4517310002776189\n",
            "Iteration 379500, Loss: 0.45172883077821846\n",
            "Iteration 379600, Loss: 0.4517266630399198\n",
            "Iteration 379700, Loss: 0.4517244970610739\n",
            "Iteration 379800, Loss: 0.4517223328400329\n",
            "Iteration 379900, Loss: 0.45172017037515105\n",
            "Iteration 380000, Loss: 0.45171800966478437\n",
            "Iteration 380100, Loss: 0.4517158507072905\n",
            "Iteration 380200, Loss: 0.45171369350102925\n",
            "Iteration 380300, Loss: 0.45171153804436204\n",
            "Iteration 380400, Loss: 0.451709384335652\n",
            "Iteration 380500, Loss: 0.4517072323732645\n",
            "Iteration 380600, Loss: 0.45170508215556604\n",
            "Iteration 380700, Loss: 0.4517029336809255\n",
            "Iteration 380800, Loss: 0.45170078694771354\n",
            "Iteration 380900, Loss: 0.45169864195430204\n",
            "Iteration 381000, Loss: 0.45169649869906525\n",
            "Iteration 381100, Loss: 0.45169435718037915\n",
            "Iteration 381200, Loss: 0.45169221739662113\n",
            "Iteration 381300, Loss: 0.4516900793461708\n",
            "Iteration 381400, Loss: 0.45168794302740933\n",
            "Iteration 381500, Loss: 0.45168580843871997\n",
            "Iteration 381600, Loss: 0.4516836755784871\n",
            "Iteration 381700, Loss: 0.4516815444450975\n",
            "Iteration 381800, Loss: 0.4516794150369398\n",
            "Iteration 381900, Loss: 0.45167728735240353\n",
            "Iteration 382000, Loss: 0.45167516138988106\n",
            "Iteration 382100, Loss: 0.45167303714776585\n",
            "Iteration 382200, Loss: 0.45167091462445336\n",
            "Iteration 382300, Loss: 0.45166879381834096\n",
            "Iteration 382400, Loss: 0.4516666747278273\n",
            "Iteration 382500, Loss: 0.4516645573513134\n",
            "Iteration 382600, Loss: 0.4516624416872015\n",
            "Iteration 382700, Loss: 0.4516603277338961\n",
            "Iteration 382800, Loss: 0.45165821548980284\n",
            "Iteration 382900, Loss: 0.4516561049533298\n",
            "Iteration 383000, Loss: 0.4516539961228865\n",
            "Iteration 383100, Loss: 0.4516518889968838\n",
            "Iteration 383200, Loss: 0.451649783573735\n",
            "Iteration 383300, Loss: 0.45164767985185483\n",
            "Iteration 383400, Loss: 0.4516455778296596\n",
            "Iteration 383500, Loss: 0.45164347750556755\n",
            "Iteration 383600, Loss: 0.4516413788779987\n",
            "Iteration 383700, Loss: 0.4516392819453747\n",
            "Iteration 383800, Loss: 0.4516371867061191\n",
            "Iteration 383900, Loss: 0.45163509315865663\n",
            "Iteration 384000, Loss: 0.4516330013014145\n",
            "Iteration 384100, Loss: 0.4516309111328212\n",
            "Iteration 384200, Loss: 0.4516288226513071\n",
            "Iteration 384300, Loss: 0.4516267358553043\n",
            "Iteration 384400, Loss: 0.4516246507432463\n",
            "Iteration 384500, Loss: 0.45162256731356865\n",
            "Iteration 384600, Loss: 0.4516204855647086\n",
            "Iteration 384700, Loss: 0.4516184054951051\n",
            "Iteration 384800, Loss: 0.45161632710319854\n",
            "Iteration 384900, Loss: 0.45161425038743136\n",
            "Iteration 385000, Loss: 0.4516121753462475\n",
            "Iteration 385100, Loss: 0.4516101019780927\n",
            "Iteration 385200, Loss: 0.45160803028141444\n",
            "Iteration 385300, Loss: 0.45160596025466176\n",
            "Iteration 385400, Loss: 0.4516038918962855\n",
            "Iteration 385500, Loss: 0.4516018252047381\n",
            "Iteration 385600, Loss: 0.45159976017847364\n",
            "Iteration 385700, Loss: 0.45159769681594836\n",
            "Iteration 385800, Loss: 0.4515956351156196\n",
            "Iteration 385900, Loss: 0.45159357507594655\n",
            "Iteration 386000, Loss: 0.45159151669539005\n",
            "Iteration 386100, Loss: 0.45158945997241295\n",
            "Iteration 386200, Loss: 0.45158740490547944\n",
            "Iteration 386300, Loss: 0.45158535149305534\n",
            "Iteration 386400, Loss: 0.45158329973360867\n",
            "Iteration 386500, Loss: 0.4515812496256084\n",
            "Iteration 386600, Loss: 0.4515792011675255\n",
            "Iteration 386700, Loss: 0.4515771543578329\n",
            "Iteration 386800, Loss: 0.45157510919500476\n",
            "Iteration 386900, Loss: 0.4515730656775169\n",
            "Iteration 387000, Loss: 0.4515710238038471\n",
            "Iteration 387100, Loss: 0.45156898357247466\n",
            "Iteration 387200, Loss: 0.45156694498188044\n",
            "Iteration 387300, Loss: 0.4515649080305473\n",
            "Iteration 387400, Loss: 0.4515628727169591\n",
            "Iteration 387500, Loss: 0.4515608390396022\n",
            "Iteration 387600, Loss: 0.45155880699696366\n",
            "Iteration 387700, Loss: 0.451556776587533\n",
            "Iteration 387800, Loss: 0.4515547478098011\n",
            "Iteration 387900, Loss: 0.45155272066226027\n",
            "Iteration 388000, Loss: 0.4515506951434047\n",
            "Iteration 388100, Loss: 0.45154867125173015\n",
            "Iteration 388200, Loss: 0.451546648985734\n",
            "Iteration 388300, Loss: 0.4515446283439154\n",
            "Iteration 388400, Loss: 0.4515426093247749\n",
            "Iteration 388500, Loss: 0.45154059192681467\n",
            "Iteration 388600, Loss: 0.4515385761485389\n",
            "Iteration 388700, Loss: 0.4515365619884529\n",
            "Iteration 388800, Loss: 0.45153454944506405\n",
            "Iteration 388900, Loss: 0.45153253851688097\n",
            "Iteration 389000, Loss: 0.4515305292024141\n",
            "Iteration 389100, Loss: 0.4515285215001754\n",
            "Iteration 389200, Loss: 0.45152651540867855\n",
            "Iteration 389300, Loss: 0.4515245109264388\n",
            "Iteration 389400, Loss: 0.45152250805197325\n",
            "Iteration 389500, Loss: 0.45152050678379996\n",
            "Iteration 389600, Loss: 0.45151850712043917\n",
            "Iteration 389700, Loss: 0.4515165090604127\n",
            "Iteration 389800, Loss: 0.4515145126022435\n",
            "Iteration 389900, Loss: 0.4515125177444567\n",
            "Iteration 390000, Loss: 0.45151052448557877\n",
            "Iteration 390100, Loss: 0.45150853282413755\n",
            "Iteration 390200, Loss: 0.4515065427586629\n",
            "Iteration 390300, Loss: 0.45150455428768627\n",
            "Iteration 390400, Loss: 0.45150256740974015\n",
            "Iteration 390500, Loss: 0.45150058212335903\n",
            "Iteration 390600, Loss: 0.4514985984270789\n",
            "Iteration 390700, Loss: 0.45149661631943766\n",
            "Iteration 390800, Loss: 0.45149463579897414\n",
            "Iteration 390900, Loss: 0.45149265686422935\n",
            "Iteration 391000, Loss: 0.45149067951374544\n",
            "Iteration 391100, Loss: 0.4514887037460665\n",
            "Iteration 391200, Loss: 0.45148672955973795\n",
            "Iteration 391300, Loss: 0.4514847569533067\n",
            "Iteration 391400, Loss: 0.4514827859253216\n",
            "Iteration 391500, Loss: 0.4514808164743325\n",
            "Iteration 391600, Loss: 0.4514788485988917\n",
            "Iteration 391700, Loss: 0.4514768822975522\n",
            "Iteration 391800, Loss: 0.45147491756886887\n",
            "Iteration 391900, Loss: 0.4514729544113982\n",
            "Iteration 392000, Loss: 0.45147099282369807\n",
            "Iteration 392100, Loss: 0.4514690328043283\n",
            "Iteration 392200, Loss: 0.45146707435184974\n",
            "Iteration 392300, Loss: 0.4514651174648252\n",
            "Iteration 392400, Loss: 0.451463162141819\n",
            "Iteration 392500, Loss: 0.4514612083813967\n",
            "Iteration 392600, Loss: 0.45145925618212557\n",
            "Iteration 392700, Loss: 0.4514573055425746\n",
            "Iteration 392800, Loss: 0.4514553564613142\n",
            "Iteration 392900, Loss: 0.45145340893691605\n",
            "Iteration 393000, Loss: 0.4514514629679539\n",
            "Iteration 393100, Loss: 0.45144951855300275\n",
            "Iteration 393200, Loss: 0.45144757569063887\n",
            "Iteration 393300, Loss: 0.4514456343794404\n",
            "Iteration 393400, Loss: 0.45144369461798706\n",
            "Iteration 393500, Loss: 0.45144175640486\n",
            "Iteration 393600, Loss: 0.45143981973864183\n",
            "Iteration 393700, Loss: 0.4514378846179165\n",
            "Iteration 393800, Loss: 0.45143595104127016\n",
            "Iteration 393900, Loss: 0.4514340190072897\n",
            "Iteration 394000, Loss: 0.45143208851456385\n",
            "Iteration 394100, Loss: 0.45143015956168275\n",
            "Iteration 394200, Loss: 0.45142823214723865\n",
            "Iteration 394300, Loss: 0.4514263062698243\n",
            "Iteration 394400, Loss: 0.4514243819280348\n",
            "Iteration 394500, Loss: 0.4514224591204663\n",
            "Iteration 394600, Loss: 0.4514205378457167\n",
            "Iteration 394700, Loss: 0.4514186181023851\n",
            "Iteration 394800, Loss: 0.4514166998890726\n",
            "Iteration 394900, Loss: 0.45141478320438133\n",
            "Iteration 395000, Loss: 0.4514128680469151\n",
            "Iteration 395100, Loss: 0.45141095441527934\n",
            "Iteration 395200, Loss: 0.45140904230808054\n",
            "Iteration 395300, Loss: 0.45140713172392744\n",
            "Iteration 395400, Loss: 0.4514052226614296\n",
            "Iteration 395500, Loss: 0.45140331511919823\n",
            "Iteration 395600, Loss: 0.45140140909584603\n",
            "Iteration 395700, Loss: 0.45139950458998745\n",
            "Iteration 395800, Loss: 0.4513976016002381\n",
            "Iteration 395900, Loss: 0.45139570012521535\n",
            "Iteration 396000, Loss: 0.4513938001635376\n",
            "Iteration 396100, Loss: 0.4513919017138253\n",
            "Iteration 396200, Loss: 0.4513900047746999\n",
            "Iteration 396300, Loss: 0.45138810934478485\n",
            "Iteration 396400, Loss: 0.4513862154227042\n",
            "Iteration 396500, Loss: 0.4513843230070845\n",
            "Iteration 396600, Loss: 0.45138243209655304\n",
            "Iteration 396700, Loss: 0.4513805426897388\n",
            "Iteration 396800, Loss: 0.4513786547852725\n",
            "Iteration 396900, Loss: 0.45137676838178575\n",
            "Iteration 397000, Loss: 0.45137488347791205\n",
            "Iteration 397100, Loss: 0.45137300007228637\n",
            "Iteration 397200, Loss: 0.4513711181635449\n",
            "Iteration 397300, Loss: 0.4513692377503253\n",
            "Iteration 397400, Loss: 0.45136735883126705\n",
            "Iteration 397500, Loss: 0.45136548140501065\n",
            "Iteration 397600, Loss: 0.45136360547019816\n",
            "Iteration 397700, Loss: 0.4513617310254734\n",
            "Iteration 397800, Loss: 0.4513598580694811\n",
            "Iteration 397900, Loss: 0.45135798660086784\n",
            "Iteration 398000, Loss: 0.45135611661828157\n",
            "Iteration 398100, Loss: 0.45135424812037145\n",
            "Iteration 398200, Loss: 0.4513523811057885\n",
            "Iteration 398300, Loss: 0.45135051557318473\n",
            "Iteration 398400, Loss: 0.451348651521214\n",
            "Iteration 398500, Loss: 0.45134678894853125\n",
            "Iteration 398600, Loss: 0.45134492785379304\n",
            "Iteration 398700, Loss: 0.4513430682356572\n",
            "Iteration 398800, Loss: 0.4513412100927834\n",
            "Iteration 398900, Loss: 0.4513393534238322\n",
            "Iteration 399000, Loss: 0.451337498227466\n",
            "Iteration 399100, Loss: 0.4513356445023483\n",
            "Iteration 399200, Loss: 0.45133379224714426\n",
            "Iteration 399300, Loss: 0.4513319414605203\n",
            "Iteration 399400, Loss: 0.45133009214114445\n",
            "Iteration 399500, Loss: 0.45132824428768575\n",
            "Iteration 399600, Loss: 0.4513263978988155\n",
            "Iteration 399700, Loss: 0.45132455297320534\n",
            "Iteration 399800, Loss: 0.4513227095095291\n",
            "Iteration 399900, Loss: 0.45132086750646166\n",
            "Iteration 400000, Loss: 0.4513190269626794\n",
            "Iteration 400100, Loss: 0.4513171878768601\n",
            "Iteration 400200, Loss: 0.451315350247683\n",
            "Iteration 400300, Loss: 0.4513135140738287\n",
            "Iteration 400400, Loss: 0.4513116793539791\n",
            "Iteration 400500, Loss: 0.4513098460868177\n",
            "Iteration 400600, Loss: 0.45130801427102923\n",
            "Iteration 400700, Loss: 0.4513061839052999\n",
            "Iteration 400800, Loss: 0.45130435498831717\n",
            "Iteration 400900, Loss: 0.4513025275187702\n",
            "Iteration 401000, Loss: 0.4513007014953492\n",
            "Iteration 401100, Loss: 0.45129887691674614\n",
            "Iteration 401200, Loss: 0.45129705378165375\n",
            "Iteration 401300, Loss: 0.4512952320887669\n",
            "Iteration 401400, Loss: 0.4512934118367813\n",
            "Iteration 401500, Loss: 0.4512915930243943\n",
            "Iteration 401600, Loss: 0.4512897756503046\n",
            "Iteration 401700, Loss: 0.4512879597132122\n",
            "Iteration 401800, Loss: 0.4512861452118184\n",
            "Iteration 401900, Loss: 0.45128433214482605\n",
            "Iteration 402000, Loss: 0.4512825205109394\n",
            "Iteration 402100, Loss: 0.45128071030886385\n",
            "Iteration 402200, Loss: 0.45127890153730643\n",
            "Iteration 402300, Loss: 0.4512770941949752\n",
            "Iteration 402400, Loss: 0.45127528828057994\n",
            "Iteration 402500, Loss: 0.4512734837928317\n",
            "Iteration 402600, Loss: 0.4512716807304427\n",
            "Iteration 402700, Loss: 0.4512698790921267\n",
            "Iteration 402800, Loss: 0.45126807887659875\n",
            "Iteration 402900, Loss: 0.4512662800825753\n",
            "Iteration 403000, Loss: 0.45126448270877423\n",
            "Iteration 403100, Loss: 0.45126268675391457\n",
            "Iteration 403200, Loss: 0.4512608922167168\n",
            "Iteration 403300, Loss: 0.4512590990959027\n",
            "Iteration 403400, Loss: 0.45125730739019565\n",
            "Iteration 403500, Loss: 0.4512555170983202\n",
            "Iteration 403600, Loss: 0.4512537282190019\n",
            "Iteration 403700, Loss: 0.4512519407509684\n",
            "Iteration 403800, Loss: 0.45125015469294794\n",
            "Iteration 403900, Loss: 0.45124837004367047\n",
            "Iteration 404000, Loss: 0.45124658680186736\n",
            "Iteration 404100, Loss: 0.45124480496627123\n",
            "Iteration 404200, Loss: 0.4512430245356157\n",
            "Iteration 404300, Loss: 0.4512412455086365\n",
            "Iteration 404400, Loss: 0.4512394678840697\n",
            "Iteration 404500, Loss: 0.45123769166065364\n",
            "Iteration 404600, Loss: 0.45123591683712727\n",
            "Iteration 404700, Loss: 0.4512341434122313\n",
            "Iteration 404800, Loss: 0.45123237138470745\n",
            "Iteration 404900, Loss: 0.4512306007532992\n",
            "Iteration 405000, Loss: 0.45122883151675097\n",
            "Iteration 405100, Loss: 0.4512270636738084\n",
            "Iteration 405200, Loss: 0.451225297223219\n",
            "Iteration 405300, Loss: 0.45122353216373123\n",
            "Iteration 405400, Loss: 0.4512217684940947\n",
            "Iteration 405500, Loss: 0.4512200062130607\n",
            "Iteration 405600, Loss: 0.45121824531938143\n",
            "Iteration 405700, Loss: 0.45121648581181095\n",
            "Iteration 405800, Loss: 0.4512147276891041\n",
            "Iteration 405900, Loss: 0.4512129709500172\n",
            "Iteration 406000, Loss: 0.4512112155933082\n",
            "Iteration 406100, Loss: 0.45120946161773595\n",
            "Iteration 406200, Loss: 0.4512077090220603\n",
            "Iteration 406300, Loss: 0.4512059578050436\n",
            "Iteration 406400, Loss: 0.45120420796544797\n",
            "Iteration 406500, Loss: 0.4512024595020382\n",
            "Iteration 406600, Loss: 0.4512007124135793\n",
            "Iteration 406700, Loss: 0.4511989666988383\n",
            "Iteration 406800, Loss: 0.4511972223565831\n",
            "Iteration 406900, Loss: 0.45119547938558324\n",
            "Iteration 407000, Loss: 0.45119373778460903\n",
            "Iteration 407100, Loss: 0.45119199755243256\n",
            "Iteration 407200, Loss: 0.45119025868782725\n",
            "Iteration 407300, Loss: 0.4511885211895672\n",
            "Iteration 407400, Loss: 0.45118678505642845\n",
            "Iteration 407500, Loss: 0.451185050287188\n",
            "Iteration 407600, Loss: 0.4511833168806243\n",
            "Iteration 407700, Loss: 0.4511815848355166\n",
            "Iteration 407800, Loss: 0.45117985415064626\n",
            "Iteration 407900, Loss: 0.451178124824795\n",
            "Iteration 408000, Loss: 0.45117639685674665\n",
            "Iteration 408100, Loss: 0.4511746702452857\n",
            "Iteration 408200, Loss: 0.45117294498919813\n",
            "Iteration 408300, Loss: 0.45117122108727126\n",
            "Iteration 408400, Loss: 0.45116949853829375\n",
            "Iteration 408500, Loss: 0.45116777734105507\n",
            "Iteration 408600, Loss: 0.4511660574943466\n",
            "Iteration 408700, Loss: 0.4511643389969603\n",
            "Iteration 408800, Loss: 0.45116262184769007\n",
            "Iteration 408900, Loss: 0.4511609060453305\n",
            "Iteration 409000, Loss: 0.4511591915886779\n",
            "Iteration 409100, Loss: 0.4511574784765294\n",
            "Iteration 409200, Loss: 0.45115576670768376\n",
            "Iteration 409300, Loss: 0.45115405628094085\n",
            "Iteration 409400, Loss: 0.45115234719510156\n",
            "Iteration 409500, Loss: 0.45115063944896855\n",
            "Iteration 409600, Loss: 0.45114893304134523\n",
            "Iteration 409700, Loss: 0.45114722797103646\n",
            "Iteration 409800, Loss: 0.45114552423684845\n",
            "Iteration 409900, Loss: 0.4511438218375885\n",
            "Iteration 410000, Loss: 0.451142120772065\n",
            "Iteration 410100, Loss: 0.4511404210390881\n",
            "Iteration 410200, Loss: 0.4511387226374686\n",
            "Iteration 410300, Loss: 0.45113702556601903\n",
            "Iteration 410400, Loss: 0.45113532982355287\n",
            "Iteration 410500, Loss: 0.45113363540888485\n",
            "Iteration 410600, Loss: 0.45113194232083087\n",
            "Iteration 410700, Loss: 0.45113025055820827\n",
            "Iteration 410800, Loss: 0.4511285601198356\n",
            "Iteration 410900, Loss: 0.45112687100453264\n",
            "Iteration 411000, Loss: 0.45112518321112005\n",
            "Iteration 411100, Loss: 0.4511234967384201\n",
            "Iteration 411200, Loss: 0.45112181158525627\n",
            "Iteration 411300, Loss: 0.45112012775045307\n",
            "Iteration 411400, Loss: 0.4511184452328363\n",
            "Iteration 411500, Loss: 0.451116764031233\n",
            "Iteration 411600, Loss: 0.45111508414447155\n",
            "Iteration 411700, Loss: 0.45111340557138135\n",
            "Iteration 411800, Loss: 0.45111172831079305\n",
            "Iteration 411900, Loss: 0.45111005236153856\n",
            "Iteration 412000, Loss: 0.45110837772245127\n",
            "Iteration 412100, Loss: 0.4511067043923652\n",
            "Iteration 412200, Loss: 0.45110503237011595\n",
            "Iteration 412300, Loss: 0.4511033616545403\n",
            "Iteration 412400, Loss: 0.4511016922444762\n",
            "Iteration 412500, Loss: 0.45110002413876293\n",
            "Iteration 412600, Loss: 0.4510983573362407\n",
            "Iteration 412700, Loss: 0.4510966918357511\n",
            "Iteration 412800, Loss: 0.4510950276361369\n",
            "Iteration 412900, Loss: 0.45109336473624223\n",
            "Iteration 413000, Loss: 0.4510917031349122\n",
            "Iteration 413100, Loss: 0.45109004283099274\n",
            "Iteration 413200, Loss: 0.4510883838233321\n",
            "Iteration 413300, Loss: 0.45108672611077855\n",
            "Iteration 413400, Loss: 0.4510850696921823\n",
            "Iteration 413500, Loss: 0.4510834145663942\n",
            "Iteration 413600, Loss: 0.451081760732267\n",
            "Iteration 413700, Loss: 0.4510801081886537\n",
            "Iteration 413800, Loss: 0.4510784569344094\n",
            "Iteration 413900, Loss: 0.4510768069683897\n",
            "Iteration 414000, Loss: 0.45107515828945194\n",
            "Iteration 414100, Loss: 0.4510735108964541\n",
            "Iteration 414200, Loss: 0.4510718647882557\n",
            "Iteration 414300, Loss: 0.4510702199637175\n",
            "Iteration 414400, Loss: 0.45106857642170106\n",
            "Iteration 414500, Loss: 0.45106693416106936\n",
            "Iteration 414600, Loss: 0.45106529318068667\n",
            "Iteration 414700, Loss: 0.45106365347941824\n",
            "Iteration 414800, Loss: 0.4510620150561305\n",
            "Iteration 414900, Loss: 0.45106037790969133\n",
            "Iteration 415000, Loss: 0.45105874203896906\n",
            "Iteration 415100, Loss: 0.45105710744283417\n",
            "Iteration 415200, Loss: 0.45105547412015756\n",
            "Iteration 415300, Loss: 0.45105384206981153\n",
            "Iteration 415400, Loss: 0.45105221129066975\n",
            "Iteration 415500, Loss: 0.4510505817816066\n",
            "Iteration 415600, Loss: 0.45104895354149815\n",
            "Iteration 415700, Loss: 0.4510473265692212\n",
            "Iteration 415800, Loss: 0.45104570086365375\n",
            "Iteration 415900, Loss: 0.45104407642367533\n",
            "Iteration 416000, Loss: 0.4510424532481663\n",
            "Iteration 416100, Loss: 0.4510408313360082\n",
            "Iteration 416200, Loss: 0.4510392106860837\n",
            "Iteration 416300, Loss: 0.45103759129727683\n",
            "Iteration 416400, Loss: 0.45103597316847255\n",
            "Iteration 416500, Loss: 0.4510343562985568\n",
            "Iteration 416600, Loss: 0.4510327406864176\n",
            "Iteration 416700, Loss: 0.45103112633094267\n",
            "Iteration 416800, Loss: 0.45102951323102203\n",
            "Iteration 416900, Loss: 0.4510279013855463\n",
            "Iteration 417000, Loss: 0.4510262907934074\n",
            "Iteration 417100, Loss: 0.4510246814534985\n",
            "Iteration 417200, Loss: 0.45102307336471353\n",
            "Iteration 417300, Loss: 0.4510214665259481\n",
            "Iteration 417400, Loss: 0.4510198609360984\n",
            "Iteration 417500, Loss: 0.4510182565940622\n",
            "Iteration 417600, Loss: 0.451016653498738\n",
            "Iteration 417700, Loss: 0.45101505164902583\n",
            "Iteration 417800, Loss: 0.4510134510438267\n",
            "Iteration 417900, Loss: 0.4510118516820427\n",
            "Iteration 418000, Loss: 0.45101025356257696\n",
            "Iteration 418100, Loss: 0.4510086566843338\n",
            "Iteration 418200, Loss: 0.45100706104621874\n",
            "Iteration 418300, Loss: 0.4510054666471387\n",
            "Iteration 418400, Loss: 0.4510038734860009\n",
            "Iteration 418500, Loss: 0.45100228156171457\n",
            "Iteration 418600, Loss: 0.4510006908731896\n",
            "Iteration 418700, Loss: 0.45099910141933686\n",
            "Iteration 418800, Loss: 0.4509975131990688\n",
            "Iteration 418900, Loss: 0.4509959262112986\n",
            "Iteration 419000, Loss: 0.4509943404549408\n",
            "Iteration 419100, Loss: 0.45099275592891086\n",
            "Iteration 419200, Loss: 0.45099117263212557\n",
            "Iteration 419300, Loss: 0.4509895905635025\n",
            "Iteration 419400, Loss: 0.45098800972196074\n",
            "Iteration 419500, Loss: 0.45098643010642\n",
            "Iteration 419600, Loss: 0.4509848517158016\n",
            "Iteration 419700, Loss: 0.45098327454902754\n",
            "Iteration 419800, Loss: 0.4509816986050213\n",
            "Iteration 419900, Loss: 0.45098012388270714\n",
            "Iteration 420000, Loss: 0.4509785503810106\n",
            "Iteration 420100, Loss: 0.4509769780988583\n",
            "Iteration 420200, Loss: 0.4509754070351778\n",
            "Iteration 420300, Loss: 0.45097383718889805\n",
            "Iteration 420400, Loss: 0.45097226855894895\n",
            "Iteration 420500, Loss: 0.4509707011442613\n",
            "Iteration 420600, Loss: 0.4509691349437672\n",
            "Iteration 420700, Loss: 0.45096756995640014\n",
            "Iteration 420800, Loss: 0.4509660061810939\n",
            "Iteration 420900, Loss: 0.4509644436167841\n",
            "Iteration 421000, Loss: 0.450962882262407\n",
            "Iteration 421100, Loss: 0.4509613221169003\n",
            "Iteration 421200, Loss: 0.45095976317920244\n",
            "Iteration 421300, Loss: 0.45095820544825327\n",
            "Iteration 421400, Loss: 0.45095664892299325\n",
            "Iteration 421500, Loss: 0.4509550936023645\n",
            "Iteration 421600, Loss: 0.4509535394853099\n",
            "Iteration 421700, Loss: 0.45095198657077334\n",
            "Iteration 421800, Loss: 0.4509504348576999\n",
            "Iteration 421900, Loss: 0.4509488843450359\n",
            "Iteration 422000, Loss: 0.4509473350317283\n",
            "Iteration 422100, Loss: 0.45094578691672577\n",
            "Iteration 422200, Loss: 0.4509442399989772\n",
            "Iteration 422300, Loss: 0.4509426942774335\n",
            "Iteration 422400, Loss: 0.4509411497510458\n",
            "Iteration 422500, Loss: 0.45093960641876696\n",
            "Iteration 422600, Loss: 0.4509380642795504\n",
            "Iteration 422700, Loss: 0.4509365233323509\n",
            "Iteration 422800, Loss: 0.4509349835761244\n",
            "Iteration 422900, Loss: 0.4509334450098275\n",
            "Iteration 423000, Loss: 0.4509319076324184\n",
            "Iteration 423100, Loss: 0.45093037144285564\n",
            "Iteration 423200, Loss: 0.45092883644009957\n",
            "Iteration 423300, Loss: 0.4509273026231112\n",
            "Iteration 423400, Loss: 0.45092576999085243\n",
            "Iteration 423500, Loss: 0.4509242385422866\n",
            "Iteration 423600, Loss: 0.45092270827637804\n",
            "Iteration 423700, Loss: 0.45092117919209196\n",
            "Iteration 423800, Loss: 0.45091965128839473\n",
            "Iteration 423900, Loss: 0.45091812456425373\n",
            "Iteration 424000, Loss: 0.45091659901863734\n",
            "Iteration 424100, Loss: 0.4509150746505151\n",
            "Iteration 424200, Loss: 0.4509135514588576\n",
            "Iteration 424300, Loss: 0.45091202944263636\n",
            "Iteration 424400, Loss: 0.45091050860082404\n",
            "Iteration 424500, Loss: 0.45090898893239423\n",
            "Iteration 424600, Loss: 0.4509074704363217\n",
            "Iteration 424700, Loss: 0.4509059531115822\n",
            "Iteration 424800, Loss: 0.4509044369571523\n",
            "Iteration 424900, Loss: 0.4509029219720102\n",
            "Iteration 425000, Loss: 0.4509014081551346\n",
            "Iteration 425100, Loss: 0.4508998955055054\n",
            "Iteration 425200, Loss: 0.45089838402210347\n",
            "Iteration 425300, Loss: 0.4508968737039107\n",
            "Iteration 425400, Loss: 0.4508953645499103\n",
            "Iteration 425500, Loss: 0.4508938565590862\n",
            "Iteration 425600, Loss: 0.45089234973042347\n",
            "Iteration 425700, Loss: 0.450890844062908\n",
            "Iteration 425800, Loss: 0.4508893395555273\n",
            "Iteration 425900, Loss: 0.45088783620726897\n",
            "Iteration 426000, Loss: 0.45088633401712264\n",
            "Iteration 426100, Loss: 0.4508848329840782\n",
            "Iteration 426200, Loss: 0.45088333310712714\n",
            "Iteration 426300, Loss: 0.4508818343852613\n",
            "Iteration 426400, Loss: 0.4508803368174743\n",
            "Iteration 426500, Loss: 0.45087884040276\n",
            "Iteration 426600, Loss: 0.4508773451401141\n",
            "Iteration 426700, Loss: 0.45087585102853245\n",
            "Iteration 426800, Loss: 0.4508743580670129\n",
            "Iteration 426900, Loss: 0.45087286625455314\n",
            "Iteration 427000, Loss: 0.4508713755901531\n",
            "Iteration 427100, Loss: 0.45086988607281264\n",
            "Iteration 427200, Loss: 0.4508683977015334\n",
            "Iteration 427300, Loss: 0.45086691047531763\n",
            "Iteration 427400, Loss: 0.4508654243931687\n",
            "Iteration 427500, Loss: 0.450863939454091\n",
            "Iteration 427600, Loss: 0.45086245565708993\n",
            "Iteration 427700, Loss: 0.4508609730011719\n",
            "Iteration 427800, Loss: 0.45085949148534415\n",
            "Iteration 427900, Loss: 0.45085801110861523\n",
            "Iteration 428000, Loss: 0.45085653186999447\n",
            "Iteration 428100, Loss: 0.45085505376849205\n",
            "Iteration 428200, Loss: 0.45085357680311966\n",
            "Iteration 428300, Loss: 0.4508521009728897\n",
            "Iteration 428400, Loss: 0.45085062627681527\n",
            "Iteration 428500, Loss: 0.4508491527139108\n",
            "Iteration 428600, Loss: 0.45084768028319183\n",
            "Iteration 428700, Loss: 0.4508462089836746\n",
            "Iteration 428800, Loss: 0.45084473881437637\n",
            "Iteration 428900, Loss: 0.4508432697743155\n",
            "Iteration 429000, Loss: 0.4508418018625115\n",
            "Iteration 429100, Loss: 0.4508403350779846\n",
            "Iteration 429200, Loss: 0.4508388694197558\n",
            "Iteration 429300, Loss: 0.4508374048868479\n",
            "Iteration 429400, Loss: 0.4508359414782837\n",
            "Iteration 429500, Loss: 0.4508344791930878\n",
            "Iteration 429600, Loss: 0.4508330180302851\n",
            "Iteration 429700, Loss: 0.45083155798890195\n",
            "Iteration 429800, Loss: 0.4508300990679656\n",
            "Iteration 429900, Loss: 0.4508286412665042\n",
            "Iteration 430000, Loss: 0.45082718458354676\n",
            "Iteration 430100, Loss: 0.45082572901812346\n",
            "Iteration 430200, Loss: 0.45082427456926555\n",
            "Iteration 430300, Loss: 0.4508228212360048\n",
            "Iteration 430400, Loss: 0.4508213690173746\n",
            "Iteration 430500, Loss: 0.4508199179124086\n",
            "Iteration 430600, Loss: 0.45081846792014196\n",
            "Iteration 430700, Loss: 0.4508170190396106\n",
            "Iteration 430800, Loss: 0.4508155712698515\n",
            "Iteration 430900, Loss: 0.45081412460990244\n",
            "Iteration 431000, Loss: 0.4508126790588023\n",
            "Iteration 431100, Loss: 0.4508112346155908\n",
            "Iteration 431200, Loss: 0.45080979127930887\n",
            "Iteration 431300, Loss: 0.45080834904899825\n",
            "Iteration 431400, Loss: 0.4508069079237014\n",
            "Iteration 431500, Loss: 0.4508054679024623\n",
            "Iteration 431600, Loss: 0.4508040289843252\n",
            "Iteration 431700, Loss: 0.450802591168336\n",
            "Iteration 431800, Loss: 0.4508011544535411\n",
            "Iteration 431900, Loss: 0.45079971883898795\n",
            "Iteration 432000, Loss: 0.45079828432372493\n",
            "Iteration 432100, Loss: 0.45079685090680144\n",
            "Iteration 432200, Loss: 0.450795418587268\n",
            "Iteration 432300, Loss: 0.4507939873641757\n",
            "Iteration 432400, Loss: 0.45079255723657696\n",
            "Iteration 432500, Loss: 0.4507911282035247\n",
            "Iteration 432600, Loss: 0.4507897002640734\n",
            "Iteration 432700, Loss: 0.4507882734172779\n",
            "Iteration 432800, Loss: 0.45078684766219423\n",
            "Iteration 432900, Loss: 0.45078542299787955\n",
            "Iteration 433000, Loss: 0.4507839994233915\n",
            "Iteration 433100, Loss: 0.450782576937789\n",
            "Iteration 433200, Loss: 0.4507811555401322\n",
            "Iteration 433300, Loss: 0.45077973522948134\n",
            "Iteration 433400, Loss: 0.4507783160048986\n",
            "Iteration 433500, Loss: 0.45077689786544595\n",
            "Iteration 433600, Loss: 0.45077548081018737\n",
            "Iteration 433700, Loss: 0.4507740648381874\n",
            "Iteration 433800, Loss: 0.45077264994851113\n",
            "Iteration 433900, Loss: 0.4507712361402252\n",
            "Iteration 434000, Loss: 0.4507698234123968\n",
            "Iteration 434100, Loss: 0.4507684117640942\n",
            "Iteration 434200, Loss: 0.4507670011943863\n",
            "Iteration 434300, Loss: 0.4507655917023435\n",
            "Iteration 434400, Loss: 0.4507641832870366\n",
            "Iteration 434500, Loss: 0.45076277594753766\n",
            "Iteration 434600, Loss: 0.4507613696829193\n",
            "Iteration 434700, Loss: 0.45075996449225536\n",
            "Iteration 434800, Loss: 0.45075856037462075\n",
            "Iteration 434900, Loss: 0.4507571573290909\n",
            "Iteration 435000, Loss: 0.45075575535474255\n",
            "Iteration 435100, Loss: 0.4507543544506529\n",
            "Iteration 435200, Loss: 0.45075295461590037\n",
            "Iteration 435300, Loss: 0.45075155584956444\n",
            "Iteration 435400, Loss: 0.45075015815072494\n",
            "Iteration 435500, Loss: 0.45074876151846344\n",
            "Iteration 435600, Loss: 0.4507473659518619\n",
            "Iteration 435700, Loss: 0.45074597145000306\n",
            "Iteration 435800, Loss: 0.450744578011971\n",
            "Iteration 435900, Loss: 0.4507431856368504\n",
            "Iteration 436000, Loss: 0.450741794323727\n",
            "Iteration 436100, Loss: 0.45074040407168736\n",
            "Iteration 436200, Loss: 0.450739014879819\n",
            "Iteration 436300, Loss: 0.4507376267472104\n",
            "Iteration 436400, Loss: 0.45073623967295084\n",
            "Iteration 436500, Loss: 0.45073485365613025\n",
            "Iteration 436600, Loss: 0.4507334686958404\n",
            "Iteration 436700, Loss: 0.4507320847911729\n",
            "Iteration 436800, Loss: 0.4507307019412206\n",
            "Iteration 436900, Loss: 0.4507293201450777\n",
            "Iteration 437000, Loss: 0.45072793940183864\n",
            "Iteration 437100, Loss: 0.45072655971059916\n",
            "Iteration 437200, Loss: 0.4507251810704558\n",
            "Iteration 437300, Loss: 0.4507238034805058\n",
            "Iteration 437400, Loss: 0.450722426939848\n",
            "Iteration 437500, Loss: 0.45072105144758096\n",
            "Iteration 437600, Loss: 0.45071967700280524\n",
            "Iteration 437700, Loss: 0.45071830360462184\n",
            "Iteration 437800, Loss: 0.4507169312521322\n",
            "Iteration 437900, Loss: 0.4507155599444399\n",
            "Iteration 438000, Loss: 0.4507141896806479\n",
            "Iteration 438100, Loss: 0.45071282045986116\n",
            "Iteration 438200, Loss: 0.45071145228118487\n",
            "Iteration 438300, Loss: 0.45071008514372546\n",
            "Iteration 438400, Loss: 0.4507087190465905\n",
            "Iteration 438500, Loss: 0.4507073539888875\n",
            "Iteration 438600, Loss: 0.4507059899697261\n",
            "Iteration 438700, Loss: 0.4507046269882157\n",
            "Iteration 438800, Loss: 0.450703265043467\n",
            "Iteration 438900, Loss: 0.45070190413459205\n",
            "Iteration 439000, Loss: 0.45070054426070316\n",
            "Iteration 439100, Loss: 0.4506991854209137\n",
            "Iteration 439200, Loss: 0.45069782761433785\n",
            "Iteration 439300, Loss: 0.45069647084009096\n",
            "Iteration 439400, Loss: 0.4506951150972889\n",
            "Iteration 439500, Loss: 0.4506937603850487\n",
            "Iteration 439600, Loss: 0.4506924067024879\n",
            "Iteration 439700, Loss: 0.45069105404872545\n",
            "Iteration 439800, Loss: 0.45068970242288064\n",
            "Iteration 439900, Loss: 0.4506883518240741\n",
            "Iteration 440000, Loss: 0.45068700225142655\n",
            "Iteration 440100, Loss: 0.45068565370406066\n",
            "Iteration 440200, Loss: 0.4506843061810994\n",
            "Iteration 440300, Loss: 0.4506829596816663\n",
            "Iteration 440400, Loss: 0.45068161420488617\n",
            "Iteration 440500, Loss: 0.4506802697498847\n",
            "Iteration 440600, Loss: 0.45067892631578826\n",
            "Iteration 440700, Loss: 0.4506775839017243\n",
            "Iteration 440800, Loss: 0.4506762425068207\n",
            "Iteration 440900, Loss: 0.4506749021302067\n",
            "Iteration 441000, Loss: 0.45067356277101234\n",
            "Iteration 441100, Loss: 0.45067222442836796\n",
            "Iteration 441200, Loss: 0.45067088710140546\n",
            "Iteration 441300, Loss: 0.45066955078925724\n",
            "Iteration 441400, Loss: 0.4506682154910566\n",
            "Iteration 441500, Loss: 0.45066688120593756\n",
            "Iteration 441600, Loss: 0.45066554793303554\n",
            "Iteration 441700, Loss: 0.45066421567148596\n",
            "Iteration 441800, Loss: 0.4506628844204258\n",
            "Iteration 441900, Loss: 0.4506615541789925\n",
            "Iteration 442000, Loss: 0.4506602249463247\n",
            "Iteration 442100, Loss: 0.4506588967215615\n",
            "Iteration 442200, Loss: 0.45065756950384306\n",
            "Iteration 442300, Loss: 0.45065624329231024\n",
            "Iteration 442400, Loss: 0.450654918086105\n",
            "Iteration 442500, Loss: 0.4506535938843701\n",
            "Iteration 442600, Loss: 0.45065227068624875\n",
            "Iteration 442700, Loss: 0.4506509484908854\n",
            "Iteration 442800, Loss: 0.45064962729742525\n",
            "Iteration 442900, Loss: 0.45064830710501447\n",
            "Iteration 443000, Loss: 0.4506469879127997\n",
            "Iteration 443100, Loss: 0.4506456697199287\n",
            "Iteration 443200, Loss: 0.45064435252555024\n",
            "Iteration 443300, Loss: 0.4506430363288134\n",
            "Iteration 443400, Loss: 0.45064172112886836\n",
            "Iteration 443500, Loss: 0.4506404069248664\n",
            "Iteration 443600, Loss: 0.45063909371595934\n",
            "Iteration 443700, Loss: 0.45063778150129985\n",
            "Iteration 443800, Loss: 0.45063647028004133\n",
            "Iteration 443900, Loss: 0.45063516005133836\n",
            "Iteration 444000, Loss: 0.45063385081434615\n",
            "Iteration 444100, Loss: 0.45063254256822066\n",
            "Iteration 444200, Loss: 0.4506312353121188\n",
            "Iteration 444300, Loss: 0.4506299290451981\n",
            "Iteration 444400, Loss: 0.45062862376661733\n",
            "Iteration 444500, Loss: 0.45062731947553564\n",
            "Iteration 444600, Loss: 0.4506260161711132\n",
            "Iteration 444700, Loss: 0.4506247138525112\n",
            "Iteration 444800, Loss: 0.4506234125188911\n",
            "Iteration 444900, Loss: 0.4506221121694159\n",
            "Iteration 445000, Loss: 0.4506208128032489\n",
            "Iteration 445100, Loss: 0.4506195144195543\n",
            "Iteration 445200, Loss: 0.4506182170174972\n",
            "Iteration 445300, Loss: 0.45061692059624364\n",
            "Iteration 445400, Loss: 0.45061562515496023\n",
            "Iteration 445500, Loss: 0.4506143306928146\n",
            "Iteration 445600, Loss: 0.450613037208975\n",
            "Iteration 445700, Loss: 0.4506117447026107\n",
            "Iteration 445800, Loss: 0.4506104531728916\n",
            "Iteration 445900, Loss: 0.45060916261898853\n",
            "Iteration 446000, Loss: 0.4506078730400731\n",
            "Iteration 446100, Loss: 0.45060658443531776\n",
            "Iteration 446200, Loss: 0.4506052968038957\n",
            "Iteration 446300, Loss: 0.45060401014498086\n",
            "Iteration 446400, Loss: 0.45060272445774824\n",
            "Iteration 446500, Loss: 0.45060143974137334\n",
            "Iteration 446600, Loss: 0.45060015599503284\n",
            "Iteration 446700, Loss: 0.4505988732179038\n",
            "Iteration 446800, Loss: 0.4505975914091644\n",
            "Iteration 446900, Loss: 0.4505963105679932\n",
            "Iteration 447000, Loss: 0.4505950306935703\n",
            "Iteration 447100, Loss: 0.45059375178507594\n",
            "Iteration 447200, Loss: 0.4505924738416914\n",
            "Iteration 447300, Loss: 0.4505911968625988\n",
            "Iteration 447400, Loss: 0.450589920846981\n",
            "Iteration 447500, Loss: 0.4505886457940215\n",
            "Iteration 447600, Loss: 0.4505873717029049\n",
            "Iteration 447700, Loss: 0.4505860985728166\n",
            "Iteration 447800, Loss: 0.45058482640294245\n",
            "Iteration 447900, Loss: 0.45058355519246923\n",
            "Iteration 448000, Loss: 0.4505822849405847\n",
            "Iteration 448100, Loss: 0.4505810156464774\n",
            "Iteration 448200, Loss: 0.4505797473093365\n",
            "Iteration 448300, Loss: 0.45057847992835165\n",
            "Iteration 448400, Loss: 0.4505772135027141\n",
            "Iteration 448500, Loss: 0.45057594803161527\n",
            "Iteration 448600, Loss: 0.45057468351424745\n",
            "Iteration 448700, Loss: 0.45057341994980393\n",
            "Iteration 448800, Loss: 0.45057215733747863\n",
            "Iteration 448900, Loss: 0.45057089567646624\n",
            "Iteration 449000, Loss: 0.45056963496596225\n",
            "Iteration 449100, Loss: 0.45056837520516296\n",
            "Iteration 449200, Loss: 0.4505671163932656\n",
            "Iteration 449300, Loss: 0.45056585852946784\n",
            "Iteration 449400, Loss: 0.4505646016129685\n",
            "Iteration 449500, Loss: 0.4505633456429668\n",
            "Iteration 449600, Loss: 0.45056209061866304\n",
            "Iteration 449700, Loss: 0.45056083653925816\n",
            "Iteration 449800, Loss: 0.450559583403954\n",
            "Iteration 449900, Loss: 0.450558331211953\n",
            "Iteration 450000, Loss: 0.4505570799624586\n",
            "Iteration 450100, Loss: 0.4505558296546747\n",
            "Iteration 450200, Loss: 0.45055458028780626\n",
            "Iteration 450300, Loss: 0.45055333186105884\n",
            "Iteration 450400, Loss: 0.4505520843736389\n",
            "Iteration 450500, Loss: 0.4505508378247536\n",
            "Iteration 450600, Loss: 0.4505495922136108\n",
            "Iteration 450700, Loss: 0.45054834753941936\n",
            "Iteration 450800, Loss: 0.45054710380138857\n",
            "Iteration 450900, Loss: 0.45054586099872873\n",
            "Iteration 451000, Loss: 0.45054461913065097\n",
            "Iteration 451100, Loss: 0.4505433781963669\n",
            "Iteration 451200, Loss: 0.4505421381950891\n",
            "Iteration 451300, Loss: 0.4505408991260309\n",
            "Iteration 451400, Loss: 0.45053966098840625\n",
            "Iteration 451500, Loss: 0.4505384237814303\n",
            "Iteration 451600, Loss: 0.4505371875043183\n",
            "Iteration 451700, Loss: 0.4505359521562867\n",
            "Iteration 451800, Loss: 0.45053471773655257\n",
            "Iteration 451900, Loss: 0.45053348424433387\n",
            "Iteration 452000, Loss: 0.45053225167884925\n",
            "Iteration 452100, Loss: 0.45053102003931794\n",
            "Iteration 452200, Loss: 0.45052978932496013\n",
            "Iteration 452300, Loss: 0.4505285595349966\n",
            "Iteration 452400, Loss: 0.45052733066864925\n",
            "Iteration 452500, Loss: 0.4505261027251404\n",
            "Iteration 452600, Loss: 0.45052487570369304\n",
            "Iteration 452700, Loss: 0.45052364960353125\n",
            "Iteration 452800, Loss: 0.4505224244238797\n",
            "Iteration 452900, Loss: 0.4505212001639633\n",
            "Iteration 453000, Loss: 0.450519976823009\n",
            "Iteration 453100, Loss: 0.4505187544002432\n",
            "Iteration 453200, Loss: 0.45051753289489355\n",
            "Iteration 453300, Loss: 0.4505163123061886\n",
            "Iteration 453400, Loss: 0.4505150926333575\n",
            "Iteration 453500, Loss: 0.45051387387562997\n",
            "Iteration 453600, Loss: 0.4505126560322368\n",
            "Iteration 453700, Loss: 0.4505114391024092\n",
            "Iteration 453800, Loss: 0.45051022308537947\n",
            "Iteration 453900, Loss: 0.4505090079803803\n",
            "Iteration 454000, Loss: 0.4505077937866455\n",
            "Iteration 454100, Loss: 0.4505065805034092\n",
            "Iteration 454200, Loss: 0.45050536812990655\n",
            "Iteration 454300, Loss: 0.45050415666537336\n",
            "Iteration 454400, Loss: 0.4505029461090462\n",
            "Iteration 454500, Loss: 0.4505017364601622\n",
            "Iteration 454600, Loss: 0.45050052771795973\n",
            "Iteration 454700, Loss: 0.4504993198816772\n",
            "Iteration 454800, Loss: 0.45049811295055425\n",
            "Iteration 454900, Loss: 0.4504969069238312\n",
            "Iteration 455000, Loss: 0.45049570180074894\n",
            "Iteration 455100, Loss: 0.450494497580549\n",
            "Iteration 455200, Loss: 0.450493294262474\n",
            "Iteration 455300, Loss: 0.45049209184576694\n",
            "Iteration 455400, Loss: 0.4504908903296719\n",
            "Iteration 455500, Loss: 0.4504896897134334\n",
            "Iteration 455600, Loss: 0.45048848999629654\n",
            "Iteration 455700, Loss: 0.45048729117750796\n",
            "Iteration 455800, Loss: 0.4504860932563138\n",
            "Iteration 455900, Loss: 0.45048489623196186\n",
            "Iteration 456000, Loss: 0.4504837001037005\n",
            "Iteration 456100, Loss: 0.45048250487077834\n",
            "Iteration 456200, Loss: 0.45048131053244556\n",
            "Iteration 456300, Loss: 0.4504801170879522\n",
            "Iteration 456400, Loss: 0.4504789245365494\n",
            "Iteration 456500, Loss: 0.45047773287748916\n",
            "Iteration 456600, Loss: 0.45047654211002386\n",
            "Iteration 456700, Loss: 0.45047535223340696\n",
            "Iteration 456800, Loss: 0.4504741632468925\n",
            "Iteration 456900, Loss: 0.45047297514973494\n",
            "Iteration 457000, Loss: 0.4504717879411899\n",
            "Iteration 457100, Loss: 0.45047060162051356\n",
            "Iteration 457200, Loss: 0.4504694161869629\n",
            "Iteration 457300, Loss: 0.45046823163979505\n",
            "Iteration 457400, Loss: 0.45046704797826864\n",
            "Iteration 457500, Loss: 0.45046586520164295\n",
            "Iteration 457600, Loss: 0.45046468330917694\n",
            "Iteration 457700, Loss: 0.45046350230013177\n",
            "Iteration 457800, Loss: 0.45046232217376814\n",
            "Iteration 457900, Loss: 0.45046114292934814\n",
            "Iteration 458000, Loss: 0.45045996456613424\n",
            "Iteration 458100, Loss: 0.4504587870833894\n",
            "Iteration 458200, Loss: 0.4504576104803782\n",
            "Iteration 458300, Loss: 0.45045643475636477\n",
            "Iteration 458400, Loss: 0.45045525991061486\n",
            "Iteration 458500, Loss: 0.45045408594239444\n",
            "Iteration 458600, Loss: 0.45045291285097017\n",
            "Iteration 458700, Loss: 0.45045174063560967\n",
            "Iteration 458800, Loss: 0.450450569295581\n",
            "Iteration 458900, Loss: 0.45044939883015334\n",
            "Iteration 459000, Loss: 0.45044822923859607\n",
            "Iteration 459100, Loss: 0.45044706052017935\n",
            "Iteration 459200, Loss: 0.45044589267417445\n",
            "Iteration 459300, Loss: 0.450444725699853\n",
            "Iteration 459400, Loss: 0.45044355959648724\n",
            "Iteration 459500, Loss: 0.4504423943633503\n",
            "Iteration 459600, Loss: 0.45044122999971603\n",
            "Iteration 459700, Loss: 0.450440066504859\n",
            "Iteration 459800, Loss: 0.45043890387805413\n",
            "Iteration 459900, Loss: 0.4504377421185776\n",
            "Iteration 460000, Loss: 0.45043658122570557\n",
            "Iteration 460100, Loss: 0.4504354211987158\n",
            "Iteration 460200, Loss: 0.45043426203688575\n",
            "Iteration 460300, Loss: 0.45043310373949413\n",
            "Iteration 460400, Loss: 0.45043194630582034\n",
            "Iteration 460500, Loss: 0.45043078973514455\n",
            "Iteration 460600, Loss: 0.4504296340267473\n",
            "Iteration 460700, Loss: 0.45042847917990997\n",
            "Iteration 460800, Loss: 0.45042732519391465\n",
            "Iteration 460900, Loss: 0.45042617206804414\n",
            "Iteration 461000, Loss: 0.4504250198015817\n",
            "Iteration 461100, Loss: 0.4504238683938116\n",
            "Iteration 461200, Loss: 0.45042271784401866\n",
            "Iteration 461300, Loss: 0.4504215681514884\n",
            "Iteration 461400, Loss: 0.4504204193155068\n",
            "Iteration 461500, Loss: 0.45041927133536097\n",
            "Iteration 461600, Loss: 0.4504181242103384\n",
            "Iteration 461700, Loss: 0.45041697793972696\n",
            "Iteration 461800, Loss: 0.450415832522816\n",
            "Iteration 461900, Loss: 0.45041468795889483\n",
            "Iteration 462000, Loss: 0.4504135442472539\n",
            "Iteration 462100, Loss: 0.4504124013871838\n",
            "Iteration 462200, Loss: 0.45041125937797627\n",
            "Iteration 462300, Loss: 0.45041011821892374\n",
            "Iteration 462400, Loss: 0.45040897790931916\n",
            "Iteration 462500, Loss: 0.450407838448456\n",
            "Iteration 462600, Loss: 0.4504066998356285\n",
            "Iteration 462700, Loss: 0.4504055620701317\n",
            "Iteration 462800, Loss: 0.45040442515126156\n",
            "Iteration 462900, Loss: 0.45040328907831406\n",
            "Iteration 463000, Loss: 0.45040215385058624\n",
            "Iteration 463100, Loss: 0.45040101946737554\n",
            "Iteration 463200, Loss: 0.4503998859279805\n",
            "Iteration 463300, Loss: 0.4503987532317002\n",
            "Iteration 463400, Loss: 0.45039762137783407\n",
            "Iteration 463500, Loss: 0.45039649036568274\n",
            "Iteration 463600, Loss: 0.4503953601945469\n",
            "Iteration 463700, Loss: 0.4503942308637283\n",
            "Iteration 463800, Loss: 0.45039310237252916\n",
            "Iteration 463900, Loss: 0.4503919747202527\n",
            "Iteration 464000, Loss: 0.4503908479062023\n",
            "Iteration 464100, Loss: 0.45038972192968246\n",
            "Iteration 464200, Loss: 0.45038859678999793\n",
            "Iteration 464300, Loss: 0.4503874724864545\n",
            "Iteration 464400, Loss: 0.45038634901835845\n",
            "Iteration 464500, Loss: 0.4503852263850166\n",
            "Iteration 464600, Loss: 0.45038410458573663\n",
            "Iteration 464700, Loss: 0.45038298361982687\n",
            "Iteration 464800, Loss: 0.4503818634865961\n",
            "Iteration 464900, Loss: 0.45038074418535406\n",
            "Iteration 465000, Loss: 0.45037962571541074\n",
            "Iteration 465100, Loss: 0.45037850807607704\n",
            "Iteration 465200, Loss: 0.4503773912666648\n",
            "Iteration 465300, Loss: 0.4503762752864859\n",
            "Iteration 465400, Loss: 0.45037516013485335\n",
            "Iteration 465500, Loss: 0.45037404581108054\n",
            "Iteration 465600, Loss: 0.4503729323144815\n",
            "Iteration 465700, Loss: 0.45037181964437134\n",
            "Iteration 465800, Loss: 0.4503707078000653\n",
            "Iteration 465900, Loss: 0.4503695967808794\n",
            "Iteration 466000, Loss: 0.4503684865861306\n",
            "Iteration 466100, Loss: 0.4503673772151362\n",
            "Iteration 466200, Loss: 0.4503662686672141\n",
            "Iteration 466300, Loss: 0.450365160941683\n",
            "Iteration 466400, Loss: 0.4503640540378624\n",
            "Iteration 466500, Loss: 0.45036294795507215\n",
            "Iteration 466600, Loss: 0.45036184269263285\n",
            "Iteration 466700, Loss: 0.45036073824986583\n",
            "Iteration 466800, Loss: 0.45035963462609296\n",
            "Iteration 466900, Loss: 0.45035853182063673\n",
            "Iteration 467000, Loss: 0.45035742983282034\n",
            "Iteration 467100, Loss: 0.4503563286619676\n",
            "Iteration 467200, Loss: 0.45035522830740304\n",
            "Iteration 467300, Loss: 0.45035412876845166\n",
            "Iteration 467400, Loss: 0.4503530300444393\n",
            "Iteration 467500, Loss: 0.45035193213469227\n",
            "Iteration 467600, Loss: 0.4503508350385375\n",
            "Iteration 467700, Loss: 0.4503497387553028\n",
            "Iteration 467800, Loss: 0.45034864328431656\n",
            "Iteration 467900, Loss: 0.4503475486249072\n",
            "Iteration 468000, Loss: 0.4503464547764049\n",
            "Iteration 468100, Loss: 0.45034536173813927\n",
            "Iteration 468200, Loss: 0.45034426950944156\n",
            "Iteration 468300, Loss: 0.450343178089643\n",
            "Iteration 468400, Loss: 0.45034208747807586\n",
            "Iteration 468500, Loss: 0.45034099767407265\n",
            "Iteration 468600, Loss: 0.4503399086769667\n",
            "Iteration 468700, Loss: 0.45033882048609214\n",
            "Iteration 468800, Loss: 0.4503377331007835\n",
            "Iteration 468900, Loss: 0.45033664652037597\n",
            "Iteration 469000, Loss: 0.4503355607442055\n",
            "Iteration 469100, Loss: 0.4503344757716084\n",
            "Iteration 469200, Loss: 0.45033339160192193\n",
            "Iteration 469300, Loss: 0.4503323082344838\n",
            "Iteration 469400, Loss: 0.4503312256686323\n",
            "Iteration 469500, Loss: 0.4503301439037065\n",
            "Iteration 469600, Loss: 0.4503290629390459\n",
            "Iteration 469700, Loss: 0.4503279827739908\n",
            "Iteration 469800, Loss: 0.45032690340788195\n",
            "Iteration 469900, Loss: 0.45032582484006095\n",
            "Iteration 470000, Loss: 0.4503247470698698\n",
            "Iteration 470100, Loss: 0.4503236700966513\n",
            "Iteration 470200, Loss: 0.45032259391974866\n",
            "Iteration 470300, Loss: 0.45032151853850594\n",
            "Iteration 470400, Loss: 0.45032044395226756\n",
            "Iteration 470500, Loss: 0.4503193701603788\n",
            "Iteration 470600, Loss: 0.4503182971621853\n",
            "Iteration 470700, Loss: 0.4503172249570338\n",
            "Iteration 470800, Loss: 0.45031615354427096\n",
            "Iteration 470900, Loss: 0.4503150829232448\n",
            "Iteration 471000, Loss: 0.4503140130933032\n",
            "Iteration 471100, Loss: 0.45031294405379524\n",
            "Iteration 471200, Loss: 0.4503118758040702\n",
            "Iteration 471300, Loss: 0.45031080834347836\n",
            "Iteration 471400, Loss: 0.4503097416713706\n",
            "Iteration 471500, Loss: 0.45030867578709777\n",
            "Iteration 471600, Loss: 0.450307610690012\n",
            "Iteration 471700, Loss: 0.45030654637946593\n",
            "Iteration 471800, Loss: 0.4503054828548126\n",
            "Iteration 471900, Loss: 0.45030442011540583\n",
            "Iteration 472000, Loss: 0.45030335816059974\n",
            "Iteration 472100, Loss: 0.45030229698974966\n",
            "Iteration 472200, Loss: 0.4503012366022107\n",
            "Iteration 472300, Loss: 0.4503001769973395\n",
            "Iteration 472400, Loss: 0.4502991181744925\n",
            "Iteration 472500, Loss: 0.4502980601330271\n",
            "Iteration 472600, Loss: 0.45029700287230157\n",
            "Iteration 472700, Loss: 0.45029594639167403\n",
            "Iteration 472800, Loss: 0.450294890690504\n",
            "Iteration 472900, Loss: 0.45029383576815135\n",
            "Iteration 473000, Loss: 0.4502927816239762\n",
            "Iteration 473100, Loss: 0.45029172825733954\n",
            "Iteration 473200, Loss: 0.45029067566760317\n",
            "Iteration 473300, Loss: 0.45028962385412896\n",
            "Iteration 473400, Loss: 0.45028857281628015\n",
            "Iteration 473500, Loss: 0.45028752255341953\n",
            "Iteration 473600, Loss: 0.45028647306491154\n",
            "Iteration 473700, Loss: 0.45028542435012064\n",
            "Iteration 473800, Loss: 0.4502843764084119\n",
            "Iteration 473900, Loss: 0.4502833292391511\n",
            "Iteration 474000, Loss: 0.4502822828417046\n",
            "Iteration 474100, Loss: 0.45028123721543933\n",
            "Iteration 474200, Loss: 0.4502801923597229\n",
            "Iteration 474300, Loss: 0.4502791482739233\n",
            "Iteration 474400, Loss: 0.4502781049574095\n",
            "Iteration 474500, Loss: 0.4502770624095506\n",
            "Iteration 474600, Loss: 0.45027602062971656\n",
            "Iteration 474700, Loss: 0.45027497961727775\n",
            "Iteration 474800, Loss: 0.45027393937160537\n",
            "Iteration 474900, Loss: 0.4502728998920713\n",
            "Iteration 475000, Loss: 0.45027186117804735\n",
            "Iteration 475100, Loss: 0.45027082322890666\n",
            "Iteration 475200, Loss: 0.45026978604402257\n",
            "Iteration 475300, Loss: 0.4502687496227692\n",
            "Iteration 475400, Loss: 0.450267713964521\n",
            "Iteration 475500, Loss: 0.4502666790686532\n",
            "Iteration 475600, Loss: 0.4502656449345415\n",
            "Iteration 475700, Loss: 0.45026461156156244\n",
            "Iteration 475800, Loss: 0.45026357894909275\n",
            "Iteration 475900, Loss: 0.45026254709651004\n",
            "Iteration 476000, Loss: 0.4502615160031924\n",
            "Iteration 476100, Loss: 0.4502604856685185\n",
            "Iteration 476200, Loss: 0.45025945609186757\n",
            "Iteration 476300, Loss: 0.45025842727261955\n",
            "Iteration 476400, Loss: 0.4502573992101546\n",
            "Iteration 476500, Loss: 0.45025637190385387\n",
            "Iteration 476600, Loss: 0.45025534535309913\n",
            "Iteration 476700, Loss: 0.45025431955727213\n",
            "Iteration 476800, Loss: 0.4502532945157558\n",
            "Iteration 476900, Loss: 0.45025227022793346\n",
            "Iteration 477000, Loss: 0.450251246693189\n",
            "Iteration 477100, Loss: 0.45025022391090674\n",
            "Iteration 477200, Loss: 0.4502492018804719\n",
            "Iteration 477300, Loss: 0.45024818060126964\n",
            "Iteration 477400, Loss: 0.45024716007268656\n",
            "Iteration 477500, Loss: 0.45024614029410936\n",
            "Iteration 477600, Loss: 0.450245121264925\n",
            "Iteration 477700, Loss: 0.4502441029845219\n",
            "Iteration 477800, Loss: 0.45024308545228797\n",
            "Iteration 477900, Loss: 0.4502420686676125\n",
            "Iteration 478000, Loss: 0.450241052629885\n",
            "Iteration 478100, Loss: 0.45024003733849577\n",
            "Iteration 478200, Loss: 0.45023902279283523\n",
            "Iteration 478300, Loss: 0.450238008992295\n",
            "Iteration 478400, Loss: 0.45023699593626676\n",
            "Iteration 478500, Loss: 0.45023598362414285\n",
            "Iteration 478600, Loss: 0.4502349720553164\n",
            "Iteration 478700, Loss: 0.45023396122918097\n",
            "Iteration 478800, Loss: 0.4502329511451306\n",
            "Iteration 478900, Loss: 0.4502319418025598\n",
            "Iteration 479000, Loss: 0.45023093320086416\n",
            "Iteration 479100, Loss: 0.45022992533943934\n",
            "Iteration 479200, Loss: 0.4502289182176815\n",
            "Iteration 479300, Loss: 0.4502279118349879\n",
            "Iteration 479400, Loss: 0.45022690619075567\n",
            "Iteration 479500, Loss: 0.4502259012843832\n",
            "Iteration 479600, Loss: 0.4502248971152688\n",
            "Iteration 479700, Loss: 0.45022389368281196\n",
            "Iteration 479800, Loss: 0.4502228909864119\n",
            "Iteration 479900, Loss: 0.4502218890254692\n",
            "Iteration 480000, Loss: 0.45022088779938496\n",
            "Iteration 480100, Loss: 0.45021988730756013\n",
            "Iteration 480200, Loss: 0.45021888754939665\n",
            "Iteration 480300, Loss: 0.45021788852429734\n",
            "Iteration 480400, Loss: 0.45021689023166495\n",
            "Iteration 480500, Loss: 0.4502158926709033\n",
            "Iteration 480600, Loss: 0.45021489584141633\n",
            "Iteration 480700, Loss: 0.4502138997426089\n",
            "Iteration 480800, Loss: 0.45021290437388645\n",
            "Iteration 480900, Loss: 0.4502119097346544\n",
            "Iteration 481000, Loss: 0.4502109158243194\n",
            "Iteration 481100, Loss: 0.4502099226422882\n",
            "Iteration 481200, Loss: 0.4502089301879682\n",
            "Iteration 481300, Loss: 0.4502079384607678\n",
            "Iteration 481400, Loss: 0.45020694746009515\n",
            "Iteration 481500, Loss: 0.4502059571853596\n",
            "Iteration 481600, Loss: 0.45020496763597057\n",
            "Iteration 481700, Loss: 0.45020397881133845\n",
            "Iteration 481800, Loss: 0.4502029907108739\n",
            "Iteration 481900, Loss: 0.4502020033339883\n",
            "Iteration 482000, Loss: 0.45020101668009355\n",
            "Iteration 482100, Loss: 0.4502000307486016\n",
            "Iteration 482200, Loss: 0.450199045538926\n",
            "Iteration 482300, Loss: 0.45019806105047966\n",
            "Iteration 482400, Loss: 0.450197077282677\n",
            "Iteration 482500, Loss: 0.45019609423493245\n",
            "Iteration 482600, Loss: 0.45019511190666095\n",
            "Iteration 482700, Loss: 0.45019413029727817\n",
            "Iteration 482800, Loss: 0.4501931494062005\n",
            "Iteration 482900, Loss: 0.4501921692328445\n",
            "Iteration 483000, Loss: 0.45019118977662737\n",
            "Iteration 483100, Loss: 0.45019021103696705\n",
            "Iteration 483200, Loss: 0.45018923301328173\n",
            "Iteration 483300, Loss: 0.45018825570499027\n",
            "Iteration 483400, Loss: 0.45018727911151224\n",
            "Iteration 483500, Loss: 0.4501863032322674\n",
            "Iteration 483600, Loss: 0.4501853280666763\n",
            "Iteration 483700, Loss: 0.45018435361416\n",
            "Iteration 483800, Loss: 0.4501833798741398\n",
            "Iteration 483900, Loss: 0.45018240684603816\n",
            "Iteration 484000, Loss: 0.45018143452927756\n",
            "Iteration 484100, Loss: 0.4501804629232809\n",
            "Iteration 484200, Loss: 0.4501794920274721\n",
            "Iteration 484300, Loss: 0.45017852184127527\n",
            "Iteration 484400, Loss: 0.45017755236411505\n",
            "Iteration 484500, Loss: 0.4501765835954169\n",
            "Iteration 484600, Loss: 0.4501756155346067\n",
            "Iteration 484700, Loss: 0.4501746481811106\n",
            "Iteration 484800, Loss: 0.45017368153435533\n",
            "Iteration 484900, Loss: 0.4501727155937685\n",
            "Iteration 485000, Loss: 0.450171750358778\n",
            "Iteration 485100, Loss: 0.450170785828812\n",
            "Iteration 485200, Loss: 0.4501698220032999\n",
            "Iteration 485300, Loss: 0.450168858881671\n",
            "Iteration 485400, Loss: 0.45016789646335503\n",
            "Iteration 485500, Loss: 0.450166934747783\n",
            "Iteration 485600, Loss: 0.4501659737343857\n",
            "Iteration 485700, Loss: 0.45016501342259474\n",
            "Iteration 485800, Loss: 0.4501640538118423\n",
            "Iteration 485900, Loss: 0.450163094901561\n",
            "Iteration 486000, Loss: 0.450162136691184\n",
            "Iteration 486100, Loss: 0.450161179180145\n",
            "Iteration 486200, Loss: 0.45016022236787806\n",
            "Iteration 486300, Loss: 0.4501592662538181\n",
            "Iteration 486400, Loss: 0.4501583108374\n",
            "Iteration 486500, Loss: 0.45015735611805996\n",
            "Iteration 486600, Loss: 0.45015640209523405\n",
            "Iteration 486700, Loss: 0.450155448768359\n",
            "Iteration 486800, Loss: 0.45015449613687236\n",
            "Iteration 486900, Loss: 0.45015354420021175\n",
            "Iteration 487000, Loss: 0.4501525929578154\n",
            "Iteration 487100, Loss: 0.45015164240912253\n",
            "Iteration 487200, Loss: 0.4501506925535722\n",
            "Iteration 487300, Loss: 0.45014974339060454\n",
            "Iteration 487400, Loss: 0.4501487949196598\n",
            "Iteration 487500, Loss: 0.45014784714017886\n",
            "Iteration 487600, Loss: 0.4501469000516034\n",
            "Iteration 487700, Loss: 0.45014595365337495\n",
            "Iteration 487800, Loss: 0.4501450079449364\n",
            "Iteration 487900, Loss: 0.4501440629257305\n",
            "Iteration 488000, Loss: 0.4501431185952008\n",
            "Iteration 488100, Loss: 0.4501421749527912\n",
            "Iteration 488200, Loss: 0.4501412319979463\n",
            "Iteration 488300, Loss: 0.450140289730111\n",
            "Iteration 488400, Loss: 0.4501393481487309\n",
            "Iteration 488500, Loss: 0.45013840725325205\n",
            "Iteration 488600, Loss: 0.45013746704312085\n",
            "Iteration 488700, Loss: 0.4501365275177845\n",
            "Iteration 488800, Loss: 0.45013558867669057\n",
            "Iteration 488900, Loss: 0.45013465051928697\n",
            "Iteration 489000, Loss: 0.45013371304502225\n",
            "Iteration 489100, Loss: 0.4501327762533457\n",
            "Iteration 489200, Loss: 0.45013184014370666\n",
            "Iteration 489300, Loss: 0.4501309047155553\n",
            "Iteration 489400, Loss: 0.45012996996834204\n",
            "Iteration 489500, Loss: 0.4501290359015183\n",
            "Iteration 489600, Loss: 0.4501281025145354\n",
            "Iteration 489700, Loss: 0.4501271698068453\n",
            "Iteration 489800, Loss: 0.45012623777790106\n",
            "Iteration 489900, Loss: 0.4501253064271552\n",
            "Iteration 490000, Loss: 0.45012437575406183\n",
            "Iteration 490100, Loss: 0.4501234457580747\n",
            "Iteration 490200, Loss: 0.4501225164386486\n",
            "Iteration 490300, Loss: 0.4501215877952383\n",
            "Iteration 490400, Loss: 0.45012065982729965\n",
            "Iteration 490500, Loss: 0.45011973253428866\n",
            "Iteration 490600, Loss: 0.450118805915662\n",
            "Iteration 490700, Loss: 0.45011787997087666\n",
            "Iteration 490800, Loss: 0.4501169546993901\n",
            "Iteration 490900, Loss: 0.4501160301006607\n",
            "Iteration 491000, Loss: 0.45011510617414674\n",
            "Iteration 491100, Loss: 0.4501141829193074\n",
            "Iteration 491200, Loss: 0.45011326033560195\n",
            "Iteration 491300, Loss: 0.450112338422491\n",
            "Iteration 491400, Loss: 0.45011141717943454\n",
            "Iteration 491500, Loss: 0.45011049660589386\n",
            "Iteration 491600, Loss: 0.4501095767013304\n",
            "Iteration 491700, Loss: 0.4501086574652063\n",
            "Iteration 491800, Loss: 0.450107738896984\n",
            "Iteration 491900, Loss: 0.4501068209961265\n",
            "Iteration 492000, Loss: 0.45010590376209714\n",
            "Iteration 492100, Loss: 0.4501049871943599\n",
            "Iteration 492200, Loss: 0.45010407129237956\n",
            "Iteration 492300, Loss: 0.45010315605562073\n",
            "Iteration 492400, Loss: 0.45010224148354916\n",
            "Iteration 492500, Loss: 0.45010132757563037\n",
            "Iteration 492600, Loss: 0.45010041433133097\n",
            "Iteration 492700, Loss: 0.4500995017501181\n",
            "Iteration 492800, Loss: 0.45009858983145884\n",
            "Iteration 492900, Loss: 0.45009767857482114\n",
            "Iteration 493000, Loss: 0.45009676797967324\n",
            "Iteration 493100, Loss: 0.45009585804548413\n",
            "Iteration 493200, Loss: 0.45009494877172324\n",
            "Iteration 493300, Loss: 0.4500940401578601\n",
            "Iteration 493400, Loss: 0.4500931322033651\n",
            "Iteration 493500, Loss: 0.45009222490770917\n",
            "Iteration 493600, Loss: 0.4500913182703635\n",
            "Iteration 493700, Loss: 0.45009041229079955\n",
            "Iteration 493800, Loss: 0.45008950696848987\n",
            "Iteration 493900, Loss: 0.4500886023029069\n",
            "Iteration 494000, Loss: 0.4500876982935241\n",
            "Iteration 494100, Loss: 0.45008679493981485\n",
            "Iteration 494200, Loss: 0.4500858922412534\n",
            "Iteration 494300, Loss: 0.4500849901973143\n",
            "Iteration 494400, Loss: 0.45008408880747286\n",
            "Iteration 494500, Loss: 0.4500831880712044\n",
            "Iteration 494600, Loss: 0.45008228798798494\n",
            "Iteration 494700, Loss: 0.4500813885572912\n",
            "Iteration 494800, Loss: 0.45008048977859993\n",
            "Iteration 494900, Loss: 0.45007959165138883\n",
            "Iteration 495000, Loss: 0.4500786941751358\n",
            "Iteration 495100, Loss: 0.4500777973493191\n",
            "Iteration 495200, Loss: 0.45007690117341764\n",
            "Iteration 495300, Loss: 0.450076005646911\n",
            "Iteration 495400, Loss: 0.45007511076927886\n",
            "Iteration 495500, Loss: 0.4500742165400016\n",
            "Iteration 495600, Loss: 0.45007332295855973\n",
            "Iteration 495700, Loss: 0.45007243002443503\n",
            "Iteration 495800, Loss: 0.45007153773710873\n",
            "Iteration 495900, Loss: 0.45007064609606334\n",
            "Iteration 496000, Loss: 0.4500697551007813\n",
            "Iteration 496100, Loss: 0.4500688647507457\n",
            "Iteration 496200, Loss: 0.4500679750454404\n",
            "Iteration 496300, Loss: 0.45006708598434947\n",
            "Iteration 496400, Loss: 0.45006619756695704\n",
            "Iteration 496500, Loss: 0.45006530979274856\n",
            "Iteration 496600, Loss: 0.4500644226612092\n",
            "Iteration 496700, Loss: 0.4500635361718252\n",
            "Iteration 496800, Loss: 0.45006265032408277\n",
            "Iteration 496900, Loss: 0.4500617651174686\n",
            "Iteration 497000, Loss: 0.45006088055147037\n",
            "Iteration 497100, Loss: 0.45005999662557566\n",
            "Iteration 497200, Loss: 0.45005911333927284\n",
            "Iteration 497300, Loss: 0.45005823069205064\n",
            "Iteration 497400, Loss: 0.45005734868339814\n",
            "Iteration 497500, Loss: 0.450056467312805\n",
            "Iteration 497600, Loss: 0.4500555865797615\n",
            "Iteration 497700, Loss: 0.450054706483758\n",
            "Iteration 497800, Loss: 0.45005382702428576\n",
            "Iteration 497900, Loss: 0.4500529482008359\n",
            "Iteration 498000, Loss: 0.4500520700129007\n",
            "Iteration 498100, Loss: 0.4500511924599726\n",
            "Iteration 498200, Loss: 0.45005031554154445\n",
            "Iteration 498300, Loss: 0.4500494392571093\n",
            "Iteration 498400, Loss: 0.4500485636061611\n",
            "Iteration 498500, Loss: 0.4500476885881942\n",
            "Iteration 498600, Loss: 0.45004681420270326\n",
            "Iteration 498700, Loss: 0.45004594044918345\n",
            "Iteration 498800, Loss: 0.4500450673271303\n",
            "Iteration 498900, Loss: 0.45004419483603986\n",
            "Iteration 499000, Loss: 0.4500433229754089\n",
            "Iteration 499100, Loss: 0.4500424517447341\n",
            "Iteration 499200, Loss: 0.4500415811435131\n",
            "Iteration 499300, Loss: 0.4500407111712438\n",
            "Iteration 499400, Loss: 0.45003984182742446\n",
            "Iteration 499500, Loss: 0.45003897311155383\n",
            "Iteration 499600, Loss: 0.45003810502313124\n",
            "Iteration 499700, Loss: 0.45003723756165626\n",
            "Iteration 499800, Loss: 0.4500363707266292\n",
            "Iteration 499900, Loss: 0.45003550451755087\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 0 0 0 1 2 0 0 0 1 2 1 1 1 1 1 3 1 0 1 2 2 2 2 2 2 2 3 2 1 3 3 3 3 3\n",
            " 3 3 3]\n",
            "Actual: [[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3\n",
            "  3 3 3 3]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_arr = np.array(predictions_arr)\n",
        "predict = predictions_arr.flatten()\n",
        "actual = target_data.flatten()\n",
        "\n",
        "corret_num = 0\n",
        "miss_num = 0;\n",
        "for i in range(len(predict)):\n",
        "  if predict[i] == actual[i]:\n",
        "    corret_num += 1\n",
        "  else:\n",
        "    miss_num += 1\n",
        "\n",
        "print(miss_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg8HjdqIb4vu",
        "outputId": "b8fd5319-5aaf-4d6e-e298-38ae8c5ecfc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    }
  ]
}